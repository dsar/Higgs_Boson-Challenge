{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCML Project-1 ~ Team #60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gradient_descent import gradient_descent\n",
    "from stochastic_gradient_descent import stochastic_gradient_descent\n",
    "from least_squares import least_squares\n",
    "from ridge_regression import ridge_regression\n",
    "from costs import compute_loss\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '/home/sarigian/Desktop/project1/Data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/19): loss=0.5, w0=43.06659804159989, w1=-10.913679441599983\n",
      "Gradient Descent(1/19): loss=321306592225.9253, w0=-30617766.000192683, w1=14368738.360768722\n",
      "Gradient Descent(2/19): loss=1.6899913118692868e+24, w0=65417515310390.805, w1=-32817891276996.22\n",
      "Gradient Descent(3/19): loss=8.89230171241133e+36, w0=-1.49636236124437e+20, w1=7.52722312830454e+19\n",
      "Gradient Descent(4/19): loss=4.678908388650185e+49, w0=3.431924401511498e+26, w1=-1.7266289192345694e+26\n",
      "Gradient Descent(5/19): loss=2.4619254533743463e+62, w0=-7.872242637026135e+32, w1=3.96062903898154e+32\n",
      "Gradient Descent(6/19): loss=1.295404063287896e+75, w0=1.8057736003582295e+39, w1=-9.085092328143551e+38\n",
      "Gradient Descent(7/19): loss=6.816094633912215e+87, w0=-4.142175050954717e+45, w1=2.0839846871149405e+45\n",
      "Gradient Descent(8/19): loss=3.586459806257528e+100, w0=9.501531757864199e+51, w1=-4.780350071591723e+51\n",
      "Gradient Descent(9/19): loss=1.8871061264180244e+113, w0=-2.179509680531091e+58, w1=1.0965410133870644e+58\n",
      "Gradient Descent(10/19): loss=9.929484016943502e+125, w0=4.999470159889333e+64, w1=-2.5153015491171325e+64\n",
      "Gradient Descent(11/19): loss=5.224648018597775e+138, w0=-1.1468038937317467e+71, w1=5.769726627413964e+70\n",
      "Gradient Descent(12/19): loss=2.749080100401859e+151, w0=2.6305971005371363e+77, w1=-1.3234892399591227e+77\n",
      "Gradient Descent(13/19): loss=1.4464977107594132e+164, w0=-6.034197427459177e+83, w1=3.0358869343392457e+83\n",
      "Gradient Descent(14/19): loss=7.611111902219211e+176, w0=1.3841548972330735e+90, w1=-6.963871862212185e+89\n",
      "Gradient Descent(15/19): loss=4.0047781588046776e+189, w0=-3.175044904589796e+96, w1=1.59740834761572e+96\n",
      "Gradient Descent(16/19): loss=2.107214859968439e+202, w0=7.283079492268798e+102, w1=-3.6642165156407837e+102\n",
      "Gradient Descent(17/19): loss=1.108764153717097e+215, w0=-1.6706298173618585e+109, w1=8.405166214096019e+108\n",
      "Gradient Descent(18/19): loss=5.834041757784578e+227, w0=3.832175647157654e+115, w1=-1.9280197768069753e+115\n",
      "Gradient Descent(19/19): loss=3.0697279595005933e+240, w0=-8.790439412759004e+121, w1=4.422589827580951e+121\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "# from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 20\n",
    "gamma = 0.4\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(30)\n",
    "\n",
    "# Start gradient descent.\n",
    "# start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tX, w_initial, max_iters, gamma)\n",
    "# end_time = datetime.datetime.now()\n",
    "\n",
    "# # Print result\n",
    "# exection_time = (end_time - start_time).total_seconds()\n",
    "# print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/49): loss=0.5, w0=-33.5692, w1=-6.409600000000001\n",
      "Stochastic Gradient Descent(1/49): loss=3990855399190.4175, w0=84261887.94450659, w1=68995706.6933846\n",
      "Stochastic Gradient Descent(2/49): loss=3.142227176912606e+25, w0=-485328226723468.9, w1=-219095762639231.8\n",
      "Stochastic Gradient Descent(3/49): loss=5.099915182647202e+38, w0=1.8944232090107854e+21, w1=3.185794717151744e+20\n",
      "Stochastic Gradient Descent(4/49): loss=4.025312760217446e+51, w0=-3.87534053471788e+27, w1=-2.1361791011400787e+27\n",
      "Stochastic Gradient Descent(5/49): loss=3.0314678799359173e+64, w0=-9.839362928908653e+34, w1=7.028986323558032e+33\n",
      "Stochastic Gradient Descent(6/49): loss=5.864711316706704e+77, w0=4.327765279173436e+41, w1=-3.665041152873232e+40\n",
      "Stochastic Gradient Descent(7/49): loss=4.308192871938005e+90, w0=1.581298851987637e+47, w1=1.659033386362798e+45\n",
      "Stochastic Gradient Descent(8/49): loss=9.076076758367199e+99, w0=-7.641938957916882e+51, w1=-4.909003068687683e+50\n",
      "Stochastic Gradient Descent(9/49): loss=3.5353336725488183e+109, w0=3.360119279729279e+57, w1=-4.342203761190517e+56\n",
      "Stochastic Gradient Descent(10/49): loss=1.2481183518805172e+119, w0=-2.49912062235591e+61, w1=-1.0691578335742331e+61\n",
      "Stochastic Gradient Descent(11/49): loss=9.856404700369568e+128, w0=1.774185923555935e+67, w1=-1.5195968597254521e+66\n",
      "Stochastic Gradient Descent(12/49): loss=1.9106486815239846e+142, w0=-7.811434549174062e+73, w1=8.436975250078241e+72\n",
      "Stochastic Gradient Descent(13/49): loss=2.977177618828404e+155, w0=-4.88303712558626e+79, w1=-2.0356595735344544e+79\n",
      "Stochastic Gradient Descent(14/49): loss=2.314888714402181e+168, w0=8.758155188016492e+85, w1=7.076463502738853e+84\n",
      "Stochastic Gradient Descent(15/49): loss=1.785336290608424e+181, w0=-1.4961713468204408e+92, w1=-1.3996079020878037e+92\n",
      "Stochastic Gradient Descent(16/49): loss=2.857716229471057e+194, w0=1.1014035512423332e+99, w1=3.3739415549767115e+98\n",
      "Stochastic Gradient Descent(17/49): loss=4.5779406217094193e+207, w0=-3.975537889994229e+105, w1=-6.480643036666732e+104\n",
      "Stochastic Gradient Descent(18/49): loss=3.5709042343119045e+220, w0=1.1021254321157493e+112, w1=5.648204487440253e+111\n",
      "Stochastic Gradient Descent(19/49): loss=1.4884262904317263e+229, w0=-2.096520414001159e+116, w1=-4.615475148952414e+115\n",
      "Stochastic Gradient Descent(20/49): loss=3.7097178560462554e+238, w0=1.2175136074300717e+121, w1=6.323567817983081e+120\n",
      "Stochastic Gradient Descent(21/49): loss=3.0752701473962307e+251, w0=-5.013262800768586e+127, w1=-2.1012035301396802e+127\n",
      "Stochastic Gradient Descent(22/49): loss=2.3518508684633068e+264, w0=9.907342380596774e+133, w1=4.467990095375804e+133\n",
      "Stochastic Gradient Descent(23/49): loss=1.7752150663509804e+277, w0=2.3810361165638572e+141, w1=-1.7755516969343972e+140\n",
      "Stochastic Gradient Descent(24/49): loss=4.73510890427126e+286, w0=-1.5886828650853673e+145, w1=-4.8405142686367705e+144\n",
      "Stochastic Gradient Descent(25/49): loss=2.9493033282966906e+296, w0=9.705088206904174e+150, w1=-8.843350913134731e+149\n",
      "Stochastic Gradient Descent(26/49): loss=inf, w0=3.3808387837355914e+156, w1=1.710880270615326e+155\n",
      "Stochastic Gradient Descent(27/49): loss=inf, w0=4.59757388712396e+161, w1=1.6392084945576292e+161\n",
      "Stochastic Gradient Descent(28/49): loss=inf, w0=-5.032972469084514e+166, w1=-1.2057995307067149e+166\n",
      "Stochastic Gradient Descent(29/49): loss=inf, w0=-1.7288429051016735e+172, w1=-4.420296071074002e+171\n",
      "Stochastic Gradient Descent(30/49): loss=inf, w0=-1.2624942537718616e+176, w1=-2.9752272385553725e+176\n",
      "Stochastic Gradient Descent(31/49): loss=inf, w0=-2.265726599259407e+181, w1=-2.4458119695329613e+181\n",
      "Stochastic Gradient Descent(32/49): loss=inf, w0=1.1024422361547135e+188, w1=5.900401095711887e+187\n",
      "Stochastic Gradient Descent(33/49): loss=inf, w0=3.558375799401383e+195, w1=-3.021341763126834e+194\n",
      "Stochastic Gradient Descent(34/49): loss=inf, w0=1.2928339800156632e+201, w1=6.845766281459347e+200\n",
      "Stochastic Gradient Descent(35/49): loss=inf, w0=-2.5527348457417018e+207, w1=-7.895070573453085e+206\n",
      "Stochastic Gradient Descent(36/49): loss=inf, w0=6.653147555418828e+213, w1=1.2254928780859031e+213\n",
      "Stochastic Gradient Descent(37/49): loss=inf, w0=-3.607112011348016e+220, w1=-1.3239367476854897e+220\n",
      "Stochastic Gradient Descent(38/49): loss=inf, w0=-2.584649954565078e+225, w1=-5.934843039484585e+224\n",
      "Stochastic Gradient Descent(39/49): loss=inf, w0=-3.6658461433422846e+230, w1=-1.4686769726934636e+230\n",
      "Stochastic Gradient Descent(40/49): loss=inf, w0=9.89807657904737e+236, w1=5.746858212290718e+235\n",
      "Stochastic Gradient Descent(41/49): loss=inf, w0=-1.5191887800913565e+243, w1=-7.588531682151218e+242\n",
      "Stochastic Gradient Descent(42/49): loss=inf, w0=4.004499620551903e+249, w1=3.595867094983181e+249\n",
      "Stochastic Gradient Descent(43/49): loss=inf, w0=-2.6405615556174112e+256, w1=-2.6883370368936308e+255\n",
      "Stochastic Gradient Descent(44/49): loss=inf, w0=7.273461381428842e+262, w1=3.0272802934867814e+262\n",
      "Stochastic Gradient Descent(45/49): loss=inf, w0=1.7664698150844674e+270, w1=-1.6589782527975524e+269\n",
      "Stochastic Gradient Descent(46/49): loss=inf, w0=-2.8869153276357676e+274, w1=-4.0460015500506045e+273\n",
      "Stochastic Gradient Descent(47/49): loss=inf, w0=-1.839818314115902e+279, w1=-1.5095748726375502e+279\n",
      "Stochastic Gradient Descent(48/49): loss=inf, w0=-9.195314771166297e+284, w1=-8.99988586278433e+283\n",
      "Stochastic Gradient Descent(49/49): loss=inf, w0=-4.048039430349348e+287, w1=-8.884682197216282e+286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarigian/Desktop/project1/scripts/costs.py:9: RuntimeWarning: overflow encountered in square\n",
      "  return 1/2*np.mean(e**2)\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.4\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(30)\n",
    "\n",
    "# Start SGD.\n",
    "# start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = stochastic_gradient_descent(y, tX, w_initial, batch_size, max_iters, gamma)\n",
    "# end_time = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.339686809908\n",
      "[  8.03908656e-05  -7.20112725e-03  -6.05470621e-03  -5.47503706e-04\n",
      "  -1.93917581e-02   4.73466159e-04  -2.60377150e-02   3.25108506e-01\n",
      "  -3.80835171e-05  -2.72729929e+00  -2.21218331e-01   9.50817492e-02\n",
      "   6.40392496e-02   2.73555898e+00  -3.31801082e-04  -9.54327750e-04\n",
      "   2.74031566e+00  -5.34164922e-04   9.73498609e-04   3.69225052e-03\n",
      "   3.54487428e-04  -5.43344599e-04  -3.30448035e-01  -1.40800497e-03\n",
      "   8.31432882e-04   1.02117272e-03  -1.68047416e-03  -5.83664815e-03\n",
      "  -1.11087997e-02   2.72775918e+00]\n"
     ]
    }
   ],
   "source": [
    "# start_ls_time = datetime.datetime.now()\n",
    "ls_wopt, ls_loss = least_squares(y,tX)\n",
    "print(ls_loss)\n",
    "print(ls_wopt)\n",
    "# end_ls_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.98628815e-04  -8.39656035e-03  -3.22311998e-03  -2.06213002e-03\n",
      "  -1.13848438e-02   4.91720836e-04  -2.05135585e-02   1.01683862e-01\n",
      "  -1.49557629e-05   3.83138841e-03  -8.60779330e-02   8.16367032e-02\n",
      "   3.42443157e-02   5.15657721e-03  -4.37703314e-04  -1.27074718e-03\n",
      "   4.15478661e-03  -5.41025172e-04   9.28858577e-04   4.71409380e-03\n",
      "   4.53983951e-04  -7.26027320e-04  -9.49952316e-02   8.41627936e-04\n",
      "  -4.41338637e-04  -2.00086355e-04  -2.04653783e-04   1.59679351e-04\n",
      "  -2.54485616e-03  -5.47587190e-03]\n"
     ]
    }
   ],
   "source": [
    "# lambdas = np.logspace(-3, 1, 10)      \n",
    "# φ_x = build_poly(x, degree)\n",
    "# x_train, x_test, y_train, y_test = split_data(tX, y, ratio, seed)\n",
    "    \n",
    "#     for lamb in lambdas:\n",
    "\n",
    "w_ridge = ridge_regression(y, tX, 0.1)\n",
    "\n",
    "print(w_ridge)\n",
    "\n",
    "# rmse_tr = np.sqrt(2*compute_loss(y, tX, w_ridge))\n",
    "# rmse_te = np.sqrt(2*compute_loss(y, tX, w_ridge))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '/home/sarigian/Desktop/project1/Data/test.csv' # TODO: download test data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
