{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCML Project-1 ~ Team #60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from costs import compute_loss\n",
    "from proj1_helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape:  (200,)\n",
      "original tX shape:  (200, 30)\n",
      "ids shape:  (200,)\n"
     ]
    }
   ],
   "source": [
    "DATA_TRAIN_PATH = \"../Data/train30.csv\" # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "#print the shape of the offset x matrix.\n",
    "print('y shape: ',y.shape)\n",
    "print('original tX shape: ',tX.shape)\n",
    "print('ids shape: ',ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Outliers - Extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature:  0  ->  35.0\n",
      "feature:  1  ->  0.0\n",
      "feature:  2  ->  0.0\n",
      "feature:  3  ->  0.0\n",
      "feature:  4  ->  144.0\n",
      "feature:  5  ->  144.0\n",
      "feature:  6  ->  144.0\n",
      "feature:  7  ->  0.0\n",
      "feature:  8  ->  0.0\n",
      "feature:  9  ->  0.0\n",
      "feature:  10  ->  0.0\n",
      "feature:  11  ->  0.0\n",
      "feature:  12  ->  144.0\n",
      "feature:  13  ->  0.0\n",
      "feature:  14  ->  0.0\n",
      "feature:  15  ->  0.0\n",
      "feature:  16  ->  0.0\n",
      "feature:  17  ->  0.0\n",
      "feature:  18  ->  0.0\n",
      "feature:  19  ->  0.0\n",
      "feature:  20  ->  0.0\n",
      "feature:  21  ->  0.0\n",
      "feature:  22  ->  0.0\n",
      "feature:  23  ->  87.0\n",
      "feature:  24  ->  87.0\n",
      "feature:  25  ->  87.0\n",
      "feature:  26  ->  144.0\n",
      "feature:  27  ->  144.0\n",
      "feature:  28  ->  144.0\n",
      "feature:  29  ->  0.0\n"
     ]
    }
   ],
   "source": [
    "outliers = count_outliers(tX,-999)\n",
    "for feature in range(tX.shape[1]):\n",
    "    print('feature: ',feature,' -> ',outliers[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 30\n",
      "(200, 31)\n",
      "standardized tX shape:  (200, 30)\n"
     ]
    }
   ],
   "source": [
    "#standardization\n",
    "# tX, mean_x, std_x = standardize(tX, mean_x=None, std_x=None)\n",
    "tX_train, _, _ = standardize_outliers(tX)\n",
    "print('standardized tX shape: ',tX.shape)\n",
    "# print('tX mean shape: ',mean_x.shape)\n",
    "# print('tX std shape: ',std_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of output y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Histogram of output y\n",
    "plt.hist(y, bins=10, align='mid')\n",
    "plt.title(\"Histogram of output y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of y as a function of all its features (one by one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Analyse y as a function of all the other features (one by one)\n",
    "plot_features_by_y(y,tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of polynomial bases, feature by feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.        0.77148 ]\n",
      " [ 5.        0.745616]\n",
      " [ 5.        0.767272]\n",
      " [ 5.        0.74788 ]\n",
      " [ 5.        0.747712]\n",
      " [ 5.        0.746608]\n",
      " [ 4.        0.745392]\n",
      " [ 4.        0.75208 ]\n",
      " [ 5.        0.744768]\n",
      " [ 4.        0.752344]\n",
      " [ 5.        0.749832]\n",
      " [ 0.        0.747256]\n",
      " [ 3.        0.746256]\n",
      " [ 5.        0.76056 ]\n",
      " [ 2.        0.746792]\n",
      " [ 1.        0.744032]\n",
      " [ 5.        0.744688]\n",
      " [ 4.        0.748096]\n",
      " [ 1.        0.744032]\n",
      " [ 0.        0.744448]\n",
      " [ 1.        0.744032]\n",
      " [ 5.        0.75064 ]\n",
      " [ 3.        0.746496]\n",
      " [ 5.        0.749736]\n",
      " [ 5.        0.74916 ]\n",
      " [ 1.        0.744032]\n",
      " [ 4.        0.744088]\n",
      " [ 4.        0.744672]\n",
      " [ 1.        0.744032]\n",
      " [ 5.        0.748272]]\n"
     ]
    }
   ],
   "source": [
    "from poly import *\n",
    "\n",
    "deg = find_best_poly(y, tX[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 108)\n"
     ]
    }
   ],
   "source": [
    "tX = build_optimal(y, tX[:,1:], deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gradient_descent import least_squares_GD\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "# Start gradient descent.\n",
    "gradient_losses, gradient_ws = least_squares_GD(y, tX, w_initial, gamma, max_iters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stochastic_gradient_descent import least_squares_SGD\n",
    "from stochastic_gradient_descent import get_min_param_index\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "batch_size = 1\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "# Start SGD.\n",
    "stoch_gradient_losses, stoch_gradient_ws = least_squares_SGD(y, tX, w_initial, batch_size, gamma, max_iters)\n",
    "\n",
    "min_stoch_i, min_stoch_loss = get_min_param_index(stoch_gradient_losses)\n",
    "print('min index: ',min_stoch_i)\n",
    "print('min loss: ',min_stoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from least_squares import least_squares\n",
    "\n",
    "ls_wopt, ls_loss = least_squares(y,tX)\n",
    "print('loss=',ls_loss)\n",
    "print('parameters w: ',ls_wopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ridge_regression import cross_validation_ridge_regression\n",
    "\n",
    "min_rr_rmse,best_rr_lambda = cross_validation_ridge_regression(y,tX,k_fold=10)\n",
    "print('min_rr_rmse: ',min_rr_rmse)\n",
    "print('best lambda: ', best_rr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ridge_regression import ridge_regression\n",
    "from build_polynomial import find_best_poly,build_optimal\n",
    "from ridge_regression import test_RR\n",
    "\n",
    "best_deg = find_best_poly(y, tX_train[:,1:],test_RR)\n",
    "rr_opt_tr = build_optimal(tX_train[:,1:], best_deg)\n",
    "print('execute with best lambda: ',best_rr_lambda) ##1.43844988829\n",
    "w_ridge, ridge_loss = ridge_regression(y, rr_opt_tr,lamb=best_rr_lambda)\n",
    "\n",
    "print('loss: ',ridge_loss)\n",
    "print('parameters w: ',w_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from logistic_regression import *\n",
    "\n",
    "min_rlr_loss,best_rlr_lambda = cross_validation_logistic_regression(y,tX,k_fold=5)\n",
    "print('min_lr_loss: ',min_rlr_loss)\n",
    "print('best_lr_gamma: ',best_rlr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from logistic_regression import logistic_regression_gradient_descent\n",
    "\n",
    "best_deg = find_best_poly(y, tX_train[:,1:],test_LR)\n",
    "opt_tr = build_optimal(tX_train[:,1:], best_deg)\n",
    "# print('execute with best gamma: ', best_lr_gamma)\n",
    "lr_loss, lr_w = logistic_regression_gradient_descent(y, opt_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda:  1e-15\n",
      "[[ 2.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 2.     0.775]\n",
      " [ 5.     0.8  ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 3.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.775]\n",
      " [ 2.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 3.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]]\n",
      "(200, 22)\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.678088616581658\n",
      "Current iteration=2000, the loss=0.6642558857672575\n",
      "Current iteration=3000, the loss=0.651456586711144\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6897125891437875\n",
      "Current iteration=2000, the loss=0.6866294908700157\n",
      "Current iteration=3000, the loss=0.6836207059952971\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6906763504648722\n",
      "Current iteration=2000, the loss=0.6892393512178465\n",
      "Current iteration=3000, the loss=0.687856841851102\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.689551966733011\n",
      "Current iteration=2000, the loss=0.6861158201253685\n",
      "Current iteration=3000, the loss=0.6827511364678874\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6884874254634632\n",
      "Current iteration=2000, the loss=0.6853133492084423\n",
      "Current iteration=3000, the loss=0.6822782524285467\n",
      "lambda:  3.16227766017e-11\n",
      "[[ 2.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 2.     0.775]\n",
      " [ 5.     0.8  ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 3.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.775]\n",
      " [ 2.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 3.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]]\n",
      "(200, 22)\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6780886165816581\n",
      "Current iteration=2000, the loss=0.6642558857672578\n",
      "Current iteration=3000, the loss=0.6514565867111447\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6897125891437875\n",
      "Current iteration=2000, the loss=0.6866294908700158\n",
      "Current iteration=3000, the loss=0.6836207059952972\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6906763504648722\n",
      "Current iteration=2000, the loss=0.6892393512178465\n",
      "Current iteration=3000, the loss=0.6878568418511021\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.689551966733011\n",
      "Current iteration=2000, the loss=0.6861158201253686\n",
      "Current iteration=3000, the loss=0.6827511364678875\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6884874254634632\n",
      "Current iteration=2000, the loss=0.6853133492084424\n",
      "Current iteration=3000, the loss=0.6822782524285468\n",
      "lambda:  1e-06\n",
      "[[ 2.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 2.     0.775]\n",
      " [ 5.     0.8  ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 3.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.775]\n",
      " [ 2.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 3.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]]\n",
      "(200, 22)\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6780886165840817\n",
      "Current iteration=2000, the loss=0.6642558857765513\n",
      "Current iteration=3000, the loss=0.6514565867312435\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.689712589144332\n",
      "Current iteration=2000, the loss=0.6866294908720713\n",
      "Current iteration=3000, the loss=0.6836207059998083\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6906763504652412\n",
      "Current iteration=2000, the loss=0.6892393512189579\n",
      "Current iteration=3000, the loss=0.687856841853346\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6895519667335871\n",
      "Current iteration=2000, the loss=0.6861158201276188\n",
      "Current iteration=3000, the loss=0.6827511364728799\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6884874254641745\n",
      "Current iteration=2000, the loss=0.6853133492107659\n",
      "Current iteration=3000, the loss=0.6822782524333723\n",
      "lambda:  0.0316227766017\n",
      "[[ 2.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 2.     0.775]\n",
      " [ 5.     0.8  ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 3.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.775]\n",
      " [ 2.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 3.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]]\n",
      "(200, 22)\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6780886932227134\n",
      "Current iteration=2000, the loss=0.6642561796626395\n",
      "Current iteration=3000, the loss=0.6514572223150723\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6897126063608019\n",
      "Current iteration=2000, the loss=0.6866295558724775\n",
      "Current iteration=3000, the loss=0.6836208486527353\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6906763621341456\n",
      "Current iteration=2000, the loss=0.6892393863593339\n",
      "Current iteration=3000, the loss=0.6878569128117329\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.689551984952515\n",
      "Current iteration=2000, the loss=0.6861158912913247\n",
      "Current iteration=3000, the loss=0.6827512943461052\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6884874479562894\n",
      "Current iteration=2000, the loss=0.685313422687893\n",
      "Current iteration=3000, the loss=0.6822784050241146\n",
      "lambda:  1000.0\n",
      "[[ 2.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 2.     0.775]\n",
      " [ 5.     0.8  ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 3.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.775]\n",
      " [ 2.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 3.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]]\n",
      "(200, 22)\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.68050743698591\n",
      "Current iteration=2000, the loss=0.6735132401597735\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6902559785489933\n",
      "Current iteration=2000, the loss=0.688677013831385\n",
      "Current iteration=3000, the loss=0.6881054265631042\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6910446993767364\n",
      "Current iteration=2000, the loss=0.6903465650335926\n",
      "Current iteration=3000, the loss=0.6900882360606648\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6901269811251216\n",
      "Current iteration=2000, the loss=0.6883574075935387\n",
      "Current iteration=3000, the loss=0.6877141785627997\n",
      "Current iteration=0, the loss=0.6931471805599454\n",
      "Current iteration=1000, the loss=0.6891973895055133\n",
      "Current iteration=2000, the loss=0.6876282488272505\n",
      "Current iteration=3000, the loss=0.6870762127452694\n",
      "min_rlr_rmse:  0.7092800669153165\n",
      "best lambda:  1000.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEdCAYAAAAikTHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XucVXW9//HXB0ENBQeUi1zHMM1LNVoHzRujmGJ5KbPC\nEsR+ptLxdI6eVMxKzTpmpXkstUjU7ALaMRMtFS+Md5OTdwGlozNy84KKjCA6zHx+f6w1sNlsYM2e\ntfb67pn38/GYB3utvS7vvfZmf/b6ftfF3B0REZGO6pF3ABERqU4qICIiUhYVEBERKYsKiIiIlEUF\nREREyqICIiIiZVEBEakgM3vZzA6JH59rZlOTTFvGeg4ws3nl5hRJomfeAUS6K3e/OK1lmVkbsLO7\nvxQv+yFgt7SWL1KK9kCkSzGzLfLOkBOdESwVpwIiVcHMhpnZzWb2upm9YWZXxONPNLOHzOwyM1sG\nnG+R75pZo5m9ambXm1nfePqtzOx3ZrbMzN42s7+b2YD4uUlm9n9mtiL+9/gSOXY0s1VmVlMwbq84\n0xZm9mEzuzde/utm9vv2dZdY1vlm9ruC4Qlx5jfM7DtF0/6LmT0SZ15sZr8ws57xc/cDBjwTZ/+S\nmY0xs4UF83/UzGbH8z9rZkcVPHedmf3SzG6P53/UzHYq642SbkUFRIJnZj2A24GXgRHAUGBGwST7\nAP8EBgI/Ak4CJgJjgA8DfYBfxNOeCPSNl9EfOA14z8x6A/8NHO7ufYH9gKeKs7j7UuAR4IsFo48H\n/uTurURf5P8FDCZqQhoGXLCJl+fxa9wduAr4GjAE2D7O2K4V+I8486eBQ4BvxpnGxNN8zN37uvuf\nipbdE7gNuBMYAHwL+IOZfaRg+V8BzgdqgP8j2o4im6QCItVgNLAjcLa7r3b3D9z9kYLnF7v7Ve7e\n5u7vA18FLnP3JndfBZwLjI8LUQvRl/MuHnnS3d+Nl9MKfMzMtnb319x9Y53Q0+N1tBsP/BHA3f/P\n3e919zXu/ibwc6JCtjlfBG5z94fdvQX4HgXNUu7+hLs/Hmd+BZhaYrm2kWV/GtjG3S+Jc80mKsiF\ne1i3uPs/3L0N+ANQlyCzdHMqIFINhgNN8ZdbKQuLhocATQXDTUAvYBDwO+AuYIaZLTKzH5vZFnGh\n+QowGVhqZreZ2a4bWd/NwL5mNsjMxgCtcac1ZjbQzKbHy14O/B7YIcFrHFL4OuI8b7YPm9lH4kxL\n4+X+KOFyISq+xduoifX3cF4teLwK2DbhsqUbUwGRarAQGBHvQZRS3IG8BBhZMDySaM/jtfgX+EXu\nvgdRM9VRRM1duPvd7n4YUfPTC8BvSq7MfTkwi2jP43jWb077L6AN2MPda4AT2PieQaGlRIUSgLhJ\nbfuC568G5gGj4uWel3C5EG2P4UXjRgCLE84vUpIKiFSDx4m+YH9sZr3jjvD9NjH9dOAMM6s1s22J\nfq3PcPc2M6s3sz3jYvQuUWFpi/ccjo6/uFvi51o3s46JRE1PfywY3yeet9nMhgJnJXyN/wMcaWb7\nmVkv4AesXyD6ACvcfZWZfZRoT6nQq0T9PaX8HVhlZmebWU8zqweOjF+DSNlUQCR4cdPVUcBHgFeI\n9ki+vIlZriVqqnqAqEN4FVHHMUR7F/8DvAM8D8yOp+0BnEn0q3wZcBAbfkkXmhnnWeruzxaMvxD4\nJLCcqOP65uKXs5HXOBf4V6Iv9SVEzVeLCib5NvA1M1sB/Jr193og6qi/wczeMrPjipbdQrT9Phu/\ntl8CE9x9waYyiWyOZX1DKTMbB1xO9B90mrtfUvR8DdF/+FHAe8DX4/9Mm51XRETyk2kBiZsJXgTG\nEv2qmgOMd/f5BdP8BGh294viTssr3f3QJPOKiEh+sm7CGg0siA+nbCHa7T6maJrdgfsA3P0FoDY+\nsSvJvCIikpOsC8hQ1j98cBHrHzoI8DRwLICZjSY6OmRYwnlFRCQnIVxM8cfAf5vZE8CzwJNs+uiX\nDZiZOgFFRDrI3ZMeCl5S1nsgi4n2KNoNo+jYc3dvdvevu/ve7n4i0eUoXkoyb9FygvobM2ZM7hmU\nqetkCjWXMlVvpjRkXUDmADub2Ugz25LoxKuZhROY2Xbxce+Y2TeA+z26tMRm5w3Z1ltvnXeEDShT\nMiFmgjBzKVMyIWZKQ6ZNWO7eamanE521234o7jwzOzV62qcSXXDutxbdz+B54P9tat4s86Zp8ODB\neUfYgDIlE2ImCDOXMiUTYqY0ZN4H4u53ArsWjft1wePHip/f1LzVYtKkSXlH2IAyJRNiJggzlzIl\nE2KmNGR+ImElmJl3hdchIlIpZoYH3omeq9raWsxMfwH/DRkyJO+PyQYaGhryjlBSiLmUKZkQM6Uh\nhMN4M9PU1JTa0QaSDbNO/QASkRx16SaseBcth0SSlN4jkXyoCUtERHKjAiJSJNT26hBzKVMyIWZK\nQ9cpIM3NeScQWV9zMzz6qD6badH2DE7X6QMZNgzOPhsKzvi0U07p0u3rkydPZtiwYZx33nl5Rymb\nmeFTp+YdI32rV8NPfgJLlsCQIRt8NqWDVq+Gn/4Uli6FPfaABx+EPn3yTlXV0ugD6TpHYS1ZAvfc\nAwMH5p0kkZ122olp06ZxyCGHlL2Mq6++OsVEOXr88bwTpO+116LPZFtb1X02g/Taa7Awvjj33Lnw\n/POw7775ZpLwLkJY5kXB3D/xCfcVK7xQ9PLCVFtb6/fee+9Gn1+zZk0F02xaqSwdzbex6UN8j2bP\nnt35haxYEX0me/Uq+dnMLVfKKpZpxQr3j33M3cx9++3d33kn/0wdEGKm+P9ep757u04fSAd3adNo\nTi13GRMnTuSVV17hqKOOom/fvvzsZz+jqamJHj16cO211zJy5EjGjh0LwJe//GV23HFH+vXrR319\nPXPnzl27nJNOOonvf//7ANx///0MHz6cyy67jEGDBjF06FCuv/76jWZYsWIFJ598MkOGDGH48OF8\n73vfW9vc99vf/pYDDjiAM888kx122IELL7yw5Dh354c//CG1tbUMHjyYSZMmsWLFCoCNvp5uo0+f\n6DP5wANqbklDnz7w8MNw990wdChccUXeiQS60B7IxivsBtp/HPbsWf6Pw84uo7a21u+77761w42N\njW5mfuKJJ/qqVat89erV7u5+3XXX+cqVK/2DDz7wM844w+vq6tbOM2nSJP/e977n7u4NDQ3es2dP\nv+CCC3zNmjX+t7/9zXv37u3Lly8vuf7Pf/7zPnnyZH/vvff8jTfe8H322cenTp3q7u7XX3+99+zZ\n06+88kpvbW311atXlxw3bdo0/8hHPuKNjY2+cuVKP/bYY33ChAmbfD3FNvYeiWzU0qXuO+3kPm1a\n3kmqGinsgeT+5Z/GX0cLyCOPRF/8kM5fr17ujz668TeqlOImrMbGRu/Ro4c3NjZudJ63337bzcxX\nxNWquID07t3bW1tb104/cOBA//vf/77Bcl577TXfaqut1vtSnz59uh988MHuHhWQkSNHrjdPqXFj\nx471q6++eu3wCy+84L169fLW1tZEr8ddBUTK9MIL7oMHu99+e95JqlYaBaTrNGF1wJ57Rgdy9OoF\nn/gErFjR8bKxYkU0b69esPvu0fLSMGzYsLWP29ramDJlCjvvvDM1NTXstNNOmBnLli0rOe/2229P\njx7r3tLevXvz7rvvbjBdU1MTLS0t7LjjjvTv359+/fpx2mmnrbfc4cOHbzBf8bglS5YwcuTItcMj\nR45kzZo1vPbaayVfT7UI9Zj9EHPllmmXXeAvf4GTTtrgIAxtp8rpOkdhdUB78/Tzz0df/OU0T3d2\nGRu7BlTh+D/+8Y/cdttt3HfffYwYMYJ33nmHfv36te91lW348OFsvfXWvPnmm4lybGzckCFDaGpq\nWjvc1NREr169GDRoEAvjI2Z0rSvJzD77wHXXwTHHwP33R0VFKqpb7oFA9IW/776d69vszDIGDx7M\nSy+9tN644sLQ3NzMVlttRb9+/Vi5ciXnnntuKl/IgwcP5rDDDuOMM86gubkZd+ell17igQce6NBy\njj/+eH7+85/T2NjIu+++y3nnncf48ePX7gV1ttDlpb6+Pu8IJYWYK/dMn/scXHQRHHEEvPpqGJlK\nCDFTGrptAcnblClTuOiii+jfvz+XXXYZsOGv9YkTJzJixAiGDh3KnnvuyX777dehdWyq2Nxwww18\n8MEH7L777vTv358vfelLvBr/B0zq61//OhMmTOCggw5i1KhR9O7dmysKjo7R3odUxMknw4knRsVE\nZ6lXVNc5E73E69CVXsMX4nvU0NAQ5C/GEHMFk8kdTjsNXn6ZhrPOov4zn8k70XqC2U4FdDVeEREA\nM7jySvjQh6JLyLS15Z2oW8h8D8TMxgGXExWrae5+SdHzfYHfAyOALYBL3f36+LlG4B2gDWhx99Eb\nWYf2QKqU3iNJ1apVcOihcOCBcMklm5++G0tjDyTTAmJmPYAXgbHAEmAOMN7d5xdMcy7Q193PNbMd\ngBeAQe6+xsxeAj7p7m9vZj0qIFVK75Gk7s034YADYPJk+Na38k4TrGpowhoNLHD3JndvAWYAxxRN\n40D7cUx9gDfdfU08bBXIKLKeUI/ZDzFXkJmefRbuvDNqyrrpprzjAGFupzRk/eU8FFhYMLwoHlfo\nl8DuZrYEeBr494LnHLjbzOaY2TcyTSoiXcfIkfDXv8Lpp0MX/fIOQQgnEh4OPOnuh5jZKKKC8XF3\nfxfY392XmtmAePw8d3+o1EImTZpEbW0tADU1NdTV1VUqv3RS+6+z9qNU8h5uHxdKnpCH6+vrg8rT\nbu37N2MGDV/4Alx6KfVf/3qu+Qqz5bX+hoYGGhsbSUvWfSD7Ahe4+7h4eArR9VcuKZjmduBid384\nHr4XOMfd/7doWecDze5+WYn1qA+kSuk9kszNmAFnnRVdzXfEiLzTBKMa+kDmADub2Ugz2xIYD8ws\nmqYJOBTAzAYBuwAvmVlvM9s2Hr8NcBjwXMZ5RYJtrw4xV1VkGj8ezjwTxo2Dt94KI1MXkWkTlru3\nmtnpwCzWHcY7z8xOjZ72qcAPgevN7Jl4trPd/S0z2wm4xcw8zvkHd5+VZV4R6aLOOAMWL46umzVr\nVnS+iHSazkTPSRq3tIXo5k/XXHMNDz74YErJKivk90i6mLY2mDAB3nsP/vQn2GKLvBPlqhqasCRj\n7p7qNadaW1sTjevoMkRy16NHdPXeFSui80P0w6XTum8ByfGetqVuaQvw2GOPsf/++9OvXz/22msv\n7r///rXzXH/99YwaNYq+ffsyatQopk+fzvz585k8eTKPPvooffr0oX///iXXp9vXdkyo7dUh5qq6\nTFtuCX/+MzzyCFx8cRiZqlln70gVwh8dvCNhCPe0Lb6l7eLFi3377bf3O++8093d77nnHt9+++19\n2bJlvnLlSu/bt68vWLDA3d1fffVVnzt3rrtHdwo88MADN7muUG5fW8pG36MczZ49O+8IJYWYq2oz\nLVkS3Rb3uuuyjuPuYW4ndEvbMgtIAPe0Lb6l7SWXXOITJ05cb5rDDz/cb7jhBl+5cqX369fP//zn\nP/t777233jSbKyAh3b62lBALiHQT8+e7Dxrk/re/5Z0kF2kUkO7ZhBXgPW2bmpq46aab6N+//9rb\nzD788MMsXbqU3r17c+ONN3L11Vez4447ctRRR/HCCy8kXq5uXytSwq67wi23RPcSmTMn7zRVqXsW\nkPb70T7wQPRvZ+5pW+Yyiju+hw8fzsSJE3nrrbd46623ePvtt2lububss88G4DOf+QyzZs3i1Vdf\nZdddd+WUU04puZxihbevbV/u8uXLeeaZZ9ZO09nb125qOdUo1PbqEHNVfaZPfxquuQaOPhr++c8w\nMlWR7llAIPd72hbf0vaEE07gtttuY9asWbS1tbF69Wruv/9+lixZwuuvv87MmTNZtWoVvXr1Yttt\nt11729hBgwaxaNEiWlpaNroe3b5WZBOOPhp+8IPoRMOCPWpJoLNtYCH80dE+kADceuutPmLECO/X\nr59feuml7u7++OOP+5gxY7x///4+cOBAP/LII33hwoW+dOlSHzNmjNfU1Hi/fv384IMP9nnz5rm7\n+wcffOBHHnmk9+/f3wcMGFByXStWrPDJkyf7sGHDvKamxvfee2+/8cYb3b10H0qpcW1tbX7RRRf5\n8OHDfeDAgT5x4kRfvny5u/vaPpDW1tYOb4eQ3yPpZs4/3/2Tn3Rvbs47SUWQQh+ITiSUXOk9kmC4\nw6mnwiuvwG23Rf2bXZhOJBTJQKjt1SHm6lKZzOCqq6JzRU4+OdUTDUPcTmlQARERadezZ3T13hdf\nhO98J+80wVMTluRK75EEadky2H9/+Ld/i25K1QWl0YQVwg2lRETCssMOcNdd0b3VBw+G447LO1GQ\n1IQlUiTU9uoQc3XpTLW1cPvt8M1vRud7dUKI2ykNKiAiIhtTVwfTp8OXvgTP6X52xbp0H0htbe16\nZ09LeEaOHJnqPZpFMjF9OpxzTnRb3BKX/qlGafSBdOkCIiKSmksvhWuvhYcegn798k7TaToPJGAh\ntnkqUzIhZoIwc3WrTP/5n3D44dFtcVevDiNTzlRARESS+tnPYOhQOOEE0J03s2/CMrNxwOVExWqa\nu19S9Hxf4PfACGAL4FJ3vz7JvAXLUBOWiFTG++/DEUdEt3C44oroDPYqFHwfiJn1AF4ExgJLgDnA\neHefXzDNuUBfdz/XzHYAXgAGAW2bm7dgGSogIlI577wDBx0Exx8PU6bknaYs1dAHMhpY4O5N7t4C\nzACOKZrGgfbrofcB3nT3NQnnDVaIbZ7KlEyImSDMXN0203bbwR13wK9+BTfcEEamHGR9JvpQYGHB\n8CKiwlDol8BMM1sCbAt8pQPziojkY8iQqIgcfDAMHBjdT6SbCeFSJocDT7r7IWY2CrjbzD7e0YVM\nmjSJ2tpaAGpqaqirq6O+vh5YV/0rPdwur/VXw3B9fX1Qedo1NDQEkyfk4W7//u22Gw3f/S6MH0/9\nPffApz4V7PdB++M0z7vKug9kX+ACdx8XD08huonJJQXT3A5c7O4Px8P3AucQFbdNzluwDPWBiEh+\nbr0VJk+Obm89alTeaRKphj6QOcDOZjbSzLYExgMzi6ZpAg4FMLNBwC7ASwnnDVbxr44QKFMyIWaC\nMHMpU+yYY+D886NmrNdfDyNTBWTahOXurWZ2OjCLdYfizjOzU6OnfSrwQ+B6M3smnu1sd38LoNS8\nWeYVESnbqafC4sXwuc/B7Nmw7bZ5J8qcLmUiIpIWd/jGN6JCMnNm0LfFrYYmLBGR7sMsOrR3iy2i\nQtLFf9iqgGQkxDZPZUomxEwQZi5lKqFnT7jxRpg3D7773TAyZSSEw3hFRLqWbbaJbka1//7RtbN2\n3z3vRJlQH4iISFZefjm6Le4vfgHHHpt3mvXonugiIiHbaadoT+Tww2HAADjwwLwTpUp9IBkJsc1T\nmZIJMROEmUuZEthrLxrOPhuOOw6efz7vNKlSARERydqnPgWXXQaf/SwsWpR3mtSoD0REpFJ++tPo\n6r0PPgg1NblGCf5+IJWiAiIiVcEdzjgDnnwS7roLtt46tyg6kTBgwbXDokxJhZgJwsylTMmszWQW\nNWUNGgQTJkBbW665OksFRESkknr0iJqx3ngj2hup4tYTNWGJiORh+fLotrgnnABnn13x1es8EBGR\nalVTE93RcL/9YMcdoyatKqMmrIwE3Q4bEGVKLsRcypTMRjMNHRoVkW9/G2bNqmimNKiAiIjkaffd\n4eabo6asJ57IO02HqA9ERCQEt9wCp58enSPy4Q9nvjr1gYiIdBVf+AK8+mp0W9yHH46unRU4NWFl\npKraYXOkTMmFmEuZkkmcafJk+PKXo9virlyZaaY0qICIiITkootgzz2jQtLSkneaTcq8D8TMxgGX\nExWrae5+SdHz3wa+BjjQC9gN2MHdl5tZI/AO0Aa0uPvojaxDfSAi0nW0tMAxx0SH915zTXQGe8qC\nvxaWmfUAXgTGAkuAOcB4d5+/kemPBP7D3Q+Nh18CPunub29mPSogItK1rFwJBx8c3UvkootSX3w1\nXAtrNLDA3ZvcvQWYARyziemPB6YXDBtV2sxW1e2wFaRMyYWYS5mSKSvTNtvAX/8KM2bAr36VeqY0\nZP3lPBRYWDC8KB63ATP7EDAOuLlgtAN3m9kcM/tGZilFREI0YEB01d4f/AD+8pe802wgpMN4jwIe\ncvflBeP2d/elZjaAqJDMc/eHSs08adIkamtrAaipqaGuro76+npgXfWv9HC7vNZfDcP19fVB5WnX\n0NAQTJ6Qh/X+VeD74JVX4PzzqT/lFBgwgIa4Y72c9Tc0NNDY2Ehasu4D2Re4wN3HxcNTAC/uSI+f\n+zNwk7vP2Miyzgea3f2yEs+pD0REurZZs6LrZc2eHZ293knV0AcyB9jZzEaa2ZbAeGBm8URmth0w\nBri1YFxvM9s2frwNcBjwXMZ5U1P8qyMEypRMiJkgzFzKlEwqmQ47DH72MzjiCFi8uPPLS0GmTVju\n3mpmpwOzWHcY7zwzOzV62qfGk34euMvd3yuYfRBwi5l5nPMP7l59VxsTEUnLhAlR8TjiCHjggfxv\ni9sVmn7UhCUi3YY7fOtb8NxzcOedsNVWZS0m+PNAKkUFRES6ldZWGD8+urvh9OnRvx1UDX0g3VaX\nbYdNmTIlF2IuZUom9UxbbAG/+1108cUzz8zttrgqICIi1WjrraNzQ+65By69NJcIasISEalmixZF\nt8W9+GL42tcSz6b7gYiIdHfDhkW3xT3kEBg0CA49tGKrVhNWRrpFO2wKlCm5EHMpUzKZZ9pjD/if\n/4GvfhWefDLbdRVQARER6QoOPDC66OKRR8LLL1dkleoDERHpSq68Eq64Irot7g47bHQynQcSUwER\nESnwne/AfffBvfdGl4UvQeeBBKxbtsOWQZmSCzGXMiVT8Uw/+hF89KPRyYZr1mS2GhUQEZGuxgx+\n85uoeJx2WmYnGqoJS0Skq3r33ei2uJ/9LFx44XpP6TwQERHZuG23jW6Lu99+MGQInHpqqotP1IRl\nkRPM7Pvx8AgzG51qki5G7bDJKFNyIeZSpmRyzTRwYHTV3gsugFtv3ezkHZG0D+Qq4NPA8fFwM3Bl\nqklERCQbO+8MM2fCySfDI4+ktthEfSBm9oS7721mT7r7XvG4p939E6kl6QT1gYiIJHDHHXDSSdDQ\ngO22W8UO420xsy0ABzCzAUBbZ1YsIiIVdsQR8OMfR7fHTUHSAnIFcAsw0Mx+BDwE/FcqCbootcMm\no0zJhZhLmZIJKtMXvwgtLaksKtFRWO7+BzP7BzAWMODz7j4vlQQiIlI5zz0Hy5alsqikfSCjgEXu\n/r6Z1QMfB25w9+UJ5h0HXE60tzPN3S8pev7bwNeImsd6AbsBO7j78s3NW7AM9YGIiCTR3AwHHog9\n/XRlroVlZk8BnwJqgb8CM4E93P2zm5mvB/Ai0Z7LEmAOMN7d529k+iOB/3D3QzsyrwqIiEgHNDdj\nfftWrBO9zd3XAMcCv3T3s4AdE8w3Gljg7k3u3gLMAI7ZxPTHA9PLnDcoQbV5xpQpmRAzQZi5lCmZ\n4DL16ZPKYjpyFNbxwETg9nhcrwTzDQUWFgwvisdtwMw+BIwDbu7ovCIiUnlJL2VyEnAa8CN3f9nM\ndgJ+l3KWo4CHkvSrlDJp0iRqa2sBqKmpoa6ujvr6emBd9a/0cLu81l8Nw/X19UHladfQ0BBMnpCH\n9f5Vz/dB++PGxkbSkunFFM1sX+ACdx8XD08BvFRnuJn9GbjJ3WeUMa/6QEREOqBi9wMxsyPN7Ekz\ne8vMVphZs5mtSDDrHGBnMxtpZlsC44k64IuXvx0wBri1o/OGqvhXRwiUKZkQM0GYuZQpmRAzpSFp\nE9blRB3oz3bkp767t5rZ6cAs1h2KO8/MTo2e9qnxpJ8H7nL39zY3b9J1i4hItpIexjsbGOvuQV6+\nRE1YIiIdU8n7gZwN/M3M7gfebx/p7pd1ZuUiIlK9kh7G+yNgFbA10KfgTzYixDZPZUomxEwQZi5l\nSibETGlIugcyxN33zDSJiIhUlaR9ID8B7nH3WdlH6jj1gYiIdEwafSCbLSBmZkBrPPg+0EJ0RV53\n976dWXlaVEBERDqmIueBxN/Mc929h7t/yN37unufUIpHqEJs81SmZELMBGHmUqZkQsyUhqSd6P8w\ns3/JNImIiFSVpH0g84GdgSZgJeuasD6ebbxk1IQlItIxlTwP5PDOrERERLqeRE1Y8T05NvjLOlw1\nC7HNU5mSCTEThJlLmZIJMVMakvaBiIiIrCfTy7lXivpAREQ6pmKXcxcRESmmApKRENs8lSmZEDNB\nmLmUKZkQM6VBBURERMqiPhARkW5IfSAiIpIbFZCMhNjmqUzJhJgJwsylTMmEmCkNKiAiIlKWzPtA\nzGwccDlRsZrm7peUmKYe+DnQC3jD3Q+OxzcC7wBtQIu7j97IOtQHIiLSARW5H0inFm7WA3gRGAss\nAeYA4919fsE02wGPAIe5+2Iz28Hdl8XPvQR80t3f3sx6VEBERDqgGjrRRwML4mtntQAzgGOKpvkq\ncLO7LwZoLx4xq0DGTITY5qlMyYSYCcLMpUzJhJgpDVl/OQ8FFhYML4rHFdoF6G9ms81sjplNKHjO\ngbvj8d/IOKuIiHRA0su5Z6knsDdwCLAN8KiZPeru/wT2d/elZjaAqJDMc/eHSi1k0qRJ1NbWAlBT\nU0NdXR319fXAuupf6eF2ea2/Gobr6+uDytOuoaEhmDwhD+v9q57vg/bHjY2NpCXrPpB9gQvcfVw8\nPIXoRlSXFExzDrC1u18YD18D3OHuNxct63yg2d0vK7Ee9YGIiHRANfSBzAF2NrORZrYlMB6YWTTN\nrcABZraFmfUG9gHmmVlvM9sWwMy2AQ4Dnss4b2qKf3WEQJmSCTEThJlLmZIJMVMaMm3CcvdWMzsd\nmMW6w3jnmdmp0dM+1d3nm9ldwDNAKzDV3eea2U7ALWbmcc4/uPusLPOKiEhyuhaWiEg3VA1NWCIi\n0kWpgGSPqgTtAAAMUklEQVQkxDZPZUomxEwQZi5lSibETGlQARERkbKoD0REpBtSH4iIiORGBSQj\nIbZ5KlMyIWaCMHMpUzIhZkqDCoiIiJRFfSAiIt2Q+kBERCQ3KiAZCbHNU5mSCTEThJlLmZIJMVMa\nVEBERKQs6gMREemG1AciIiK5UQHJSIhtnsqUTIiZIMxcypRMiJnSoAIiIiJlUR+IiEg3pD4QERHJ\njQpIRkJs81SmZELMBGHmUqZkQsyUBhUQEREpS+Z9IGY2DricqFhNc/dLSkxTD/wc6AW84e4HJ503\nnk59ICIiHZBGH0imBcTMegAvAmOBJcAcYLy7zy+YZjvgEeAwd19sZju4+7Ik8xYsQwVERKQDqqET\nfTSwwN2b3L0FmAEcUzTNV4Gb3X0xgLsv68C8wQqxzVOZkgkxE4SZS5mSCTFTGrIuIEOBhQXDi+Jx\nhXYB+pvZbDObY2YTOjCviIjkpGfeAYgy7A0cAmwDPGpmj3Z0IZMmTaK2thaAmpoa6urqqK+vB9ZV\n/0oPt8tr/dUwXF9fH1Sedg0NDcHkCXlY71/1fB+0P25sbCQtWfeB7Atc4O7j4uEpgBd2hpvZOcDW\n7n5hPHwNcAeweHPzFixDfSAiIh1QDX0gc4CdzWykmW0JjAdmFk1zK3CAmW1hZr2BfYB5CecNVvGv\njhAoUzIhZoIwcylTMiFmSkOmTVju3mpmpwOzWHco7jwzOzV62qe6+3wzuwt4BmgFprr7XIBS82aZ\nV0REktO1sEREuqFqaMISEZEuSgUkIyG2eSpTMiFmgjBzKVMyIWZKgwqIiIiURX0gIiLdkPpAREQk\nNyogGQmxzVOZkgkxE4SZS5mSCTFTGlRARESkLOoDERHphtQHIiIiuVEByUiIbZ7KlEyImSDMXMqU\nTIiZ0qACIiIiZVEfiIhIN6Q+EBERyY0KSEZCbPNUpmRCzARh5lKmZELMlAYVEBERKYv6QEREuiH1\ngYiISG5UQDISYpunMiUTYiYIM5cyJRNipjRkXkDMbJyZzTezF83snBLPjzGz5Wb2RPz33YLnGs3s\naTN70swezzqriIgkl2kfiJn1AF4ExgJLgDnAeHefXzDNGOA/3f3oEvO/BHzS3d/ezHrUByIi0gHV\n0AcyGljg7k3u3gLMAI4pMd3GXoShZjYRkSBl/eU8FFhYMLwoHlfs02b2lJn91cx2LxjvwN1mNsfM\nvpFl0LSF2OapTMmEmAnCzKVMyYSYKQ098w4A/AMY4e6rzOwI4C/ALvFz+7v7UjMbQFRI5rn7Q7kl\nFRGRtbIuIIuBEQXDw+Jxa7n7uwWP7zCzq8ysv7u/5e5L4/FvmNktRE1iJQvIpEmTqK2tBaCmpoa6\nujrq6+uBddW/0sPt8lp/NQzX19cHladdQ0NDMHlCHtb7Vz3fB+2PGxsbSUvWnehbAC8QdaIvBR4H\njnf3eQXTDHL31+LHo4Gb3L3WzHoDPdz9XTPbBpgFXOjus0qsR53oIiIdEHwnuru3AqcTffk/D8xw\n93lmdqqZnRJPdpyZPWdmTwKXA1+Jxw8CHorHPwbcVqp4hKr4V0cIlCmZEDNBmLmUKZkQM6Uh8z4Q\nd78T2LVo3K8LHl8JXFlivpeBuqzziYhIeXQtLBGRbij4JiwREem6VEAyEmKbpzIlE2ImCDOXMiUT\nYqY0qICIiEhZ1AciItINqQ9ERERyowKSkRDbPJUpmRAzQZi5lCmZEDOlQQVERETKoj4QEZFuSH0g\nIiKSGxWQjITY5qlMyYSYCcLMpUzJhJgpDSogIiJSFvWBiIh0Q+oDERGR3KiAZCTENk9lSibETBBm\nLmVKJsRMaVABERGRsqgPRESkG1IfiIiI5EYFJCMhtnkqUzIhZoIwcylTMiFmSkPmBcTMxpnZfDN7\n0czOKfH8GDNbbmZPxH/fTTpvoebmLNKX76mnnso7wgY2l6m5GR59tLLbshq3U1Jpb8+uvK2SSLo9\nu/t2qqSeWS7czHoAvwTGAkuAOWZ2q7vPL5r0AXc/usx5Adh7b7jqKthmm9RfRlmefXY5jzySd4r1\nbSrTypXwzW9CYyPU1lZuW1bbdkoqi+3ZVbdVEsXb8+qrYdttwWzdH0T/zp27nCee2HB8qWmTjO/s\ntACLFy9n2bLKrCuJtH7UZFpAgNHAAndvAjCzGcAxQHERKPXSk84LwD//CWeeCX36pBW9cxYsaGTe\nvLxTrG9TmZqbo20Ild2W1badkspie3bVbZVE8fb893+PCoh79AfrHr/8ciNz5mw4vtS0mxvf2Wnb\nx61Y0ci112a3rmKbKjYALS3lvQ/Fsi4gQ4GFBcOLiApDsU+b2VPAYuAsd5/bgXkB+MQn4MEHwykg\ndXVPBfdrcVOZmpvhwANh7lzYfffKbctq205JZbE9u+q2SqIj27Ou7imefDL7TB1RV/cUWbdiJS02\njz0Gn/kMrFnT+XVmXUCS+Acwwt1XmdkRwF+AXTq6kKefNvr2TT1bp1hH9ikrJEmmp5+motuyWrdT\nUmluz66+rZJIsj21nSoj6wKyGBhRMDwsHreWu79b8PgOM7vKzPonmbdgvq73zoiIBC7ro7DmADub\n2Ugz2xIYD8wsnMDMBhU8Hk10cuNbSeYVEZH8ZLoH4u6tZnY6MIuoWE1z93lmdmr0tE8FjjOzyUAL\n8B7wlU3Nm2VeERFJrktcykRERCpPZ6KLiEhZVEBERKQsXa6AmNlOZnaNmd1UMG6MmT1gZleb2UGB\nZPponOcmMzstkEwbjMuDme1mZjea2ZVm9sU8sxQyswPi9+w3ZvZQ3nkg/892KXl/tksJ5bNdKNBM\nHXrvulwBcfeX3f3k4tFAM7AV0QmJuWdy9/nuPpnooIH9AslUatvl4QjgCnf/V2Bi3mHauftD8Xt2\nO/DbvPPEcv1sl5L3Z7uUgD7bawWaqUPvXbAFxMymmdlrZvZM0fjEF1hs5+4PuPvngCnAD0LIFM93\nFNGX0d9CyZSmTmT7HTDezH4C9A8oV7uvAn8MIVNan+00M8XTdPqznXamrIWYrZxMHXrv3D3IP+AA\noA54pmBcD+CfwEigF/AU8NH4uQnAZcCO8fCfSixzS+CmkDLF428PKdPGcuaQrQdwS0ifLWA48OuQ\nMqXx2c4iU2c/25X6vOeVrWCa4DIlfe8yCZ3iix9Z9ML3Be4oGJ4CnFM0T3/gamBB+3PAF4BfAdOB\ngwLJNAb47zjX5EAybTAup/dwJPBroj2R/UL5bMXjLwD2DSVTmp/tFDOl9tlOMVPqn+3OZgs0U4fe\nuxCuhdURm73AokdnsU8uGncLcEtgme4H7g8s0wbjcsrWBJyacY5iiS7e6e4XVCoQybZVlp/tcjNl\n+dkuN1MlPtulbDRboJk69N4F2wciIiJhq7YCkvgCixWkTB0TarYQcylTMiFmahdittQyhV5AjPVv\nNhXCBRaVqWNCzRZiLmWq3kztQsyWXaasOm5S6Pj5I9GtbN8HXgFOiscfAbxA1PE0RZnCyxR6thBz\nKVP1Zgo5W9aZdDFFEREpS+hNWCIiEigVEBERKYsKiIiIlEUFREREyqICIiIiZVEBERGRsqiAiIhI\nWVRAREows+aUlnO+mZ2ZYLrrzOzYNNYpUikqICKl6Qxbkc1QARHZBDPbxszuMbP/NbOnzezoePxI\nM5sX7zm8YGa/N7OxZvZQPPypgsXUmdkj8fiTC5b9y3gZs4CBBeO/Z2Z/N7NnzOxXlXu1Ih2jAiKy\naauBz7v7p4BDgEsLnhsF/NTddwU+Chzv7gcAZwHnFUz3MaCe6B7T3zezwWb2BeAj7r4bcCLr33/6\nF+6+j7t/HOhtZp/L6LWJdIoKiMimGXCxmT0N3AMMMbP2vYWX3X1u/Ph54N748bNEd4Frd6u7f+Du\nbwL3AfsABxHdRRB3XxqPbzfWzB6L72N9MLBHBq9LpNOq7Y6EIpX2NWAHYC93bzOzl4Gt4+feL5iu\nrWC4jfX/bxX2p1j8fElmthVwJbC3uy8xs/ML1icSFO2BiJTWfv+E7YDX4+JxMOvvWdiGs5V0jJlt\naWbbE91zeg7wAPAVM+thZjsS7WlAVCwceNPMtgWO6+wLEcmK9kBESmvfa/gDcFvchPW/wLwS0xQ/\nLvYM0ABsD/zA3V8FbjGzQ4iavl4BHgFw93fM7Jp4/FLg8c6/FJFs6H4gIiJSFjVhiYhIWVRARESk\nLCogIiJSFhUQEREpiwqIiIiURQVERETKogIiIiJl+f+7nLQ4a8U/NQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25edce042b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from regularized_logistic_regression import cross_validation_regularized_logistic_regression\n",
    "\n",
    "min_rlr_rmse,best_rlr_lambda = cross_validation_regularized_logistic_regression(y,tX,k_fold=5)\n",
    "print('min_rlr_rmse: ',min_rlr_rmse)\n",
    "print('best lambda: ', best_rlr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from regularized_logistic_regression import regularized_logistic_regression_gradient_descent\n",
    "from build_polynomial import find_best_poly,build_optimal \n",
    "from regularized_logistic_regression import test_RLR\n",
    "\n",
    "best_deg = find_best_poly(y, tX_train[:,1:],test_RLR)\n",
    "rlr_opt_tr = build_optimal(tX_train[:,1:], best_deg)\n",
    "print('execute with best lambda: ', best_rlr_lambda)\n",
    "rlr_loss, rlr_w = regular_logistic_regression_gradient_descent(y, rlr_opt_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle score Aproximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters w:  [-0.03574765 -0.29307716 -0.25622237  0.02914541  0.01437547  0.15190617\n",
      "  0.22636287  0.28250279 -0.09065928  0.14286233 -0.14485509  0.04698598\n",
      "  0.18770091  0.1328155  -0.10799591  0.04080291  0.04158353 -0.05181993\n",
      " -0.10623274  0.09005564 -0.14231326 -0.00480513 -0.06340605  0.09870042\n",
      "  0.09999228 -0.16019252  0.08644647 -0.17649949 -0.11447075  0.12312552]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gradient_descent import test_GD\n",
    "test_GD(y, tX, ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters w:  [-0.05156778 -0.18065851 -0.09073065 -0.00239607  0.02824679  0.05645477\n",
      "  0.01136668  0.14440678 -0.10286569  0.02112294 -0.1271831   0.07590447\n",
      "  0.08419775  0.05606966 -0.04511965  0.08109771 -0.13074    -0.03614835\n",
      " -0.08166958 -0.03843294 -0.11223096 -0.02342437 -0.01267796  0.05771319\n",
      "  0.08870854 -0.07798771  0.04068466 -0.08403514 -0.05952428  0.04321297]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.675"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stochastic_gradient_descent import test_SGD\n",
    "test_SGD(y, tX, ratio=0.2,max_iters=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.675"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from least_squares import test_LS\n",
    "test_LS(y, tX, ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.675"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ridge_regression import test_RR\n",
    "print('execute with best lambda: ',best_rr_lambda)\n",
    "test_RR(y, rr_opt_tr, ratio=0.2,lambda_=best_rr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from logistic_regression import test_LR\n",
    "test_LR(y, lr_opt_tr,ratio=0.2, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from regularized_logistic_regression import test_RLR\n",
    "test_RLR(y, tX, ratio=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "weights = gradient_ws[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "weights = stoch_gradient_ws[min_stoch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Least Squares\n",
    "weights = ls_wopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "weights = w_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "weights = lr_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weights ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = \"../Data/test.csv\" # TODO: download test data and supply path here \n",
    "y_test, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test, _, _ = standardize_outliers(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../Data/results.csv' # TODO: fill in desired name of output file for submission\n",
    "# y_pred = predict_logistic_labels(weights, tX_test,threshold=0.5)\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from poly import build_optimal\n",
    "\n",
    "OUTPUT_PATH = '../Data/results.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "tx = tX_test[:,1:]\n",
    "opt_tr = build_optimal(tx, best_deg)\n",
    "\n",
    "y_pred = predict_labels(w, opt_tr)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
