{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCML Project-1 ~ Team #60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data with dimensions  (250000, 30)\n"
     ]
    }
   ],
   "source": [
    "DATA_TRAIN_PATH = \"../Data/train.csv\"\n",
    "y_train, tx_train, _ = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "print(\"Loaded training data with dimensions \", tx_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful information on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count outliers - Extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data_preparation import count_outliers\n",
    "outliers = count_outliers(tx_train, -999)\n",
    "for feature in range(tx_train.shape[1]):\n",
    "    print('feature: ',feature,' -> ',outliers[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of output y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(y_train, bins=2, align='mid')\n",
    "plt.title(\"Histogram of output y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of y as a function of all its features (one by one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data_preparation import plot_features_by_y\n",
    "plot_features_by_y(y_train,tx_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank Deficiency of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_rank = np.linalg.matrix_rank(tx_train)\n",
    "print('original tx_train rank: ',original_rank)\n",
    "# indices = np.arange(tx_train.shape[1])\n",
    "# for col in range(30):\n",
    "#     indices = np.delete(indices,col)\n",
    "#     rank = np.linalg.matrix_rank(tx_train[:,indices])\n",
    "#     indices = np.insert(indices,col,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our matrix-rank is 30 which means that we do not have any ill-conditioning between our columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tx standardized shape:  (250000, 30)\n"
     ]
    }
   ],
   "source": [
    "from data_preparation import standardize_outliers\n",
    "# Standardize the data and replace undefined values with the mean, column by column\n",
    "tx_train, _, _ = standardize_outliers(tx_train, -999)\n",
    "print('tx standardized shape: ',tx_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from quadratic_array import *\n",
    "indices = select_features(tx_train, y_train, 0.05)\n",
    "tx_train = tx_train[:,indices]\n",
    "indices, len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only run if we don't run build_poly_by_feature (adds the ones column in front)\n",
    "tx_train = np.c_[np.ones(tx_train.shape[0]), tx_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from feature_selection import best_feature_degrees\n",
    "from implementations import least_squares\n",
    "\n",
    "best_degrees = best_feature_degrees(y_train, tx_train, least_squares, max_degree=12)\n",
    "print(best_degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from feature_selection import build_poly_by_feature\n",
    "tx_train = build_poly_by_feature(tx_train, best_degrees)\n",
    "\n",
    "print(\"Created expanded data with shape \", tx_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = {} # Weight vector with the best score of each method\n",
    "s = {} # Best score for each method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.74104\n",
      "0.141253754462 0.7425\n",
      "0.199526231497 0.743476\n",
      "0.281838293126 0.74406\n",
      "0.398107170553 0.373304\n",
      "best gamma:  0.281838293126\n",
      "Estimated leaderboard score:  0.74406\n"
     ]
    }
   ],
   "source": [
    "from cross_validation import test_GD\n",
    "from implementations import least_squares_GD\n",
    "\n",
    "# Find the best gamma for gradient descent\n",
    "gammas = np.logspace(-1, -0.4, num=5)\n",
    "best_loss = 1000000 # TODO change\n",
    "best_score = 0\n",
    "for gamma in gammas:\n",
    "    loss, score = test_GD(y_train, tx_train, gamma)\n",
    "    #if loss < best_loss:\n",
    "    #    best_loss = loss\n",
    "    #    best_gamma = gamma\n",
    "    #    best_score = score\n",
    "    print(gamma, score)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_gamma = gamma\n",
    "        best_loss = loss\n",
    "\n",
    "print('best gamma: ', best_gamma)\n",
    "print(\"Estimated leaderboard score: \", best_score)\n",
    "w[\"gd\"], _ = least_squares_GD(y_train, tx_train, np.zeros(tx_train.shape[1]), 100, best_gamma)\n",
    "s[\"gd\"] = best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.74104\n",
      "0.141253754462 0.7425\n",
      "0.199526231497 0.743476\n",
      "0.281838293126 0.74406\n",
      "0.398107170553 0.71282\n",
      "best gamma:  0.281838293126\n",
      "Estimated leaderboard score:  0.74406\n"
     ]
    }
   ],
   "source": [
    "from cross_validation import test_SGD\n",
    "from implementations import least_squares_SGD\n",
    "\n",
    "# Find the best gamma for stochastic gradient descent\n",
    "gammas = np.logspace(-1, -0.4, num=5)\n",
    "best_loss = 1000000 # TODO change\n",
    "best_score = 0\n",
    "for gamma in gammas:\n",
    "    loss, score = test_SGD(y_train, tx_train, gamma)\n",
    "    #if loss < best_loss:\n",
    "    #    best_loss = loss\n",
    "    #    best_gamma = gamma\n",
    "    #    best_score = score\n",
    "    print(gamma, score)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_gamma = gamma\n",
    "        best_loss = loss\n",
    "\n",
    "print('best gamma: ', best_gamma)\n",
    "print(\"Estimated leaderboard score: \", best_score)\n",
    "w[\"sgd\"], _ = least_squares_SGD(y_train, tx_train, np.zeros(tx_train.shape[1]), 100, best_gamma)\n",
    "s[\"sgd\"] = best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated leaderboard score:  0.744388\n"
     ]
    }
   ],
   "source": [
    "from cross_validation import test_LS\n",
    "from implementations import least_squares\n",
    "\n",
    "# Compute the score we get with least squares\n",
    "_, score = test_LS(y_train, tx_train)\n",
    "\n",
    "print(\"Estimated leaderboard score: \", score)\n",
    "w[\"ls\"], _ = least_squares(y_train, tx_train)\n",
    "s[\"ls\"] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-15 0.744388\n",
      "1e-14 0.744388\n",
      "1e-13 0.74438\n",
      "1e-12 0.74438\n",
      "1e-11 0.744396\n",
      "1e-10 0.744364\n",
      "1e-09 0.744356\n",
      "1e-08 0.744356\n",
      "1e-07 0.74436\n",
      "1e-06 0.74436\n",
      "1e-05 0.744372\n",
      "0.0001 0.744356\n",
      "0.001 0.744304\n",
      "0.01 0.743012\n",
      "0.1 0.736068\n",
      "Estimated leaderboard score:  0.744396\n",
      "1e-11\n"
     ]
    }
   ],
   "source": [
    "from cross_validation import test_RR\n",
    "from implementations import ridge_regression\n",
    "\n",
    "# Find the best lambda for ridge regression\n",
    "lambdas = np.logspace(-15, -1, num=15)\n",
    "best_loss = 1000000 # TODO change\n",
    "best_score = 0\n",
    "for lambda_ in lambdas:\n",
    "    loss, score = test_RR(y_train, tx_train, lambda_)\n",
    "    #if loss < best_loss:\n",
    "    #    best_loss = loss\n",
    "    #    best_lambda = lambda_\n",
    "    #    best_score = score\n",
    "    print(lambda_, score)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_lambda = lambda_\n",
    "        best_loss = loss\n",
    "    \n",
    "print(\"Estimated leaderboard score: \", best_score)\n",
    "print(best_lambda)\n",
    "w[\"rr\"], _ = ridge_regression(y_train, tx_train, best_lambda)\n",
    "s[\"rr\"] = best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/work/Documents/PCML/Project/scripts/costs.py:18: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-t))\n",
      "/Users/work/Documents/PCML/Project/scripts/costs.py:14: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(2*mse)\n",
      "/Users/work/Documents/PCML/Project/scripts/costs.py:32: RuntimeWarning: overflow encountered in exp\n",
      "  return (1/N) * np.sum(np.log(1 + np.exp(tx.dot(w))) - y * tx.dot(w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated leaderboard score:  0.708524\n",
      "best_gamma:  1e-05\n"
     ]
    }
   ],
   "source": [
    "from cross_validation import test_LR\n",
    "from implementations import logistic_regression\n",
    "\n",
    "# Find the best gamma for logistic regression\n",
    "gammas = np.logspace(-5, 3, num=15)\n",
    "best_loss = 1000000 # TODO change\n",
    "best_score = 0\n",
    "for gamma in gammas:\n",
    "    loss, score = test_LR(y_train, tx_train, gamma)\n",
    "    #if loss < best_loss:\n",
    "    #    best_loss = loss\n",
    "    #    best_gamma = gamma\n",
    "    #    best_score = score\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_gamma = gamma\n",
    "        best_loss = loss\n",
    "    \n",
    "print(\"Estimated leaderboard score: \", best_score)\n",
    "print('best_gamma: ',best_gamma)\n",
    "w[\"lr\"], _ = logistic_regression(y_train, tx_train, np.zeros(tx_train.shape[1]), 1000, best_gamma)\n",
    "s[\"lr\"] = best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/work/Documents/PCML/Project/scripts/costs.py:18: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-t))\n",
      "/Users/work/Documents/PCML/Project/scripts/costs.py:32: RuntimeWarning: overflow encountered in exp\n",
      "  return (1/N) * np.sum(np.log(1 + np.exp(tx.dot(w))) - y * tx.dot(w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.1 0.708468\n",
      "0.01 0.316227766017 0.70846\n",
      "0.01 1.0 0.708536\n",
      "0.01 3.16227766017 0.708644\n",
      "0.01 10.0 0.708784\n",
      "0.0237137370566 0.1 0.708448\n",
      "0.0237137370566 0.316227766017 0.708516\n",
      "0.0237137370566 1.0 0.70864\n",
      "0.0237137370566 3.16227766017 0.708808\n",
      "0.0237137370566 10.0 0.708724\n",
      "0.056234132519 0.1 0.708512\n",
      "0.056234132519 0.316227766017 0.708588\n",
      "0.056234132519 1.0 0.708852\n",
      "0.056234132519 3.16227766017 0.708824\n",
      "0.056234132519 10.0 0.706976\n",
      "0.133352143216 0.1 0.708564\n",
      "0.133352143216 0.316227766017 0.708704\n",
      "0.133352143216 1.0 0.708756\n",
      "0.133352143216 3.16227766017 0.707668\n",
      "0.133352143216 10.0 0.702956\n",
      "0.316227766017 0.1 0.708644\n",
      "0.316227766017 0.316227766017 0.708784\n",
      "0.316227766017 1.0 0.7083\n",
      "0.316227766017 3.16227766017 0.704704\n",
      "0.316227766017 10.0 0.698572\n",
      "Estimated leaderboard score:  0.708852\n",
      "best lambda:  1.0\n",
      "best gamma:  0.056234132519\n"
     ]
    }
   ],
   "source": [
    "from cross_validation import test_RLR\n",
    "from implementations import reg_logistic_regression\n",
    "\n",
    "# Find the best lambda and gamma for regularized logistic regression\n",
    "gammas = np.logspace(-2, -0.5, 5) #100\n",
    "lambdas = np.logspace(-1, 1, 5)\n",
    "best_loss = 10000 # TODO change\n",
    "best_score = 0\n",
    "ii = 0\n",
    "for gamma in gammas:\n",
    "    for lambda_ in lambdas:\n",
    "        loss, score = test_RLR(y_train, tx_train, lambda_, gamma)\n",
    "        print(gamma, lambda_, score)\n",
    "        #if loss < best_loss:\n",
    "        #    best_loss = loss\n",
    "        #    best_gamma = gamma\n",
    "        #    best_lambda = lambda_\n",
    "        #    best_score = score\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_gamma = gamma\n",
    "            best_lambda = lambda_\n",
    "            best_loss = loss\n",
    "    \n",
    "print(\"Estimated leaderboard score: \", best_score)\n",
    "print('best lambda: ',best_lambda)\n",
    "print('best gamma: ',best_gamma)\n",
    "w[\"rlr\"], _ = reg_logistic_regression(y_train, tx_train, best_lambda, np.zeros(tx_train.shape[1]), 50, best_gamma)\n",
    "s[\"rlr\"] = best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose the best method based on the score\n",
    "best = 0\n",
    "for method, score in s.items():\n",
    "    if score > best:\n",
    "        best = score\n",
    "        best_method = method\n",
    "\n",
    "print(best_method)\n",
    "print(s[best_method])\n",
    "weights = w[best_method] # Weight vector of the best method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data_preparation import standardize_outliers\n",
    "from feature_selection import build_poly_by_feature\n",
    "\n",
    "DATA_TEST_PATH = \"../Data/test.csv\"\n",
    "_, tx_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tx_test, _, _ = standardize_outliers(tx_test, -999)\n",
    "# Build test data with the same shape as the training data\n",
    "tx_test = build_poly_by_feature(tx_test, best_degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if best_method in [ \"lr\", \"rlr\" ]:\n",
    "    y_pred = predict_logistic_labels(weights, tx_test, threshold = 0.5)\n",
    "else:\n",
    "    y_pred = predict_labels(weights, tx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"../Data/results.csv\"\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
