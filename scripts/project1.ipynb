{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCML Project-1 ~ Team #60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from test import *\n",
    "\n",
    "from costs import compute_loss\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape:  (200,)\n",
      "original tX shape:  (200, 30)\n",
      "ids shape:  (200,)\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "\n",
    "DATA_TRAIN_PATH = \"../Data/train.csv\" # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "#print the shape of the offset x matrix.\n",
    "print('y shape: ',y.shape)\n",
    "print('original tX shape: ',tX.shape)\n",
    "print('ids shape: ',ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Outliers - Extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature:  0  ->  35.0\n",
      "feature:  1  ->  0.0\n",
      "feature:  2  ->  0.0\n",
      "feature:  3  ->  0.0\n",
      "feature:  4  ->  144.0\n",
      "feature:  5  ->  144.0\n",
      "feature:  6  ->  144.0\n",
      "feature:  7  ->  0.0\n",
      "feature:  8  ->  0.0\n",
      "feature:  9  ->  0.0\n",
      "feature:  10  ->  0.0\n",
      "feature:  11  ->  0.0\n",
      "feature:  12  ->  144.0\n",
      "feature:  13  ->  0.0\n",
      "feature:  14  ->  0.0\n",
      "feature:  15  ->  0.0\n",
      "feature:  16  ->  0.0\n",
      "feature:  17  ->  0.0\n",
      "feature:  18  ->  0.0\n",
      "feature:  19  ->  0.0\n",
      "feature:  20  ->  0.0\n",
      "feature:  21  ->  0.0\n",
      "feature:  22  ->  0.0\n",
      "feature:  23  ->  87.0\n",
      "feature:  24  ->  87.0\n",
      "feature:  25  ->  87.0\n",
      "feature:  26  ->  144.0\n",
      "feature:  27  ->  144.0\n",
      "feature:  28  ->  144.0\n",
      "feature:  29  ->  0.0\n"
     ]
    }
   ],
   "source": [
    "outliers = count_outliers(tX,-999)\n",
    "for feature in range(tX.shape[1]):\n",
    "    print('feature: ',feature,' -> ',outliers[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 30\n",
      "(200, 31)\n",
      "standardized tX shape:  (200, 30)\n"
     ]
    }
   ],
   "source": [
    "#standardization\n",
    "# tX, mean_x, std_x = standardize(tX, mean_x=None, std_x=None)\n",
    "tX_train, _, _ = standardize_outliers(tX)\n",
    "print('standardized tX shape: ',tX.shape)\n",
    "# print('tX mean shape: ',mean_x.shape)\n",
    "# print('tX std shape: ',std_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of output y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Histogram of output y\n",
    "plt.hist(y, bins=10, align='mid')\n",
    "plt.title(\"Histogram of output y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of y as a function of all its features (one by one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Analyse y as a function of all the other features (one by one)\n",
    "plot_features_by_y(y,tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape:  (250000,)\n",
      "tx shape:  (250000, 30)\n",
      "Gradient Descent(0/999): loss=0.5\n",
      "Gradient Descent(1/999): loss=0.4960259021131301\n",
      "Gradient Descent(2/999): loss=0.4923109658542728\n",
      "Gradient Descent(3/999): loss=0.4888317608849261\n",
      "Gradient Descent(4/999): loss=0.48556733695387544\n",
      "Gradient Descent(5/999): loss=0.4824989446355241\n",
      "Gradient Descent(6/999): loss=0.4796097882241068\n",
      "Gradient Descent(7/999): loss=0.4768848070514233\n",
      "Gradient Descent(8/999): loss=0.4743104819302093\n",
      "Gradient Descent(9/999): loss=0.4718746638091148\n",
      "Gradient Descent(10/999): loss=0.46956642206437404\n",
      "Gradient Descent(11/999): loss=0.4673759101528658\n",
      "Gradient Descent(12/999): loss=0.46529424661594687\n",
      "Gradient Descent(13/999): loss=0.4633134096573078\n",
      "Gradient Descent(14/999): loss=0.4614261437247118\n",
      "Gradient Descent(15/999): loss=0.4596258767080175\n",
      "Gradient Descent(16/999): loss=0.45790664652717655\n",
      "Gradient Descent(17/999): loss=0.4562630360263872\n",
      "Gradient Descent(18/999): loss=0.45469011521649483\n",
      "Gradient Descent(19/999): loss=0.4531833900189712\n",
      "Gradient Descent(20/999): loss=0.4517387567630904\n",
      "Gradient Descent(21/999): loss=0.4503524617747762\n",
      "Gradient Descent(22/999): loss=0.44902106547232845\n",
      "Gradient Descent(23/999): loss=0.447741410452045\n",
      "Gradient Descent(24/999): loss=0.4465105931066774\n",
      "Gradient Descent(25/999): loss=0.44532593837260087\n",
      "Gradient Descent(26/999): loss=0.4441849772483714\n",
      "Gradient Descent(27/999): loss=0.4430854267686862\n",
      "Gradient Descent(28/999): loss=0.44202517215430526\n",
      "Gradient Descent(29/999): loss=0.4410022508907835\n",
      "Gradient Descent(30/999): loss=0.44001483851740336\n",
      "Gradient Descent(31/999): loss=0.4390612359329155\n",
      "Gradient Descent(32/999): loss=0.4381398580469998\n",
      "Gradient Descent(33/999): loss=0.43724922362605406\n",
      "Gradient Descent(34/999): loss=0.436387946199341\n",
      "Gradient Descent(35/999): loss=0.43555472590692196\n",
      "Gradient Descent(36/999): loss=0.43474834218441577\n",
      "Gradient Descent(37/999): loss=0.4339676471916548\n",
      "Gradient Descent(38/999): loss=0.43321155990295196\n",
      "Gradient Descent(39/999): loss=0.43247906078609555\n",
      "Gradient Descent(40/999): loss=0.4317691870055148\n",
      "Gradient Descent(41/999): loss=0.4310810280924045\n",
      "Gradient Descent(42/999): loss=0.43041372203111383\n",
      "Gradient Descent(43/999): loss=0.42976645171685113\n",
      "Gradient Descent(44/999): loss=0.42913844174485166\n",
      "Gradient Descent(45/999): loss=0.4285289554956544\n",
      "Gradient Descent(46/999): loss=0.4279372924851216\n",
      "Gradient Descent(47/999): loss=0.42736278595135724\n",
      "Gradient Descent(48/999): loss=0.42680480065380677\n",
      "Gradient Descent(49/999): loss=0.42626273086256994\n",
      "Gradient Descent(50/999): loss=0.42573599851842087\n",
      "Gradient Descent(51/999): loss=0.42522405154617643\n",
      "Gradient Descent(52/999): loss=0.4247263623059868\n",
      "Gradient Descent(53/999): loss=0.4242424261688065\n",
      "Gradient Descent(54/999): loss=0.42377176020381957\n",
      "Gradient Descent(55/999): loss=0.42331390196691876\n",
      "Gradient Descent(56/999): loss=0.42286840838052325\n",
      "Gradient Descent(57/999): loss=0.42243485469606756\n",
      "Gradient Descent(58/999): loss=0.42201283353141933\n",
      "Gradient Descent(59/999): loss=0.42160195397631534\n",
      "Gradient Descent(60/999): loss=0.421201840759627\n",
      "Gradient Descent(61/999): loss=0.42081213347292645\n",
      "Gradient Descent(62/999): loss=0.4204324858453904\n",
      "Gradient Descent(63/999): loss=0.4200625650656005\n",
      "Gradient Descent(64/999): loss=0.41970205114624903\n",
      "Gradient Descent(65/999): loss=0.4193506363281628\n",
      "Gradient Descent(66/999): loss=0.419008024520424\n",
      "Gradient Descent(67/999): loss=0.41867393077368215\n",
      "Gradient Descent(68/999): loss=0.4183480807840378\n",
      "Gradient Descent(69/999): loss=0.418030210425135\n",
      "Gradient Descent(70/999): loss=0.4177200653063264\n",
      "Gradient Descent(71/999): loss=0.4174174003549757\n",
      "Gradient Descent(72/999): loss=0.41712197942114193\n",
      "Gradient Descent(73/999): loss=0.4168335749030581\n",
      "Gradient Descent(74/999): loss=0.41655196739195305\n",
      "Gradient Descent(75/999): loss=0.41627694533489973\n",
      "Gradient Descent(76/999): loss=0.4160083047144892\n",
      "Gradient Descent(77/999): loss=0.41574584874423115\n",
      "Gradient Descent(78/999): loss=0.41548938757867726\n",
      "Gradient Descent(79/999): loss=0.4152387380373463\n",
      "Gradient Descent(80/999): loss=0.41499372334160584\n",
      "Gradient Descent(81/999): loss=0.4147541728637357\n",
      "Gradient Descent(82/999): loss=0.41451992188745435\n",
      "Gradient Descent(83/999): loss=0.41429081137924995\n",
      "Gradient Descent(84/999): loss=0.4140666877699075\n",
      "Gradient Descent(85/999): loss=0.4138474027456619\n",
      "Gradient Descent(86/999): loss=0.4136328130484621\n",
      "Gradient Descent(87/999): loss=0.41342278028485047\n",
      "Gradient Descent(88/999): loss=0.41321717074301284\n",
      "Gradient Descent(89/999): loss=0.41301585521757356\n",
      "Gradient Descent(90/999): loss=0.41281870884174504\n",
      "Gradient Descent(91/999): loss=0.41262561092646133\n",
      "Gradient Descent(92/999): loss=0.4124364448061566\n",
      "Gradient Descent(93/999): loss=0.4122510976908619\n",
      "Gradient Descent(94/999): loss=0.4120694605243206\n",
      "Gradient Descent(95/999): loss=0.4118914278478384\n",
      "Gradient Descent(96/999): loss=0.4117168976696001\n",
      "Gradient Descent(97/999): loss=0.411545771339201\n",
      "Gradient Descent(98/999): loss=0.4113779534271579\n",
      "Gradient Descent(99/999): loss=0.4112133516091726\n",
      "Gradient Descent(100/999): loss=0.4110518765549387\n",
      "Gradient Descent(101/999): loss=0.41089344182129045\n",
      "Gradient Descent(102/999): loss=0.4107379637495038\n",
      "Gradient Descent(103/999): loss=0.41058536136657064\n",
      "Gradient Descent(104/999): loss=0.41043555629027395\n",
      "Gradient Descent(105/999): loss=0.41028847263790535\n",
      "Gradient Descent(106/999): loss=0.4101440369384671\n",
      "Gradient Descent(107/999): loss=0.4100021780482159\n",
      "Gradient Descent(108/999): loss=0.4098628270694069\n",
      "Gradient Descent(109/999): loss=0.4097259172721061\n",
      "Gradient Descent(110/999): loss=0.40959138401894646\n",
      "Gradient Descent(111/999): loss=0.40945916469270405\n",
      "Gradient Descent(112/999): loss=0.40932919862658357\n",
      "Gradient Descent(113/999): loss=0.40920142703710005\n",
      "Gradient Descent(114/999): loss=0.40907579295945407\n",
      "Gradient Descent(115/999): loss=0.40895224118530105\n",
      "Gradient Descent(116/999): loss=0.4088307182028176\n",
      "Gradient Descent(117/999): loss=0.40871117213897473\n",
      "Gradient Descent(118/999): loss=0.40859355270393105\n",
      "Gradient Descent(119/999): loss=0.40847781113746073\n",
      "Gradient Descent(120/999): loss=0.40836390015733864\n",
      "Gradient Descent(121/999): loss=0.4082517739096047\n",
      "Gradient Descent(122/999): loss=0.40814138792063465\n",
      "Gradient Descent(123/999): loss=0.40803269905094747\n",
      "Gradient Descent(124/999): loss=0.407925665450681\n",
      "Gradient Descent(125/999): loss=0.4078202465166734\n",
      "Gradient Descent(126/999): loss=0.40771640285108673\n",
      "Gradient Descent(127/999): loss=0.4076140962215156\n",
      "Gradient Descent(128/999): loss=0.40751328952252147\n",
      "Gradient Descent(129/999): loss=0.4074139467385407\n",
      "Gradient Descent(130/999): loss=0.4073160329081133\n",
      "Gradient Descent(131/999): loss=0.40721951408938106\n",
      "Gradient Descent(132/999): loss=0.4071243573268091\n",
      "Gradient Descent(133/999): loss=0.4070305306190838\n",
      "Gradient Descent(134/999): loss=0.4069380028881418\n",
      "Gradient Descent(135/999): loss=0.40684674394928905\n",
      "Gradient Descent(136/999): loss=0.4067567244823688\n",
      "Gradient Descent(137/999): loss=0.40666791600393937\n",
      "Gradient Descent(138/999): loss=0.40658029084042197\n",
      "Gradient Descent(139/999): loss=0.40649382210218665\n",
      "Gradient Descent(140/999): loss=0.406408483658538\n",
      "Gradient Descent(141/999): loss=0.4063242501135676\n",
      "Gradient Descent(142/999): loss=0.4062410967828445\n",
      "Gradient Descent(143/999): loss=0.4061589996709072\n",
      "Gradient Descent(144/999): loss=0.40607793544953386\n",
      "Gradient Descent(145/999): loss=0.40599788143675664\n",
      "Gradient Descent(146/999): loss=0.4059188155765969\n",
      "Gradient Descent(147/999): loss=0.4058407164194929\n",
      "Gradient Descent(148/999): loss=0.405763563103394\n",
      "Gradient Descent(149/999): loss=0.40568733533550116\n",
      "Gradient Descent(150/999): loss=0.40561201337462394\n",
      "Gradient Descent(151/999): loss=0.4055375780141375\n",
      "Gradient Descent(152/999): loss=0.40546401056551395\n",
      "Gradient Descent(153/999): loss=0.40539129284240993\n",
      "Gradient Descent(154/999): loss=0.4053194071452885\n",
      "Gradient Descent(155/999): loss=0.405248336246557\n",
      "Gradient Descent(156/999): loss=0.4051780633762018\n",
      "Gradient Descent(157/999): loss=0.40510857220790364\n",
      "Gradient Descent(158/999): loss=0.40503984684561384\n",
      "Gradient Descent(159/999): loss=0.4049718718105776\n",
      "Gradient Descent(160/999): loss=0.40490463202878635\n",
      "Gradient Descent(161/999): loss=0.4048381128188453\n",
      "Gradient Descent(162/999): loss=0.40477229988024044\n",
      "Gradient Descent(163/999): loss=0.40470717928199196\n",
      "Gradient Descent(164/999): loss=0.4046427374516793\n",
      "Gradient Descent(165/999): loss=0.404578961164826\n",
      "Gradient Descent(166/999): loss=0.4045158375346291\n",
      "Gradient Descent(167/999): loss=0.4044533540020262\n",
      "Gradient Descent(168/999): loss=0.40439149832608123\n",
      "Gradient Descent(169/999): loss=0.40433025857468413\n",
      "Gradient Descent(170/999): loss=0.40426962311555037\n",
      "Gradient Descent(171/999): loss=0.4042095806075094\n",
      "Gradient Descent(172/999): loss=0.4041501199920745\n",
      "Gradient Descent(173/999): loss=0.40409123048528167\n",
      "Gradient Descent(174/999): loss=0.4040329015697902\n",
      "Gradient Descent(175/999): loss=0.4039751229872342\n",
      "Gradient Descent(176/999): loss=0.4039178847308183\n",
      "Gradient Descent(177/999): loss=0.40386117703814745\n",
      "Gradient Descent(178/999): loss=0.4038049903842835\n",
      "Gradient Descent(179/999): loss=0.40374931547502124\n",
      "Gradient Descent(180/999): loss=0.40369414324037567\n",
      "Gradient Descent(181/999): loss=0.40363946482827295\n",
      "Gradient Descent(182/999): loss=0.4035852715984408\n",
      "Gradient Descent(183/999): loss=0.40353155511648636\n",
      "Gradient Descent(184/999): loss=0.4034783071481623\n",
      "Gradient Descent(185/999): loss=0.40342551965380596\n",
      "Gradient Descent(186/999): loss=0.40337318478295553\n",
      "Gradient Descent(187/999): loss=0.40332129486912954\n",
      "Gradient Descent(188/999): loss=0.40326984242476677\n",
      "Gradient Descent(189/999): loss=0.40321882013632326\n",
      "Gradient Descent(190/999): loss=0.4031682208595184\n",
      "Gradient Descent(191/999): loss=0.40311803761472553\n",
      "Gradient Descent(192/999): loss=0.4030682635825043\n",
      "Gradient Descent(193/999): loss=0.40301889209926633\n",
      "Gradient Descent(194/999): loss=0.4029699166530754\n",
      "Gradient Descent(195/999): loss=0.40292133087957127\n",
      "Gradient Descent(196/999): loss=0.4028731285580172\n",
      "Gradient Descent(197/999): loss=0.40282530360746666\n",
      "Gradient Descent(198/999): loss=0.40277785008304523\n",
      "Gradient Descent(199/999): loss=0.40273076217234016\n",
      "Gradient Descent(200/999): loss=0.4026840341919025\n",
      "Gradient Descent(201/999): loss=0.40263766058384765\n",
      "Gradient Descent(202/999): loss=0.40259163591256125\n",
      "Gradient Descent(203/999): loss=0.40254595486149825\n",
      "Gradient Descent(204/999): loss=0.4025006122300796\n",
      "Gradient Descent(205/999): loss=0.4024556029306763\n",
      "Gradient Descent(206/999): loss=0.4024109219856851\n",
      "Gradient Descent(207/999): loss=0.40236656452468744\n",
      "Gradient Descent(208/999): loss=0.4023225257816912\n",
      "Gradient Descent(209/999): loss=0.402278801092453\n",
      "Gradient Descent(210/999): loss=0.40223538589187774\n",
      "Gradient Descent(211/999): loss=0.40219227571149235\n",
      "Gradient Descent(212/999): loss=0.402149466176993\n",
      "Gradient Descent(213/999): loss=0.4021069530058625\n",
      "Gradient Descent(214/999): loss=0.4020647320050547\n",
      "Gradient Descent(215/999): loss=0.4020227990687465\n",
      "Gradient Descent(216/999): loss=0.4019811501761527\n",
      "Gradient Descent(217/999): loss=0.4019397813894024\n",
      "Gradient Descent(218/999): loss=0.4018986888514757\n",
      "Gradient Descent(219/999): loss=0.40185786878419943\n",
      "Gradient Descent(220/999): loss=0.40181731748629756\n",
      "Gradient Descent(221/999): loss=0.40177703133149667\n",
      "Gradient Descent(222/999): loss=0.40173700676668533\n",
      "Gradient Descent(223/999): loss=0.40169724031012255\n",
      "Gradient Descent(224/999): loss=0.4016577285496983\n",
      "Gradient Descent(225/999): loss=0.40161846814124\n",
      "Gradient Descent(226/999): loss=0.40157945580686755\n",
      "Gradient Descent(227/999): loss=0.4015406883333918\n",
      "Gradient Descent(228/999): loss=0.4015021625707585\n",
      "Gradient Descent(229/999): loss=0.4014638754305334\n",
      "Gradient Descent(230/999): loss=0.40142582388442943\n",
      "Gradient Descent(231/999): loss=0.4013880049628739\n",
      "Gradient Descent(232/999): loss=0.4013504157536143\n",
      "Gradient Descent(233/999): loss=0.40131305340036116\n",
      "Gradient Descent(234/999): loss=0.4012759151014682\n",
      "Gradient Descent(235/999): loss=0.4012389981086484\n",
      "Gradient Descent(236/999): loss=0.4012022997257226\n",
      "Gradient Descent(237/999): loss=0.4011658173074029\n",
      "Gradient Descent(238/999): loss=0.4011295482581086\n",
      "Gradient Descent(239/999): loss=0.40109349003081257\n",
      "Gradient Descent(240/999): loss=0.40105764012591805\n",
      "Gradient Descent(241/999): loss=0.40102199609016703\n",
      "Gradient Descent(242/999): loss=0.4009865555155743\n",
      "Gradient Descent(243/999): loss=0.4009513160383918\n",
      "Gradient Descent(244/999): loss=0.40091627533809887\n",
      "Gradient Descent(245/999): loss=0.40088143113641944\n",
      "Gradient Descent(246/999): loss=0.4008467811963643\n",
      "Gradient Descent(247/999): loss=0.40081232332129807\n",
      "Gradient Descent(248/999): loss=0.40077805535403077\n",
      "Gradient Descent(249/999): loss=0.4007439751759321\n",
      "Gradient Descent(250/999): loss=0.4007100807060697\n",
      "Gradient Descent(251/999): loss=0.4006763699003671\n",
      "Gradient Descent(252/999): loss=0.4006428407507865\n",
      "Gradient Descent(253/999): loss=0.40060949128452855\n",
      "Gradient Descent(254/999): loss=0.40057631956325596\n",
      "Gradient Descent(255/999): loss=0.4005433236823344\n",
      "Gradient Descent(256/999): loss=0.4005105017700926\n",
      "Gradient Descent(257/999): loss=0.40047785198710334\n",
      "Gradient Descent(258/999): loss=0.40044537252547946\n",
      "Gradient Descent(259/999): loss=0.40041306160818924\n",
      "Gradient Descent(260/999): loss=0.4003809174883887\n",
      "Gradient Descent(261/999): loss=0.4003489384487701\n",
      "Gradient Descent(262/999): loss=0.40031712280092685\n",
      "Gradient Descent(263/999): loss=0.400285468884733\n",
      "Gradient Descent(264/999): loss=0.4002539750677406\n",
      "Gradient Descent(265/999): loss=0.4002226397445887\n",
      "Gradient Descent(266/999): loss=0.4001914613364299\n",
      "Gradient Descent(267/999): loss=0.40016043829036785\n",
      "Gradient Descent(268/999): loss=0.40012956907891106\n",
      "Gradient Descent(269/999): loss=0.40009885219943836\n",
      "Gradient Descent(270/999): loss=0.4000682861736777\n",
      "Gradient Descent(271/999): loss=0.4000378695471985\n",
      "Gradient Descent(272/999): loss=0.4000076008889142\n",
      "Gradient Descent(273/999): loss=0.39997747879059875\n",
      "Gradient Descent(274/999): loss=0.3999475018664138\n",
      "Gradient Descent(275/999): loss=0.39991766875244655\n",
      "Gradient Descent(276/999): loss=0.3998879781062601\n",
      "Gradient Descent(277/999): loss=0.399858428606453\n",
      "Gradient Descent(278/999): loss=0.3998290189522309\n",
      "Gradient Descent(279/999): loss=0.39979974786298533\n",
      "Gradient Descent(280/999): loss=0.3997706140778866\n",
      "Gradient Descent(281/999): loss=0.3997416163554825\n",
      "Gradient Descent(282/999): loss=0.3997127534733082\n",
      "Gradient Descent(283/999): loss=0.39968402422750576\n",
      "Gradient Descent(284/999): loss=0.39965542743244975\n",
      "Gradient Descent(285/999): loss=0.3996269619203861\n",
      "Gradient Descent(286/999): loss=0.3995986265410744\n",
      "Gradient Descent(287/999): loss=0.3995704201614422\n",
      "Gradient Descent(288/999): loss=0.39954234166524516\n",
      "Gradient Descent(289/999): loss=0.39951438995273575\n",
      "Gradient Descent(290/999): loss=0.3994865639403396\n",
      "Gradient Descent(291/999): loss=0.39945886256033897\n",
      "Gradient Descent(292/999): loss=0.39943128476056355\n",
      "Gradient Descent(293/999): loss=0.39940382950408887\n",
      "Gradient Descent(294/999): loss=0.39937649576893913\n",
      "Gradient Descent(295/999): loss=0.39934928254780144\n",
      "Gradient Descent(296/999): loss=0.3993221888477406\n",
      "Gradient Descent(297/999): loss=0.3992952136899258\n",
      "Gradient Descent(298/999): loss=0.39926835610935935\n",
      "Gradient Descent(299/999): loss=0.3992416151546136\n",
      "Gradient Descent(300/999): loss=0.3992149898875731\n",
      "Gradient Descent(301/999): loss=0.3991884793831831\n",
      "Gradient Descent(302/999): loss=0.39916208272920256\n",
      "Gradient Descent(303/999): loss=0.399135799025963\n",
      "Gradient Descent(304/999): loss=0.3991096273861332\n",
      "Gradient Descent(305/999): loss=0.3990835669344891\n",
      "Gradient Descent(306/999): loss=0.3990576168076877\n",
      "Gradient Descent(307/999): loss=0.3990317761540464\n",
      "Gradient Descent(308/999): loss=0.39900604413332824\n",
      "Gradient Descent(309/999): loss=0.39898041991653016\n",
      "Gradient Descent(310/999): loss=0.39895490268567685\n",
      "Gradient Descent(311/999): loss=0.39892949163361896\n",
      "Gradient Descent(312/999): loss=0.39890418596383476\n",
      "Gradient Descent(313/999): loss=0.39887898489023804\n",
      "Gradient Descent(314/999): loss=0.398853887636988\n",
      "Gradient Descent(315/999): loss=0.39882889343830424\n",
      "Gradient Descent(316/999): loss=0.3988040015382863\n",
      "Gradient Descent(317/999): loss=0.3987792111907355\n",
      "Gradient Descent(318/999): loss=0.39875452165898173\n",
      "Gradient Descent(319/999): loss=0.3987299322157136\n",
      "Gradient Descent(320/999): loss=0.39870544214281256\n",
      "Gradient Descent(321/999): loss=0.39868105073118926\n",
      "Gradient Descent(322/999): loss=0.3986567572806253\n",
      "Gradient Descent(323/999): loss=0.3986325610996166\n",
      "Gradient Descent(324/999): loss=0.39860846150522067\n",
      "Gradient Descent(325/999): loss=0.3985844578229071\n",
      "Gradient Descent(326/999): loss=0.3985605493864117\n",
      "Gradient Descent(327/999): loss=0.3985367355375927\n",
      "Gradient Descent(328/999): loss=0.3985130156262901\n",
      "Gradient Descent(329/999): loss=0.3984893890101888\n",
      "Gradient Descent(330/999): loss=0.39846585505468335\n",
      "Gradient Descent(331/999): loss=0.39844241313274636\n",
      "Gradient Descent(332/999): loss=0.39841906262480004\n",
      "Gradient Descent(333/999): loss=0.39839580291858767\n",
      "Gradient Descent(334/999): loss=0.39837263340905205\n",
      "Gradient Descent(335/999): loss=0.39834955349821244\n",
      "Gradient Descent(336/999): loss=0.3983265625950461\n",
      "Gradient Descent(337/999): loss=0.39830366011537227\n",
      "Gradient Descent(338/999): loss=0.3982808454817368\n",
      "Gradient Descent(339/999): loss=0.39825811812330125\n",
      "Gradient Descent(340/999): loss=0.39823547747573346\n",
      "Gradient Descent(341/999): loss=0.3982129229810987\n",
      "Gradient Descent(342/999): loss=0.39819045408775616\n",
      "Gradient Descent(343/999): loss=0.3981680702502544\n",
      "Gradient Descent(344/999): loss=0.3981457709292309\n",
      "Gradient Descent(345/999): loss=0.3981235555913127\n",
      "Gradient Descent(346/999): loss=0.39810142370901963\n",
      "Gradient Descent(347/999): loss=0.3980793747606681\n",
      "Gradient Descent(348/999): loss=0.39805740823027963\n",
      "Gradient Descent(349/999): loss=0.3980355236074868\n",
      "Gradient Descent(350/999): loss=0.39801372038744576\n",
      "Gradient Descent(351/999): loss=0.3979919980707467\n",
      "Gradient Descent(352/999): loss=0.39797035616332915\n",
      "Gradient Descent(353/999): loss=0.3979487941763954\n",
      "Gradient Descent(354/999): loss=0.3979273116263299\n",
      "Gradient Descent(355/999): loss=0.39790590803461584\n",
      "Gradient Descent(356/999): loss=0.39788458292775664\n",
      "Gradient Descent(357/999): loss=0.39786333583719735\n",
      "Gradient Descent(358/999): loss=0.3978421662992481\n",
      "Gradient Descent(359/999): loss=0.39782107385500864\n",
      "Gradient Descent(360/999): loss=0.3978000580502949\n",
      "Gradient Descent(361/999): loss=0.39777911843556607\n",
      "Gradient Descent(362/999): loss=0.3977582545658539\n",
      "Gradient Descent(363/999): loss=0.39773746600069354\n",
      "Gradient Descent(364/999): loss=0.39771675230405423\n",
      "Gradient Descent(365/999): loss=0.3976961130442727\n",
      "Gradient Descent(366/999): loss=0.3976755477939877\n",
      "Gradient Descent(367/999): loss=0.3976550561300752\n",
      "Gradient Descent(368/999): loss=0.39763463763358453\n",
      "Gradient Descent(369/999): loss=0.39761429188967756\n",
      "Gradient Descent(370/999): loss=0.397594018487566\n",
      "Gradient Descent(371/999): loss=0.3975738170204531\n",
      "Gradient Descent(372/999): loss=0.39755368708547384\n",
      "Gradient Descent(373/999): loss=0.39753362828363803\n",
      "Gradient Descent(374/999): loss=0.3975136402197728\n",
      "Gradient Descent(375/999): loss=0.3974937225024682\n",
      "Gradient Descent(376/999): loss=0.39747387474402157\n",
      "Gradient Descent(377/999): loss=0.39745409656038516\n",
      "Gradient Descent(378/999): loss=0.39743438757111216\n",
      "Gradient Descent(379/999): loss=0.39741474739930666\n",
      "Gradient Descent(380/999): loss=0.3973951756715717\n",
      "Gradient Descent(381/999): loss=0.39737567201796037\n",
      "Gradient Descent(382/999): loss=0.39735623607192666\n",
      "Gradient Descent(383/999): loss=0.39733686747027747\n",
      "Gradient Descent(384/999): loss=0.3973175658531253\n",
      "Gradient Descent(385/999): loss=0.39729833086384303\n",
      "Gradient Descent(386/999): loss=0.3972791621490171\n",
      "Gradient Descent(387/999): loss=0.39726005935840386\n",
      "Gradient Descent(388/999): loss=0.39724102214488566\n",
      "Gradient Descent(389/999): loss=0.3972220501644273\n",
      "Gradient Descent(390/999): loss=0.3972031430760342\n",
      "Gradient Descent(391/999): loss=0.39718430054171106\n",
      "Gradient Descent(392/999): loss=0.3971655222264205\n",
      "Gradient Descent(393/999): loss=0.3971468077980433\n",
      "Gradient Descent(394/999): loss=0.39712815692733944\n",
      "Gradient Descent(395/999): loss=0.3971095692879086\n",
      "Gradient Descent(396/999): loss=0.39709104455615285\n",
      "Gradient Descent(397/999): loss=0.3970725824112386\n",
      "Gradient Descent(398/999): loss=0.3970541825350608\n",
      "Gradient Descent(399/999): loss=0.3970358446122063\n",
      "Gradient Descent(400/999): loss=0.39701756832991836\n",
      "Gradient Descent(401/999): loss=0.3969993533780623\n",
      "Gradient Descent(402/999): loss=0.3969811994490906\n",
      "Gradient Descent(403/999): loss=0.39696310623801\n",
      "Gradient Descent(404/999): loss=0.3969450734423476\n",
      "Gradient Descent(405/999): loss=0.39692710076211907\n",
      "Gradient Descent(406/999): loss=0.39690918789979673\n",
      "Gradient Descent(407/999): loss=0.3968913345602774\n",
      "Gradient Descent(408/999): loss=0.39687354045085205\n",
      "Gradient Descent(409/999): loss=0.39685580528117537\n",
      "Gradient Descent(410/999): loss=0.39683812876323604\n",
      "Gradient Descent(411/999): loss=0.39682051061132695\n",
      "Gradient Descent(412/999): loss=0.3968029505420171\n",
      "Gradient Descent(413/999): loss=0.3967854482741218\n",
      "Gradient Descent(414/999): loss=0.39676800352867697\n",
      "Gradient Descent(415/999): loss=0.39675061602890915\n",
      "Gradient Descent(416/999): loss=0.3967332855002111\n",
      "Gradient Descent(417/999): loss=0.3967160116701124\n",
      "Gradient Descent(418/999): loss=0.39669879426825605\n",
      "Gradient Descent(419/999): loss=0.3966816330263711\n",
      "Gradient Descent(420/999): loss=0.39666452767824784\n",
      "Gradient Descent(421/999): loss=0.39664747795971295\n",
      "Gradient Descent(422/999): loss=0.3966304836086054\n",
      "Gradient Descent(423/999): loss=0.39661354436475166\n",
      "Gradient Descent(424/999): loss=0.39659665996994276\n",
      "Gradient Descent(425/999): loss=0.3965798301679101\n",
      "Gradient Descent(426/999): loss=0.39656305470430364\n",
      "Gradient Descent(427/999): loss=0.3965463333266686\n",
      "Gradient Descent(428/999): loss=0.3965296657844237\n",
      "Gradient Descent(429/999): loss=0.39651305182883856\n",
      "Gradient Descent(430/999): loss=0.3964964912130131\n",
      "Gradient Descent(431/999): loss=0.3964799836918561\n",
      "Gradient Descent(432/999): loss=0.3964635290220644\n",
      "Gradient Descent(433/999): loss=0.39644712696210177\n",
      "Gradient Descent(434/999): loss=0.3964307772721798\n",
      "Gradient Descent(435/999): loss=0.3964144797142373\n",
      "Gradient Descent(436/999): loss=0.39639823405192137\n",
      "Gradient Descent(437/999): loss=0.396382040050567\n",
      "Gradient Descent(438/999): loss=0.39636589747717976\n",
      "Gradient Descent(439/999): loss=0.3963498061004156\n",
      "Gradient Descent(440/999): loss=0.3963337656905636\n",
      "Gradient Descent(441/999): loss=0.3963177760195273\n",
      "Gradient Descent(442/999): loss=0.39630183686080683\n",
      "Gradient Descent(443/999): loss=0.3962859479894822\n",
      "Gradient Descent(444/999): loss=0.3962701091821946\n",
      "Gradient Descent(445/999): loss=0.396254320217131\n",
      "Gradient Descent(446/999): loss=0.39623858087400593\n",
      "Gradient Descent(447/999): loss=0.3962228909340459\n",
      "Gradient Descent(448/999): loss=0.39620725017997305\n",
      "Gradient Descent(449/999): loss=0.39619165839598885\n",
      "Gradient Descent(450/999): loss=0.3961761153677584\n",
      "Gradient Descent(451/999): loss=0.3961606208823947\n",
      "Gradient Descent(452/999): loss=0.3961451747284446\n",
      "Gradient Descent(453/999): loss=0.3961297766958716\n",
      "Gradient Descent(454/999): loss=0.39611442657604257\n",
      "Gradient Descent(455/999): loss=0.3960991241617126\n",
      "Gradient Descent(456/999): loss=0.39608386924701056\n",
      "Gradient Descent(457/999): loss=0.3960686616274251\n",
      "Gradient Descent(458/999): loss=0.3960535010997901\n",
      "Gradient Descent(459/999): loss=0.3960383874622721\n",
      "Gradient Descent(460/999): loss=0.39602332051435457\n",
      "Gradient Descent(461/999): loss=0.3960083000568268\n",
      "Gradient Descent(462/999): loss=0.39599332589176917\n",
      "Gradient Descent(463/999): loss=0.39597839782254035\n",
      "Gradient Descent(464/999): loss=0.3959635156537655\n",
      "Gradient Descent(465/999): loss=0.3959486791913219\n",
      "Gradient Descent(466/999): loss=0.3959338882423279\n",
      "Gradient Descent(467/999): loss=0.3959191426151298\n",
      "Gradient Descent(468/999): loss=0.3959044421192902\n",
      "Gradient Descent(469/999): loss=0.39588978656557555\n",
      "Gradient Descent(470/999): loss=0.3958751757659448\n",
      "Gradient Descent(471/999): loss=0.3958606095335376\n",
      "Gradient Descent(472/999): loss=0.3958460876826626\n",
      "Gradient Descent(473/999): loss=0.39583161002878664\n",
      "Gradient Descent(474/999): loss=0.39581717638852315\n",
      "Gradient Descent(475/999): loss=0.3958027865796208\n",
      "Gradient Descent(476/999): loss=0.39578844042095424\n",
      "Gradient Descent(477/999): loss=0.3957741377325112\n",
      "Gradient Descent(478/999): loss=0.3957598783353835\n",
      "Gradient Descent(479/999): loss=0.3957456620517561\n",
      "Gradient Descent(480/999): loss=0.39573148870489694\n",
      "Gradient Descent(481/999): loss=0.3957173581191464\n",
      "Gradient Descent(482/999): loss=0.395703270119908\n",
      "Gradient Descent(483/999): loss=0.3956892245336381\n",
      "Gradient Descent(484/999): loss=0.3956752211878355\n",
      "Gradient Descent(485/999): loss=0.3956612599110336\n",
      "Gradient Descent(486/999): loss=0.3956473405327892\n",
      "Gradient Descent(487/999): loss=0.39563346288367424\n",
      "Gradient Descent(488/999): loss=0.3956196267952652\n",
      "Gradient Descent(489/999): loss=0.39560583210013595\n",
      "Gradient Descent(490/999): loss=0.39559207863184725\n",
      "Gradient Descent(491/999): loss=0.395578366224938\n",
      "Gradient Descent(492/999): loss=0.3955646947149177\n",
      "Gradient Descent(493/999): loss=0.39555106393825584\n",
      "Gradient Descent(494/999): loss=0.3955374737323751\n",
      "Gradient Descent(495/999): loss=0.395523923935642\n",
      "Gradient Descent(496/999): loss=0.3955104143873586\n",
      "Gradient Descent(497/999): loss=0.39549694492775544\n",
      "Gradient Descent(498/999): loss=0.39548351539798104\n",
      "Gradient Descent(499/999): loss=0.39547012564009637\n",
      "Gradient Descent(500/999): loss=0.39545677549706576\n",
      "Gradient Descent(501/999): loss=0.39544346481274856\n",
      "Gradient Descent(502/999): loss=0.39543019343189295\n",
      "Gradient Descent(503/999): loss=0.3954169612001271\n",
      "Gradient Descent(504/999): loss=0.39540376796395177\n",
      "Gradient Descent(505/999): loss=0.3953906135707337\n",
      "Gradient Descent(506/999): loss=0.39537749786869686\n",
      "Gradient Descent(507/999): loss=0.3953644207069168\n",
      "Gradient Descent(508/999): loss=0.3953513819353123\n",
      "Gradient Descent(509/999): loss=0.3953383814046388\n",
      "Gradient Descent(510/999): loss=0.3953254189664816\n",
      "Gradient Descent(511/999): loss=0.3953124944732481\n",
      "Gradient Descent(512/999): loss=0.39529960777816225\n",
      "Gradient Descent(513/999): loss=0.39528675873525637\n",
      "Gradient Descent(514/999): loss=0.39527394719936615\n",
      "Gradient Descent(515/999): loss=0.39526117302612207\n",
      "Gradient Descent(516/999): loss=0.395248436071945\n",
      "Gradient Descent(517/999): loss=0.3952357361940382\n",
      "Gradient Descent(518/999): loss=0.3952230732503819\n",
      "Gradient Descent(519/999): loss=0.39521044709972625\n",
      "Gradient Descent(520/999): loss=0.39519785760158577\n",
      "Gradient Descent(521/999): loss=0.39518530461623314\n",
      "Gradient Descent(522/999): loss=0.395172788004693\n",
      "Gradient Descent(523/999): loss=0.3951603076287356\n",
      "Gradient Descent(524/999): loss=0.39514786335087143\n",
      "Gradient Descent(525/999): loss=0.39513545503434533\n",
      "Gradient Descent(526/999): loss=0.3951230825431306\n",
      "Gradient Descent(527/999): loss=0.3951107457419229\n",
      "Gradient Descent(528/999): loss=0.3950984444961351\n",
      "Gradient Descent(529/999): loss=0.395086178671892\n",
      "Gradient Descent(530/999): loss=0.3950739481360233\n",
      "Gradient Descent(531/999): loss=0.39506175275606037\n",
      "Gradient Descent(532/999): loss=0.395049592400229\n",
      "Gradient Descent(533/999): loss=0.39503746693744474\n",
      "Gradient Descent(534/999): loss=0.3950253762373077\n",
      "Gradient Descent(535/999): loss=0.3950133201700974\n",
      "Gradient Descent(536/999): loss=0.39500129860676697\n",
      "Gradient Descent(537/999): loss=0.39498931141893884\n",
      "Gradient Descent(538/999): loss=0.3949773584788994\n",
      "Gradient Descent(539/999): loss=0.3949654396595939\n",
      "Gradient Descent(540/999): loss=0.3949535548346214\n",
      "Gradient Descent(541/999): loss=0.3949417038782302\n",
      "Gradient Descent(542/999): loss=0.39492988666531276\n",
      "Gradient Descent(543/999): loss=0.39491810307140107\n",
      "Gradient Descent(544/999): loss=0.39490635297266186\n",
      "Gradient Descent(545/999): loss=0.3948946362458917\n",
      "Gradient Descent(546/999): loss=0.3948829527685129\n",
      "Gradient Descent(547/999): loss=0.3948713024185683\n",
      "Gradient Descent(548/999): loss=0.3948596850747173\n",
      "Gradient Descent(549/999): loss=0.39484810061623077\n",
      "Gradient Descent(550/999): loss=0.3948365489229873\n",
      "Gradient Descent(551/999): loss=0.39482502987546825\n",
      "Gradient Descent(552/999): loss=0.3948135433547536\n",
      "Gradient Descent(553/999): loss=0.39480208924251753\n",
      "Gradient Descent(554/999): loss=0.39479066742102487\n",
      "Gradient Descent(555/999): loss=0.39477927777312544\n",
      "Gradient Descent(556/999): loss=0.39476792018225143\n",
      "Gradient Descent(557/999): loss=0.39475659453241224\n",
      "Gradient Descent(558/999): loss=0.3947453007081908\n",
      "Gradient Descent(559/999): loss=0.3947340385947394\n",
      "Gradient Descent(560/999): loss=0.3947228080777758\n",
      "Gradient Descent(561/999): loss=0.39471160904357927\n",
      "Gradient Descent(562/999): loss=0.39470044137898647\n",
      "Gradient Descent(563/999): loss=0.39468930497138777\n",
      "Gradient Descent(564/999): loss=0.3946781997087236\n",
      "Gradient Descent(565/999): loss=0.39466712547947963\n",
      "Gradient Descent(566/999): loss=0.39465608217268444\n",
      "Gradient Descent(567/999): loss=0.39464506967790497\n",
      "Gradient Descent(568/999): loss=0.39463408788524257\n",
      "Gradient Descent(569/999): loss=0.3946231366853305\n",
      "Gradient Descent(570/999): loss=0.3946122159693285\n",
      "Gradient Descent(571/999): loss=0.39460132562892103\n",
      "Gradient Descent(572/999): loss=0.39459046555631244\n",
      "Gradient Descent(573/999): loss=0.3945796356442244\n",
      "Gradient Descent(574/999): loss=0.3945688357858915\n",
      "Gradient Descent(575/999): loss=0.3945580658750582\n",
      "Gradient Descent(576/999): loss=0.3945473258059759\n",
      "Gradient Descent(577/999): loss=0.3945366154733986\n",
      "Gradient Descent(578/999): loss=0.3945259347725805\n",
      "Gradient Descent(579/999): loss=0.39451528359927174\n",
      "Gradient Descent(580/999): loss=0.3945046618497161\n",
      "Gradient Descent(581/999): loss=0.39449406942064685\n",
      "Gradient Descent(582/999): loss=0.39448350620928413\n",
      "Gradient Descent(583/999): loss=0.39447297211333127\n",
      "Gradient Descent(584/999): loss=0.3944624670309723\n",
      "Gradient Descent(585/999): loss=0.39445199086086763\n",
      "Gradient Descent(586/999): loss=0.39444154350215277\n",
      "Gradient Descent(587/999): loss=0.39443112485443316\n",
      "Gradient Descent(588/999): loss=0.39442073481778267\n",
      "Gradient Descent(589/999): loss=0.39441037329273987\n",
      "Gradient Descent(590/999): loss=0.3944000401803054\n",
      "Gradient Descent(591/999): loss=0.39438973538193833\n",
      "Gradient Descent(592/999): loss=0.3943794587995543\n",
      "Gradient Descent(593/999): loss=0.3943692103355214\n",
      "Gradient Descent(594/999): loss=0.3943589898926582\n",
      "Gradient Descent(595/999): loss=0.39434879737423034\n",
      "Gradient Descent(596/999): loss=0.39433863268394814\n",
      "Gradient Descent(597/999): loss=0.3943284957259632\n",
      "Gradient Descent(598/999): loss=0.3943183864048663\n",
      "Gradient Descent(599/999): loss=0.39430830462568417\n",
      "Gradient Descent(600/999): loss=0.39429825029387666\n",
      "Gradient Descent(601/999): loss=0.39428822331533453\n",
      "Gradient Descent(602/999): loss=0.3942782235963761\n",
      "Gradient Descent(603/999): loss=0.39426825104374524\n",
      "Gradient Descent(604/999): loss=0.39425830556460845\n",
      "Gradient Descent(605/999): loss=0.39424838706655185\n",
      "Gradient Descent(606/999): loss=0.3942384954575794\n",
      "Gradient Descent(607/999): loss=0.3942286306461094\n",
      "Gradient Descent(608/999): loss=0.3942187925409728\n",
      "Gradient Descent(609/999): loss=0.39420898105141006\n",
      "Gradient Descent(610/999): loss=0.3941991960870687\n",
      "Gradient Descent(611/999): loss=0.3941894375580013\n",
      "Gradient Descent(612/999): loss=0.39417970537466246\n",
      "Gradient Descent(613/999): loss=0.39416999944790637\n",
      "Gradient Descent(614/999): loss=0.3941603196889852\n",
      "Gradient Descent(615/999): loss=0.39415066600954535\n",
      "Gradient Descent(616/999): loss=0.3941410383216262\n",
      "Gradient Descent(617/999): loss=0.3941314365376575\n",
      "Gradient Descent(618/999): loss=0.39412186057045623\n",
      "Gradient Descent(619/999): loss=0.3941123103332255\n",
      "Gradient Descent(620/999): loss=0.39410278573955165\n",
      "Gradient Descent(621/999): loss=0.39409328670340127\n",
      "Gradient Descent(622/999): loss=0.39408381313912044\n",
      "Gradient Descent(623/999): loss=0.394074364961431\n",
      "Gradient Descent(624/999): loss=0.39406494208542936\n",
      "Gradient Descent(625/999): loss=0.39405554442658386\n",
      "Gradient Descent(626/999): loss=0.3940461719007325\n",
      "Gradient Descent(627/999): loss=0.3940368244240805\n",
      "Gradient Descent(628/999): loss=0.3940275019131993\n",
      "Gradient Descent(629/999): loss=0.3940182042850231\n",
      "Gradient Descent(630/999): loss=0.39400893145684723\n",
      "Gradient Descent(631/999): loss=0.3939996833463263\n",
      "Gradient Descent(632/999): loss=0.39399045987147163\n",
      "Gradient Descent(633/999): loss=0.39398126095064956\n",
      "Gradient Descent(634/999): loss=0.3939720865025791\n",
      "Gradient Descent(635/999): loss=0.39396293644633024\n",
      "Gradient Descent(636/999): loss=0.3939538107013216\n",
      "Gradient Descent(637/999): loss=0.39394470918731855\n",
      "Gradient Descent(638/999): loss=0.3939356318244313\n",
      "Gradient Descent(639/999): loss=0.39392657853311225\n",
      "Gradient Descent(640/999): loss=0.39391754923415534\n",
      "Gradient Descent(641/999): loss=0.393908543848693\n",
      "Gradient Descent(642/999): loss=0.3938995622981945\n",
      "Gradient Descent(643/999): loss=0.39389060450446456\n",
      "Gradient Descent(644/999): loss=0.39388167038964\n",
      "Gradient Descent(645/999): loss=0.39387275987618997\n",
      "Gradient Descent(646/999): loss=0.39386387288691227\n",
      "Gradient Descent(647/999): loss=0.3938550093449324\n",
      "Gradient Descent(648/999): loss=0.39384616917370097\n",
      "Gradient Descent(649/999): loss=0.3938373522969932\n",
      "Gradient Descent(650/999): loss=0.3938285586389058\n",
      "Gradient Descent(651/999): loss=0.39381978812385554\n",
      "Gradient Descent(652/999): loss=0.39381104067657796\n",
      "Gradient Descent(653/999): loss=0.3938023162221245\n",
      "Gradient Descent(654/999): loss=0.3937936146858625\n",
      "Gradient Descent(655/999): loss=0.3937849359934711\n",
      "Gradient Descent(656/999): loss=0.3937762800709417\n",
      "Gradient Descent(657/999): loss=0.393767646844575\n",
      "Gradient Descent(658/999): loss=0.39375903624097974\n",
      "Gradient Descent(659/999): loss=0.39375044818707045\n",
      "Gradient Descent(660/999): loss=0.39374188261006665\n",
      "Gradient Descent(661/999): loss=0.3937333394374907\n",
      "Gradient Descent(662/999): loss=0.3937248185971656\n",
      "Gradient Descent(663/999): loss=0.3937163200172143\n",
      "Gradient Descent(664/999): loss=0.3937078436260578\n",
      "Gradient Descent(665/999): loss=0.393699389352413\n",
      "Gradient Descent(666/999): loss=0.39369095712529145\n",
      "Gradient Descent(667/999): loss=0.39368254687399806\n",
      "Gradient Descent(668/999): loss=0.39367415852812926\n",
      "Gradient Descent(669/999): loss=0.39366579201757057\n",
      "Gradient Descent(670/999): loss=0.39365744727249674\n",
      "Gradient Descent(671/999): loss=0.3936491242233688\n",
      "Gradient Descent(672/999): loss=0.3936408228009333\n",
      "Gradient Descent(673/999): loss=0.39363254293621996\n",
      "Gradient Descent(674/999): loss=0.3936242845605413\n",
      "Gradient Descent(675/999): loss=0.39361604760549007\n",
      "Gradient Descent(676/999): loss=0.39360783200293814\n",
      "Gradient Descent(677/999): loss=0.3935996376850351\n",
      "Gradient Descent(678/999): loss=0.3935914645842071\n",
      "Gradient Descent(679/999): loss=0.393583312633154\n",
      "Gradient Descent(680/999): loss=0.3935751817648501\n",
      "Gradient Descent(681/999): loss=0.3935670719125409\n",
      "Gradient Descent(682/999): loss=0.39355898300974224\n",
      "Gradient Descent(683/999): loss=0.3935509149902386\n",
      "Gradient Descent(684/999): loss=0.3935428677880828\n",
      "Gradient Descent(685/999): loss=0.39353484133759287\n",
      "Gradient Descent(686/999): loss=0.3935268355733522\n",
      "Gradient Descent(687/999): loss=0.39351885043020735\n",
      "Gradient Descent(688/999): loss=0.3935108858432661\n",
      "Gradient Descent(689/999): loss=0.39350294174789796\n",
      "Gradient Descent(690/999): loss=0.3934950180797308\n",
      "Gradient Descent(691/999): loss=0.3934871147746504\n",
      "Gradient Descent(692/999): loss=0.39347923176879956\n",
      "Gradient Descent(693/999): loss=0.39347136899857577\n",
      "Gradient Descent(694/999): loss=0.3934635264006306\n",
      "Gradient Descent(695/999): loss=0.3934557039118678\n",
      "Gradient Descent(696/999): loss=0.393447901469443\n",
      "Gradient Descent(697/999): loss=0.3934401190107611\n",
      "Gradient Descent(698/999): loss=0.39343235647347585\n",
      "Gradient Descent(699/999): loss=0.3934246137954887\n",
      "Gradient Descent(700/999): loss=0.39341689091494675\n",
      "Gradient Descent(701/999): loss=0.3934091877702421\n",
      "Gradient Descent(702/999): loss=0.39340150430001014\n",
      "Gradient Descent(703/999): loss=0.39339384044312936\n",
      "Gradient Descent(704/999): loss=0.39338619613871845\n",
      "Gradient Descent(705/999): loss=0.3933785713261366\n",
      "Gradient Descent(706/999): loss=0.39337096594498117\n",
      "Gradient Descent(707/999): loss=0.39336337993508724\n",
      "Gradient Descent(708/999): loss=0.3933558132365258\n",
      "Gradient Descent(709/999): loss=0.3933482657896036\n",
      "Gradient Descent(710/999): loss=0.3933407375348602\n",
      "Gradient Descent(711/999): loss=0.3933332284130685\n",
      "Gradient Descent(712/999): loss=0.3933257383652328\n",
      "Gradient Descent(713/999): loss=0.39331826733258757\n",
      "Gradient Descent(714/999): loss=0.39331081525659617\n",
      "Gradient Descent(715/999): loss=0.3933033820789508\n",
      "Gradient Descent(716/999): loss=0.39329596774156966\n",
      "Gradient Descent(717/999): loss=0.393288572186597\n",
      "Gradient Descent(718/999): loss=0.3932811953564018\n",
      "Gradient Descent(719/999): loss=0.39327383719357645\n",
      "Gradient Descent(720/999): loss=0.3932664976409356\n",
      "Gradient Descent(721/999): loss=0.3932591766415148\n",
      "Gradient Descent(722/999): loss=0.39325187413857066\n",
      "Gradient Descent(723/999): loss=0.3932445900755782\n",
      "Gradient Descent(724/999): loss=0.39323732439623055\n",
      "Gradient Descent(725/999): loss=0.3932300770444376\n",
      "Gradient Descent(726/999): loss=0.3932228479643255\n",
      "Gradient Descent(727/999): loss=0.39321563710023466\n",
      "Gradient Descent(728/999): loss=0.39320844439671965\n",
      "Gradient Descent(729/999): loss=0.393201269798547\n",
      "Gradient Descent(730/999): loss=0.3931941132506958\n",
      "Gradient Descent(731/999): loss=0.3931869746983549\n",
      "Gradient Descent(732/999): loss=0.39317985408692324\n",
      "Gradient Descent(733/999): loss=0.39317275136200774\n",
      "Gradient Descent(734/999): loss=0.39316566646942347\n",
      "Gradient Descent(735/999): loss=0.3931585993551912\n",
      "Gradient Descent(736/999): loss=0.3931515499655377\n",
      "Gradient Descent(737/999): loss=0.3931445182468945\n",
      "Gradient Descent(738/999): loss=0.3931375041458959\n",
      "Gradient Descent(739/999): loss=0.393130507609379\n",
      "Gradient Descent(740/999): loss=0.3931235285843826\n",
      "Gradient Descent(741/999): loss=0.3931165670181456\n",
      "Gradient Descent(742/999): loss=0.39310962285810686\n",
      "Gradient Descent(743/999): loss=0.393102696051904\n",
      "Gradient Descent(744/999): loss=0.3930957865473716\n",
      "Gradient Descent(745/999): loss=0.39308889429254174\n",
      "Gradient Descent(746/999): loss=0.3930820192356415\n",
      "Gradient Descent(747/999): loss=0.39307516132509335\n",
      "Gradient Descent(748/999): loss=0.3930683205095135\n",
      "Gradient Descent(749/999): loss=0.3930614967377109\n",
      "Gradient Descent(750/999): loss=0.3930546899586866\n",
      "Gradient Descent(751/999): loss=0.3930479001216332\n",
      "Gradient Descent(752/999): loss=0.39304112717593276\n",
      "Gradient Descent(753/999): loss=0.3930343710711573\n",
      "Gradient Descent(754/999): loss=0.3930276317570668\n",
      "Gradient Descent(755/999): loss=0.3930209091836087\n",
      "Gradient Descent(756/999): loss=0.39301420330091763\n",
      "Gradient Descent(757/999): loss=0.3930075140593134\n",
      "Gradient Descent(758/999): loss=0.3930008414093006\n",
      "Gradient Descent(759/999): loss=0.3929941853015686\n",
      "Gradient Descent(760/999): loss=0.3929875456869887\n",
      "Gradient Descent(761/999): loss=0.39298092251661526\n",
      "Gradient Descent(762/999): loss=0.39297431574168407\n",
      "Gradient Descent(763/999): loss=0.39296772531361074\n",
      "Gradient Descent(764/999): loss=0.3929611511839916\n",
      "Gradient Descent(765/999): loss=0.3929545933046007\n",
      "Gradient Descent(766/999): loss=0.392948051627391\n",
      "Gradient Descent(767/999): loss=0.39294152610449207\n",
      "Gradient Descent(768/999): loss=0.39293501668821007\n",
      "Gradient Descent(769/999): loss=0.39292852333102674\n",
      "Gradient Descent(770/999): loss=0.3929220459855983\n",
      "Gradient Descent(771/999): loss=0.3929155846047552\n",
      "Gradient Descent(772/999): loss=0.3929091391415007\n",
      "Gradient Descent(773/999): loss=0.3929027095490105\n",
      "Gradient Descent(774/999): loss=0.39289629578063134\n",
      "Gradient Descent(775/999): loss=0.39288989778988137\n",
      "Gradient Descent(776/999): loss=0.3928835155304481\n",
      "Gradient Descent(777/999): loss=0.3928771489561884\n",
      "Gradient Descent(778/999): loss=0.39287079802112734\n",
      "Gradient Descent(779/999): loss=0.3928644626794576\n",
      "Gradient Descent(780/999): loss=0.3928581428855388\n",
      "Gradient Descent(781/999): loss=0.3928518385938961\n",
      "Gradient Descent(782/999): loss=0.39284554975922087\n",
      "Gradient Descent(783/999): loss=0.3928392763363676\n",
      "Gradient Descent(784/999): loss=0.3928330182803558\n",
      "Gradient Descent(785/999): loss=0.39282677554636725\n",
      "Gradient Descent(786/999): loss=0.3928205480897462\n",
      "Gradient Descent(787/999): loss=0.3928143358659984\n",
      "Gradient Descent(788/999): loss=0.3928081388307906\n",
      "Gradient Descent(789/999): loss=0.39280195693994907\n",
      "Gradient Descent(790/999): loss=0.3927957901494598\n",
      "Gradient Descent(791/999): loss=0.39278963841546716\n",
      "Gradient Descent(792/999): loss=0.3927835016942737\n",
      "Gradient Descent(793/999): loss=0.39277737994233863\n",
      "Gradient Descent(794/999): loss=0.3927712731162782\n",
      "Gradient Descent(795/999): loss=0.39276518117286413\n",
      "Gradient Descent(796/999): loss=0.39275910406902254\n",
      "Gradient Descent(797/999): loss=0.3927530417618351\n",
      "Gradient Descent(798/999): loss=0.3927469942085362\n",
      "Gradient Descent(799/999): loss=0.3927409613665138\n",
      "Gradient Descent(800/999): loss=0.39273494319330715\n",
      "Gradient Descent(801/999): loss=0.3927289396466082\n",
      "Gradient Descent(802/999): loss=0.39272295068425905\n",
      "Gradient Descent(803/999): loss=0.39271697626425245\n",
      "Gradient Descent(804/999): loss=0.39271101634473043\n",
      "Gradient Descent(805/999): loss=0.392705070883984\n",
      "Gradient Descent(806/999): loss=0.39269913984045246\n",
      "Gradient Descent(807/999): loss=0.39269322317272226\n",
      "Gradient Descent(808/999): loss=0.3926873208395274\n",
      "Gradient Descent(809/999): loss=0.39268143279974743\n",
      "Gradient Descent(810/999): loss=0.39267555901240797\n",
      "Gradient Descent(811/999): loss=0.3926696994366797\n",
      "Gradient Descent(812/999): loss=0.39266385403187687\n",
      "Gradient Descent(813/999): loss=0.39265802275745826\n",
      "Gradient Descent(814/999): loss=0.3926522055730251\n",
      "Gradient Descent(815/999): loss=0.39264640243832105\n",
      "Gradient Descent(816/999): loss=0.3926406133132321\n",
      "Gradient Descent(817/999): loss=0.3926348381577842\n",
      "Gradient Descent(818/999): loss=0.3926290769321454\n",
      "Gradient Descent(819/999): loss=0.3926233295966223\n",
      "Gradient Descent(820/999): loss=0.39261759611166125\n",
      "Gradient Descent(821/999): loss=0.3926118764378477\n",
      "Gradient Descent(822/999): loss=0.3926061705359044\n",
      "Gradient Descent(823/999): loss=0.39260047836669215\n",
      "Gradient Descent(824/999): loss=0.3925947998912082\n",
      "Gradient Descent(825/999): loss=0.39258913507058624\n",
      "Gradient Descent(826/999): loss=0.3925834838660959\n",
      "Gradient Descent(827/999): loss=0.3925778462391414\n",
      "Gradient Descent(828/999): loss=0.3925722221512617\n",
      "Gradient Descent(829/999): loss=0.39256661156412936\n",
      "Gradient Descent(830/999): loss=0.3925610144395509\n",
      "Gradient Descent(831/999): loss=0.3925554307394648\n",
      "Gradient Descent(832/999): loss=0.39254986042594214\n",
      "Gradient Descent(833/999): loss=0.3925443034611857\n",
      "Gradient Descent(834/999): loss=0.3925387598075286\n",
      "Gradient Descent(835/999): loss=0.3925332294274354\n",
      "Gradient Descent(836/999): loss=0.39252771228349953\n",
      "Gradient Descent(837/999): loss=0.39252220833844476\n",
      "Gradient Descent(838/999): loss=0.39251671755512246\n",
      "Gradient Descent(839/999): loss=0.39251123989651326\n",
      "Gradient Descent(840/999): loss=0.392505775325725\n",
      "Gradient Descent(841/999): loss=0.3925003238059927\n",
      "Gradient Descent(842/999): loss=0.3924948853006775\n",
      "Gradient Descent(843/999): loss=0.3924894597732674\n",
      "Gradient Descent(844/999): loss=0.3924840471873752\n",
      "Gradient Descent(845/999): loss=0.39247864750673866\n",
      "Gradient Descent(846/999): loss=0.39247326069522065\n",
      "Gradient Descent(847/999): loss=0.3924678867168071\n",
      "Gradient Descent(848/999): loss=0.3924625255356075\n",
      "Gradient Descent(849/999): loss=0.3924571771158545\n",
      "Gradient Descent(850/999): loss=0.3924518414219027\n",
      "Gradient Descent(851/999): loss=0.3924465184182283\n",
      "Gradient Descent(852/999): loss=0.39244120806942945\n",
      "Gradient Descent(853/999): loss=0.3924359103402245\n",
      "Gradient Descent(854/999): loss=0.392430625195452\n",
      "Gradient Descent(855/999): loss=0.3924253526000706\n",
      "Gradient Descent(856/999): loss=0.39242009251915755\n",
      "Gradient Descent(857/999): loss=0.39241484491790973\n",
      "Gradient Descent(858/999): loss=0.3924096097616412\n",
      "Gradient Descent(859/999): loss=0.3924043870157845\n",
      "Gradient Descent(860/999): loss=0.392399176645889\n",
      "Gradient Descent(861/999): loss=0.39239397861762076\n",
      "Gradient Descent(862/999): loss=0.3923887928967624\n",
      "Gradient Descent(863/999): loss=0.3923836194492119\n",
      "Gradient Descent(864/999): loss=0.39237845824098283\n",
      "Gradient Descent(865/999): loss=0.39237330923820296\n",
      "Gradient Descent(866/999): loss=0.39236817240711513\n",
      "Gradient Descent(867/999): loss=0.3923630477140754\n",
      "Gradient Descent(868/999): loss=0.3923579351255534\n",
      "Gradient Descent(869/999): loss=0.3923528346081315\n",
      "Gradient Descent(870/999): loss=0.39234774612850454\n",
      "Gradient Descent(871/999): loss=0.3923426696534792\n",
      "Gradient Descent(872/999): loss=0.39233760514997373\n",
      "Gradient Descent(873/999): loss=0.3923325525850171\n",
      "Gradient Descent(874/999): loss=0.3923275119257493\n",
      "Gradient Descent(875/999): loss=0.3923224831394199\n",
      "Gradient Descent(876/999): loss=0.39231746619338814\n",
      "Gradient Descent(877/999): loss=0.39231246105512285\n",
      "Gradient Descent(878/999): loss=0.39230746769220065\n",
      "Gradient Descent(879/999): loss=0.39230248607230755\n",
      "Gradient Descent(880/999): loss=0.3922975161632369\n",
      "Gradient Descent(881/999): loss=0.3922925579328886\n",
      "Gradient Descent(882/999): loss=0.39228761134927065\n",
      "Gradient Descent(883/999): loss=0.39228267638049696\n",
      "Gradient Descent(884/999): loss=0.3922777529947873\n",
      "Gradient Descent(885/999): loss=0.3922728411604679\n",
      "Gradient Descent(886/999): loss=0.3922679408459687\n",
      "Gradient Descent(887/999): loss=0.39226305201982586\n",
      "Gradient Descent(888/999): loss=0.39225817465067886\n",
      "Gradient Descent(889/999): loss=0.3922533087072713\n",
      "Gradient Descent(890/999): loss=0.39224845415845067\n",
      "Gradient Descent(891/999): loss=0.39224361097316685\n",
      "Gradient Descent(892/999): loss=0.3922387791204724\n",
      "Gradient Descent(893/999): loss=0.3922339585695225\n",
      "Gradient Descent(894/999): loss=0.3922291492895736\n",
      "Gradient Descent(895/999): loss=0.39222435124998417\n",
      "Gradient Descent(896/999): loss=0.3922195644202124\n",
      "Gradient Descent(897/999): loss=0.39221478876981863\n",
      "Gradient Descent(898/999): loss=0.39221002426846185\n",
      "Gradient Descent(899/999): loss=0.3922052708859016\n",
      "Gradient Descent(900/999): loss=0.3922005285919965\n",
      "Gradient Descent(901/999): loss=0.39219579735670396\n",
      "Gradient Descent(902/999): loss=0.3921910771500801\n",
      "Gradient Descent(903/999): loss=0.392186367942279\n",
      "Gradient Descent(904/999): loss=0.3921816697035524\n",
      "Gradient Descent(905/999): loss=0.3921769824042494\n",
      "Gradient Descent(906/999): loss=0.3921723060148163\n",
      "Gradient Descent(907/999): loss=0.39216764050579545\n",
      "Gradient Descent(908/999): loss=0.3921629858478256\n",
      "Gradient Descent(909/999): loss=0.3921583420116412\n",
      "Gradient Descent(910/999): loss=0.39215370896807195\n",
      "Gradient Descent(911/999): loss=0.39214908668804266\n",
      "Gradient Descent(912/999): loss=0.39214447514257306\n",
      "Gradient Descent(913/999): loss=0.39213987430277647\n",
      "Gradient Descent(914/999): loss=0.39213528413986043\n",
      "Gradient Descent(915/999): loss=0.3921307046251257\n",
      "Gradient Descent(916/999): loss=0.39212613572996674\n",
      "Gradient Descent(917/999): loss=0.3921215774258696\n",
      "Gradient Descent(918/999): loss=0.39211702968441403\n",
      "Gradient Descent(919/999): loss=0.39211249247727087\n",
      "Gradient Descent(920/999): loss=0.3921079657762025\n",
      "Gradient Descent(921/999): loss=0.3921034495530635\n",
      "Gradient Descent(922/999): loss=0.3920989437797979\n",
      "Gradient Descent(923/999): loss=0.3920944484284416\n",
      "Gradient Descent(924/999): loss=0.39208996347112\n",
      "Gradient Descent(925/999): loss=0.3920854888800483\n",
      "Gradient Descent(926/999): loss=0.39208102462753136\n",
      "Gradient Descent(927/999): loss=0.392076570685963\n",
      "Gradient Descent(928/999): loss=0.3920721270278256\n",
      "Gradient Descent(929/999): loss=0.3920676936256907\n",
      "Gradient Descent(930/999): loss=0.3920632704522169\n",
      "Gradient Descent(931/999): loss=0.39205885748015123\n",
      "Gradient Descent(932/999): loss=0.3920544546823275\n",
      "Gradient Descent(933/999): loss=0.39205006203166687\n",
      "Gradient Descent(934/999): loss=0.39204567950117736\n",
      "Gradient Descent(935/999): loss=0.3920413070639528\n",
      "Gradient Descent(936/999): loss=0.39203694469317313\n",
      "Gradient Descent(937/999): loss=0.39203259236210436\n",
      "Gradient Descent(938/999): loss=0.3920282500440973\n",
      "Gradient Descent(939/999): loss=0.39202391771258804\n",
      "Gradient Descent(940/999): loss=0.3920195953410972\n",
      "Gradient Descent(941/999): loss=0.3920152829032297\n",
      "Gradient Descent(942/999): loss=0.3920109803726744\n",
      "Gradient Descent(943/999): loss=0.39200668772320413\n",
      "Gradient Descent(944/999): loss=0.3920024049286745\n",
      "Gradient Descent(945/999): loss=0.3919981319630245\n",
      "Gradient Descent(946/999): loss=0.3919938688002757\n",
      "Gradient Descent(947/999): loss=0.39198961541453226\n",
      "Gradient Descent(948/999): loss=0.3919853717799797\n",
      "Gradient Descent(949/999): loss=0.3919811378708862\n",
      "Gradient Descent(950/999): loss=0.39197691366160053\n",
      "Gradient Descent(951/999): loss=0.39197269912655286\n",
      "Gradient Descent(952/999): loss=0.39196849424025404\n",
      "Gradient Descent(953/999): loss=0.39196429897729584\n",
      "Gradient Descent(954/999): loss=0.3919601133123497\n",
      "Gradient Descent(955/999): loss=0.39195593722016664\n",
      "Gradient Descent(956/999): loss=0.39195177067557774\n",
      "Gradient Descent(957/999): loss=0.3919476136534934\n",
      "Gradient Descent(958/999): loss=0.3919434661289022\n",
      "Gradient Descent(959/999): loss=0.391939328076872\n",
      "Gradient Descent(960/999): loss=0.3919351994725486\n",
      "Gradient Descent(961/999): loss=0.3919310802911562\n",
      "Gradient Descent(962/999): loss=0.3919269705079964\n",
      "Gradient Descent(963/999): loss=0.39192287009844795\n",
      "Gradient Descent(964/999): loss=0.39191877903796724\n",
      "Gradient Descent(965/999): loss=0.39191469730208744\n",
      "Gradient Descent(966/999): loss=0.3919106248664176\n",
      "Gradient Descent(967/999): loss=0.39190656170664384\n",
      "Gradient Descent(968/999): loss=0.3919025077985276\n",
      "Gradient Descent(969/999): loss=0.3918984631179064\n",
      "Gradient Descent(970/999): loss=0.39189442764069254\n",
      "Gradient Descent(971/999): loss=0.3918904013428741\n",
      "Gradient Descent(972/999): loss=0.3918863842005134\n",
      "Gradient Descent(973/999): loss=0.39188237618974747\n",
      "Gradient Descent(974/999): loss=0.3918783772867876\n",
      "Gradient Descent(975/999): loss=0.39187438746791886\n",
      "Gradient Descent(976/999): loss=0.3918704067095002\n",
      "Gradient Descent(977/999): loss=0.39186643498796364\n",
      "Gradient Descent(978/999): loss=0.39186247227981436\n",
      "Gradient Descent(979/999): loss=0.3918585185616307\n",
      "Gradient Descent(980/999): loss=0.39185457381006294\n",
      "Gradient Descent(981/999): loss=0.39185063800183423\n",
      "Gradient Descent(982/999): loss=0.39184671111373964\n",
      "Gradient Descent(983/999): loss=0.39184279312264514\n",
      "Gradient Descent(984/999): loss=0.3918388840054893\n",
      "Gradient Descent(985/999): loss=0.39183498373928116\n",
      "Gradient Descent(986/999): loss=0.3918310923011008\n",
      "Gradient Descent(987/999): loss=0.39182720966809914\n",
      "Gradient Descent(988/999): loss=0.39182333581749723\n",
      "Gradient Descent(989/999): loss=0.39181947072658635\n",
      "Gradient Descent(990/999): loss=0.3918156143727275\n",
      "Gradient Descent(991/999): loss=0.3918117667333516\n",
      "Gradient Descent(992/999): loss=0.39180792778595863\n",
      "Gradient Descent(993/999): loss=0.39180409750811773\n",
      "Gradient Descent(994/999): loss=0.3918002758774666\n",
      "Gradient Descent(995/999): loss=0.3917964628717117\n",
      "Gradient Descent(996/999): loss=0.39179265846862815\n",
      "Gradient Descent(997/999): loss=0.39178886264605833\n",
      "Gradient Descent(998/999): loss=0.39178507538191315\n",
      "Gradient Descent(999/999): loss=0.3917812966541705\n",
      "parameters w:  [-0.01829413 -0.24191848 -0.16352452  0.03445307  0.07969718  0.09564326\n",
      " -0.0355027   0.22307672 -0.02932353  0.039769   -0.12288585  0.12151715\n",
      "  0.12351414  0.17332256 -0.00101466 -0.00135561  0.17727163 -0.00074839\n",
      "  0.00269817  0.07461698  0.00130639 -0.03715966  0.02531108 -0.04204077\n",
      "  0.00042709  0.00061439 -0.0527029   0.00264801 -0.00243218 -0.03259151]\n"
     ]
    }
   ],
   "source": [
    "from gradient_descent import least_squares_GD\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "# Start gradient descent.\n",
    "gradient_losses, gradient_ws = least_squares_GD(y, tX, w_initial, gamma, max_iters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stochastic_gradient_descent import least_squares_SGD\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "batch_size = 1\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "# Start SGD.\n",
    "stoch_gradient_losses, stoch_gradient_ws = least_squares_SGD(y, tX, w_initial, batch_size, gamma, max_iters)\n",
    "\n",
    "min_stoch_i, min_stoch_loss = get_min_param_index(stoch_gradient_losses)\n",
    "print('min index: ',min_stoch_i)\n",
    "print('min loss: ',min_stoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from least_squares import least_squares\n",
    "\n",
    "ls_wopt, ls_loss = least_squares(y,tX)\n",
    "print('loss=',ls_loss)\n",
    "print('parameters w: ',ls_wopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.     0.8  ]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 1.     0.75 ]\n",
      " [ 5.     0.8  ]\n",
      " [ 0.     0.825]\n",
      " [ 5.     0.825]\n",
      " [ 2.     0.775]\n",
      " [ 4.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.8  ]\n",
      " [ 4.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.75 ]\n",
      " [ 2.     0.775]\n",
      " [ 0.     0.8  ]\n",
      " [ 3.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.75 ]]\n",
      "(200, 40)\n",
      "[[ 5.     0.8  ]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 1.     0.75 ]\n",
      " [ 5.     0.8  ]\n",
      " [ 0.     0.825]\n",
      " [ 5.     0.825]\n",
      " [ 2.     0.775]\n",
      " [ 4.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.8  ]\n",
      " [ 4.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.75 ]\n",
      " [ 2.     0.775]\n",
      " [ 0.     0.8  ]\n",
      " [ 3.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.75 ]]\n",
      "(200, 40)\n",
      "[[ 5.     0.8  ]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 1.     0.75 ]\n",
      " [ 5.     0.8  ]\n",
      " [ 0.     0.825]\n",
      " [ 5.     0.825]\n",
      " [ 2.     0.775]\n",
      " [ 4.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.8  ]\n",
      " [ 4.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.75 ]\n",
      " [ 2.     0.775]\n",
      " [ 0.     0.8  ]\n",
      " [ 3.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.75 ]]\n",
      "(200, 40)\n",
      "[[ 5.     0.8  ]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 1.     0.75 ]\n",
      " [ 5.     0.8  ]\n",
      " [ 0.     0.825]\n",
      " [ 5.     0.825]\n",
      " [ 2.     0.775]\n",
      " [ 4.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.8  ]\n",
      " [ 4.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.75 ]\n",
      " [ 2.     0.775]\n",
      " [ 0.     0.8  ]\n",
      " [ 3.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.75 ]]\n",
      "(200, 40)\n",
      "[[ 5.     0.8  ]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 1.     0.75 ]\n",
      " [ 5.     0.8  ]\n",
      " [ 0.     0.825]\n",
      " [ 5.     0.825]\n",
      " [ 2.     0.775]\n",
      " [ 4.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.8  ]\n",
      " [ 4.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.75 ]\n",
      " [ 2.     0.775]\n",
      " [ 0.     0.8  ]\n",
      " [ 3.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 2.     0.8  ]\n",
      " [ 0.     0.75 ]]\n",
      "(200, 40)\n",
      "0.877118032703\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGNCAYAAAAsFhqMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5v8BjgSSXl3cFtsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2fcad61198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ridge_regression import cross_validation_ridge_regression\n",
    "from build_polynomial import build_poly\n",
    "from poly import *\n",
    "from build_polynomial import *\n",
    "\n",
    "min_rr_rmse,best_rr_lambda = (cross_validation_ridge_regression(y,tX,k_fold=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.     0.825]\n",
      " [ 5.     0.8  ]\n",
      " [ 2.     0.775]\n",
      " [ 5.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 2.     0.775]\n",
      " [ 4.     0.8  ]\n",
      " [ 0.     0.825]\n",
      " [ 5.     0.825]\n",
      " [ 1.     0.75 ]\n",
      " [ 3.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 3.     0.8  ]\n",
      " [ 4.     0.8  ]\n",
      " [ 4.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 2.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 5.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 3.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.75 ]\n",
      " [ 0.     0.775]\n",
      " [ 0.     0.775]\n",
      " [ 1.     0.75 ]]\n",
      "(200, 52)\n",
      "loss:  0.284033488627\n",
      "parameters w:  [-0.0263187   0.01158362 -0.03933718 -0.04891752 -0.01838251 -0.0511348\n",
      " -0.01086517  0.00664632  0.00693832 -0.03589002  0.0146597   0.0027643\n",
      "  0.01849414  0.01061297 -0.0043979   0.00883833  0.0152619   0.00113195\n",
      "  0.0174042  -0.01392594  0.0146732  -0.00250006 -0.00819308 -0.01044038\n",
      " -0.01719831  0.00492555  0.03537614 -0.02481036 -0.00364749 -0.01590135\n",
      "  0.00790204  0.00335939  0.01418658  0.031905   -0.00810612  0.02822154\n",
      " -0.00466199 -0.00741689 -0.01253939 -0.01842252 -0.02202067  0.00434623\n",
      "  0.00228775 -0.01810308 -0.01709596 -0.01162247 -0.03530801 -0.00351131\n",
      " -0.02062029 -0.02021249 -0.04719723  0.032214  ]\n"
     ]
    }
   ],
   "source": [
    "from ridge_regression import ridge_regression\n",
    "\n",
    "best_deg = find_best_poly(y, tX_train[:,1:],test_RR)\n",
    "opt_tr = build_optimal(tX_train[:,1:], best_deg)\n",
    "w_ridge, ridge_loss = ridge_regression(y, opt_tr,best_rr_lambda)\n",
    "\n",
    "print('loss: ',ridge_loss)\n",
    "print('parameters w: ',w_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from logistic_regression import logistic_regression_gradient_descent\n",
    "from plots import visualization\n",
    "\n",
    "lr_loss, lr_w = logistic_regression_gradient_descent(y, tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle score Aproximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD test\n",
      "parameters w:  [-0.03574765 -0.29307716 -0.25622237  0.02914541  0.01437547  0.15190617\n",
      "  0.22636287  0.28250279 -0.09065928  0.14286233 -0.14485509  0.04698598\n",
      "  0.18770091  0.1328155  -0.10799591  0.04080291  0.04158353 -0.05181993\n",
      " -0.10623274  0.09005564 -0.14231326 -0.00480513 -0.06340605  0.09870042\n",
      "  0.09999228 -0.16019252  0.08644647 -0.17649949 -0.11447075  0.12312552]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_GD(y, tX, ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD test\n",
      "parameters w:  [-0.05156778 -0.18065851 -0.09073065 -0.00239607  0.02824679  0.05645477\n",
      "  0.01136668  0.14440678 -0.10286569  0.02112294 -0.1271831   0.07590447\n",
      "  0.08419775  0.05606966 -0.04511965  0.08109771 -0.13074    -0.03614835\n",
      " -0.08166958 -0.03843294 -0.11223096 -0.02342437 -0.01267796  0.05771319\n",
      "  0.08870854 -0.07798771  0.04068466 -0.08403514 -0.05952428  0.04321297]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.675"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_SGD(y, tX, ratio=0.2,max_iters=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.675"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_LS(y, tX, ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_RR(y, opt_tr, ratio=0.2,lambda_=best_rr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_LR(y, tX, ratio=0.2,threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_RLR(y, tX, ratio=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "weights = gradient_ws[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "weights = stoch_gradient_ws[min_stoch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Least Squares\n",
    "weights = ls_wopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "weights = w_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "weights = lr_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weights ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = \"../Data/test.csv\" # TODO: download test data and supply path here \n",
    "y_test, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test, _, _ = standardize_outliers(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../Data/results.csv' # TODO: fill in desired name of output file for submission\n",
    "# y_pred = predict_logistic_labels(weights, tX_test,threshold=0.5)\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from poly import build_optimal\n",
    "\n",
    "OUTPUT_PATH = '../Data/results.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "tx = tX_test[:,1:]\n",
    "opt_tr = build_optimal(tx, best_deg)\n",
    "\n",
    "y_pred = predict_labels(w, opt_tr)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
