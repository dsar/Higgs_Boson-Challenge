{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCML Project-1 ~ Team #60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from gradient_descent import gradient_descent\n",
    "from stochastic_gradient_descent import stochastic_gradient_descent\n",
    "from least_squares import least_squares\n",
    "from ridge_regression import ridge_regression\n",
    "from costs import compute_loss\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 31)\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "\n",
    "DATA_TRAIN_PATH = \"../Data/train.csv\" # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "tX, mean_x, std_x = standardize(tX, mean_x=None, std_x=None)\n",
    "\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.5\n",
      "Gradient Descent(1/999): loss=0.4930403211757793\n",
      "Gradient Descent(2/999): loss=0.4870098501022072\n",
      "Gradient Descent(3/999): loss=0.4817168268664753\n",
      "Gradient Descent(4/999): loss=0.4770128047663221\n",
      "Gradient Descent(5/999): loss=0.4727827104481938\n",
      "Gradient Descent(6/999): loss=0.4689371916527971\n",
      "Gradient Descent(7/999): loss=0.4654067258061777\n",
      "Gradient Descent(8/999): loss=0.4621370840030196\n",
      "Gradient Descent(9/999): loss=0.45908583830059513\n",
      "Gradient Descent(10/999): loss=0.45621967211044767\n",
      "Gradient Descent(11/999): loss=0.4535123087925472\n",
      "Gradient Descent(12/999): loss=0.45094291613495796\n",
      "Gradient Descent(13/999): loss=0.4484948771748208\n",
      "Gradient Descent(14/999): loss=0.4461548430418665\n",
      "Gradient Descent(15/999): loss=0.4439120029218537\n",
      "Gradient Descent(16/999): loss=0.4417575211821269\n",
      "Gradient Descent(17/999): loss=0.4396841032046329\n",
      "Gradient Descent(18/999): loss=0.43768566032584444\n",
      "Gradient Descent(19/999): loss=0.43575705109814233\n",
      "Gradient Descent(20/999): loss=0.4338938813329179\n",
      "Gradient Descent(21/999): loss=0.4320923494233507\n",
      "Gradient Descent(22/999): loss=0.4303491265527386\n",
      "Gradient Descent(23/999): loss=0.4286612637864871\n",
      "Gradient Descent(24/999): loss=0.4270261198872494\n",
      "Gradient Descent(25/999): loss=0.42544130511008876\n",
      "Gradient Descent(26/999): loss=0.42390463732556116\n",
      "Gradient Descent(27/999): loss=0.4224141076584347\n",
      "Gradient Descent(28/999): loss=0.4209678534762368\n",
      "Gradient Descent(29/999): loss=0.4195641370594624\n",
      "Gradient Descent(30/999): loss=0.4182013286683655\n",
      "Gradient Descent(31/999): loss=0.41687789301616646\n",
      "Gradient Descent(32/999): loss=0.4155923783855599\n",
      "Gradient Descent(33/999): loss=0.4143434078001904\n",
      "Gradient Descent(34/999): loss=0.41312967179736826\n",
      "Gradient Descent(35/999): loss=0.4119499224519139\n",
      "Gradient Descent(36/999): loss=0.4108029683808321\n",
      "Gradient Descent(37/999): loss=0.4096876705199781\n",
      "Gradient Descent(38/999): loss=0.4086029385112189\n",
      "Gradient Descent(39/999): loss=0.4075477275750668\n",
      "Gradient Descent(40/999): loss=0.40652103577187293\n",
      "Gradient Descent(41/999): loss=0.4055219015763233\n",
      "Gradient Descent(42/999): loss=0.4045494017066949\n",
      "Gradient Descent(43/999): loss=0.4036026491632055\n",
      "Gradient Descent(44/999): loss=0.4026807914397397\n",
      "Gradient Descent(45/999): loss=0.40178300888091195\n",
      "Gradient Descent(46/999): loss=0.4009085131623653\n",
      "Gradient Descent(47/999): loss=0.40005654587679157\n",
      "Gradient Descent(48/999): loss=0.3992263772117204\n",
      "Gradient Descent(49/999): loss=0.3984173047078789\n",
      "Gradient Descent(50/999): loss=0.3976286520890715\n",
      "Gradient Descent(51/999): loss=0.3968597681561937\n",
      "Gradient Descent(52/999): loss=0.3961100257393025\n",
      "Gradient Descent(53/999): loss=0.3953788207026833\n",
      "Gradient Descent(54/999): loss=0.3946655709986549\n",
      "Gradient Descent(55/999): loss=0.3939697157664929\n",
      "Gradient Descent(56/999): loss=0.3932907144733497\n",
      "Gradient Descent(57/999): loss=0.3926280460944537\n",
      "Gradient Descent(58/999): loss=0.39198120833019573\n",
      "Gradient Descent(59/999): loss=0.39134971685796766\n",
      "Gradient Descent(60/999): loss=0.3907331046168399\n",
      "Gradient Descent(61/999): loss=0.3901309211233352\n",
      "Gradient Descent(62/999): loss=0.38954273181671084\n",
      "Gradient Descent(63/999): loss=0.38896811743228027\n",
      "Gradient Descent(64/999): loss=0.38840667340141793\n",
      "Gradient Descent(65/999): loss=0.3878580092769802\n",
      "Gradient Descent(66/999): loss=0.3873217481829583\n",
      "Gradient Descent(67/999): loss=0.3867975262872486\n",
      "Gradient Descent(68/999): loss=0.3862849922964908\n",
      "Gradient Descent(69/999): loss=0.38578380697198617\n",
      "Gradient Descent(70/999): loss=0.3852936426657501\n",
      "Gradient Descent(71/999): loss=0.38481418287581554\n",
      "Gradient Descent(72/999): loss=0.38434512181993413\n",
      "Gradient Descent(73/999): loss=0.3838861640268767\n",
      "Gradient Descent(74/999): loss=0.3834370239445601\n",
      "Gradient Descent(75/999): loss=0.3829974255642747\n",
      "Gradient Descent(76/999): loss=0.38256710206031536\n",
      "Gradient Descent(77/999): loss=0.382145795444348\n",
      "Gradient Descent(78/999): loss=0.3817332562338833\n",
      "Gradient Descent(79/999): loss=0.38132924313424604\n",
      "Gradient Descent(80/999): loss=0.38093352273346764\n",
      "Gradient Descent(81/999): loss=0.38054586920954475\n",
      "Gradient Descent(82/999): loss=0.38016606404953845\n",
      "Gradient Descent(83/999): loss=0.37979389578000927\n",
      "Gradient Descent(84/999): loss=0.379429159708306\n",
      "Gradient Descent(85/999): loss=0.3790716576742464\n",
      "Gradient Descent(86/999): loss=0.3787211978117502\n",
      "Gradient Descent(87/999): loss=0.37837759432000334\n",
      "Gradient Descent(88/999): loss=0.37804066724374963\n",
      "Gradient Descent(89/999): loss=0.37771024226232447\n",
      "Gradient Descent(90/999): loss=0.3773861504870649\n",
      "Gradient Descent(91/999): loss=0.37706822826673775\n",
      "Gradient Descent(92/999): loss=0.37675631700065787\n",
      "Gradient Descent(93/999): loss=0.3764502629591642\n",
      "Gradient Descent(94/999): loss=0.376149917111152\n",
      "Gradient Descent(95/999): loss=0.37585513495836603\n",
      "Gradient Descent(96/999): loss=0.37556577637616756\n",
      "Gradient Descent(97/999): loss=0.3752817054605107\n",
      "Gradient Descent(98/999): loss=0.37500279038086753\n",
      "Gradient Descent(99/999): loss=0.3747289032388546\n",
      "Gradient Descent(100/999): loss=0.3744599199323246\n",
      "Gradient Descent(101/999): loss=0.3741957200246976\n",
      "Gradient Descent(102/999): loss=0.3739361866193146\n",
      "Gradient Descent(103/999): loss=0.37368120623860707\n",
      "Gradient Descent(104/999): loss=0.373430668707883\n",
      "Gradient Descent(105/999): loss=0.37318446704354036\n",
      "Gradient Descent(106/999): loss=0.3729424973455276\n",
      "Gradient Descent(107/999): loss=0.3727046586938736\n",
      "Gradient Descent(108/999): loss=0.3724708530491261\n",
      "Gradient Descent(109/999): loss=0.3722409851565345\n",
      "Gradient Descent(110/999): loss=0.3720149624538261\n",
      "Gradient Descent(111/999): loss=0.3717926949824323\n",
      "Gradient Descent(112/999): loss=0.3715740953020202\n",
      "Gradient Descent(113/999): loss=0.37135907840820054\n",
      "Gradient Descent(114/999): loss=0.3711475616532802\n",
      "Gradient Descent(115/999): loss=0.37093946466993916\n",
      "Gradient Descent(116/999): loss=0.370734709297712\n",
      "Gradient Descent(117/999): loss=0.37053321951216506\n",
      "Gradient Descent(118/999): loss=0.3703349213566558\n",
      "Gradient Descent(119/999): loss=0.37013974287657847\n",
      "Gradient Descent(120/999): loss=0.3699476140559895\n",
      "Gradient Descent(121/999): loss=0.3697584667565246\n",
      "Gradient Descent(122/999): loss=0.3695722346585125\n",
      "Gradient Descent(123/999): loss=0.36938885320419956\n",
      "Gradient Descent(124/999): loss=0.36920825954300374\n",
      "Gradient Descent(125/999): loss=0.3690303924787152\n",
      "Gradient Descent(126/999): loss=0.368855192418569\n",
      "Gradient Descent(127/999): loss=0.368682601324117\n",
      "Gradient Descent(128/999): loss=0.3685125626638262\n",
      "Gradient Descent(129/999): loss=0.36834502136734015\n",
      "Gradient Descent(130/999): loss=0.3681799237813348\n",
      "Gradient Descent(131/999): loss=0.3680172176269095\n",
      "Gradient Descent(132/999): loss=0.3678568519584542\n",
      "Gradient Descent(133/999): loss=0.3676987771239343\n",
      "Gradient Descent(134/999): loss=0.36754294472653914\n",
      "Gradient Descent(135/999): loss=0.3673893075876433\n",
      "Gradient Descent(136/999): loss=0.36723781971102837\n",
      "Gradient Descent(137/999): loss=0.3670884362483197\n",
      "Gradient Descent(138/999): loss=0.3669411134655903\n",
      "Gradient Descent(139/999): loss=0.36679580871108647\n",
      "Gradient Descent(140/999): loss=0.36665248038403814\n",
      "Gradient Descent(141/999): loss=0.36651108790450554\n",
      "Gradient Descent(142/999): loss=0.36637159168423006\n",
      "Gradient Descent(143/999): loss=0.36623395309844803\n",
      "Gradient Descent(144/999): loss=0.3660981344586321\n",
      "Gradient Descent(145/999): loss=0.3659640989861277\n",
      "Gradient Descent(146/999): loss=0.36583181078664845\n",
      "Gradient Descent(147/999): loss=0.36570123482560074\n",
      "Gradient Descent(148/999): loss=0.3655723369042077\n",
      "Gradient Descent(149/999): loss=0.36544508363640094\n",
      "Gradient Descent(150/999): loss=0.3653194424264537\n",
      "Gradient Descent(151/999): loss=0.36519538144732827\n",
      "Gradient Descent(152/999): loss=0.3650728696197119\n",
      "Gradient Descent(153/999): loss=0.36495187659171485\n",
      "Gradient Descent(154/999): loss=0.3648323727192085\n",
      "Gradient Descent(155/999): loss=0.36471432904678064\n",
      "Gradient Descent(156/999): loss=0.3645977172892844\n",
      "Gradient Descent(157/999): loss=0.36448250981395985\n",
      "Gradient Descent(158/999): loss=0.3643686796231107\n",
      "Gradient Descent(159/999): loss=0.3642562003373118\n",
      "Gradient Descent(160/999): loss=0.3641450461791334\n",
      "Gradient Descent(161/999): loss=0.3640351919573606\n",
      "Gradient Descent(162/999): loss=0.3639266130516928\n",
      "Gradient Descent(163/999): loss=0.36381928539790576\n",
      "Gradient Descent(164/999): loss=0.3637131854734592\n",
      "Gradient Descent(165/999): loss=0.36360829028353725\n",
      "Gradient Descent(166/999): loss=0.363504577347503\n",
      "Gradient Descent(167/999): loss=0.36340202468575744\n",
      "Gradient Descent(168/999): loss=0.3633006108069846\n",
      "Gradient Descent(169/999): loss=0.36320031469577313\n",
      "Gradient Descent(170/999): loss=0.36310111580059984\n",
      "Gradient Descent(171/999): loss=0.36300299402216324\n",
      "Gradient Descent(172/999): loss=0.3629059297020566\n",
      "Gradient Descent(173/999): loss=0.36280990361176635\n",
      "Gradient Descent(174/999): loss=0.36271489694198844\n",
      "Gradient Descent(175/999): loss=0.36262089129224945\n",
      "Gradient Descent(176/999): loss=0.3625278686608239\n",
      "Gradient Descent(177/999): loss=0.3624358114349372\n",
      "Gradient Descent(178/999): loss=0.36234470238124566\n",
      "Gradient Descent(179/999): loss=0.3622545246365834\n",
      "Gradient Descent(180/999): loss=0.36216526169896857\n",
      "Gradient Descent(181/999): loss=0.36207689741886084\n",
      "Gradient Descent(182/999): loss=0.3619894159906598\n",
      "Gradient Descent(183/999): loss=0.3619028019444396\n",
      "Gradient Descent(184/999): loss=0.36181704013790955\n",
      "Gradient Descent(185/999): loss=0.3617321157485961\n",
      "Gradient Descent(186/999): loss=0.36164801426623705\n",
      "Gradient Descent(187/999): loss=0.361564721485382\n",
      "Gradient Descent(188/999): loss=0.36148222349819314\n",
      "Gradient Descent(189/999): loss=0.36140050668744006\n",
      "Gradient Descent(190/999): loss=0.36131955771968066\n",
      "Gradient Descent(191/999): loss=0.36123936353862673\n",
      "Gradient Descent(192/999): loss=0.3611599113586822\n",
      "Gradient Descent(193/999): loss=0.3610811886586557\n",
      "Gradient Descent(194/999): loss=0.36100318317563623\n",
      "Gradient Descent(195/999): loss=0.360925882899031\n",
      "Gradient Descent(196/999): loss=0.36084927606475836\n",
      "Gradient Descent(197/999): loss=0.36077335114959186\n",
      "Gradient Descent(198/999): loss=0.3606980968656512\n",
      "Gradient Descent(199/999): loss=0.3606235021550349\n",
      "Gradient Descent(200/999): loss=0.36054955618459134\n",
      "Gradient Descent(201/999): loss=0.3604762483408235\n",
      "Gradient Descent(202/999): loss=0.36040356822492364\n",
      "Gradient Descent(203/999): loss=0.3603315056479341\n",
      "Gradient Descent(204/999): loss=0.360260050626031\n",
      "Gradient Descent(205/999): loss=0.36018919337592575\n",
      "Gradient Descent(206/999): loss=0.36011892431038384\n",
      "Gradient Descent(207/999): loss=0.360049234033853\n",
      "Gradient Descent(208/999): loss=0.3599801133382045\n",
      "Gradient Descent(209/999): loss=0.35991155319857493\n",
      "Gradient Descent(210/999): loss=0.3598435447693159\n",
      "Gradient Descent(211/999): loss=0.3597760793800403\n",
      "Gradient Descent(212/999): loss=0.3597091485317665\n",
      "Gradient Descent(213/999): loss=0.3596427438931587\n",
      "Gradient Descent(214/999): loss=0.3595768572968554\n",
      "Gradient Descent(215/999): loss=0.359511480735891\n",
      "Gradient Descent(216/999): loss=0.3594466063602014\n",
      "Gradient Descent(217/999): loss=0.3593822264732148\n",
      "Gradient Descent(218/999): loss=0.35931833352852505\n",
      "Gradient Descent(219/999): loss=0.35925492012664373\n",
      "Gradient Descent(220/999): loss=0.3591919790118313\n",
      "Gradient Descent(221/999): loss=0.35912950306900165\n",
      "Gradient Descent(222/999): loss=0.359067485320703\n",
      "Gradient Descent(223/999): loss=0.35900591892416667\n",
      "Gradient Descent(224/999): loss=0.35894479716842903\n",
      "Gradient Descent(225/999): loss=0.3588841134715191\n",
      "Gradient Descent(226/999): loss=0.3588238613777123\n",
      "Gradient Descent(227/999): loss=0.3587640345548489\n",
      "Gradient Descent(228/999): loss=0.3587046267917148\n",
      "Gradient Descent(229/999): loss=0.35864563199548294\n",
      "Gradient Descent(230/999): loss=0.3585870441892144\n",
      "Gradient Descent(231/999): loss=0.358528857509417\n",
      "Gradient Descent(232/999): loss=0.3584710662036595\n",
      "Gradient Descent(233/999): loss=0.35841366462824226\n",
      "Gradient Descent(234/999): loss=0.3583566472459196\n",
      "Gradient Descent(235/999): loss=0.3583000086236748\n",
      "Gradient Descent(236/999): loss=0.35824374343054627\n",
      "Gradient Descent(237/999): loss=0.3581878464355023\n",
      "Gradient Descent(238/999): loss=0.3581323125053643\n",
      "Gradient Descent(239/999): loss=0.35807713660277707\n",
      "Gradient Descent(240/999): loss=0.35802231378422483\n",
      "Gradient Descent(241/999): loss=0.35796783919809194\n",
      "Gradient Descent(242/999): loss=0.35791370808276646\n",
      "Gradient Descent(243/999): loss=0.3578599157647867\n",
      "Gradient Descent(244/999): loss=0.3578064576570296\n",
      "Gradient Descent(245/999): loss=0.3577533292569382\n",
      "Gradient Descent(246/999): loss=0.35770052614478975\n",
      "Gradient Descent(247/999): loss=0.35764804398200095\n",
      "Gradient Descent(248/999): loss=0.35759587850947183\n",
      "Gradient Descent(249/999): loss=0.3575440255459655\n",
      "Gradient Descent(250/999): loss=0.35749248098652353\n",
      "Gradient Descent(251/999): loss=0.35744124080091616\n",
      "Gradient Descent(252/999): loss=0.3573903010321269\n",
      "Gradient Descent(253/999): loss=0.35733965779486965\n",
      "Gradient Descent(254/999): loss=0.3572893072741385\n",
      "Gradient Descent(255/999): loss=0.35723924572378896\n",
      "Gradient Descent(256/999): loss=0.35718946946514973\n",
      "Gradient Descent(257/999): loss=0.35713997488566607\n",
      "Gradient Descent(258/999): loss=0.3570907584375692\n",
      "Gradient Descent(259/999): loss=0.3570418166365786\n",
      "Gradient Descent(260/999): loss=0.3569931460606292\n",
      "Gradient Descent(261/999): loss=0.35694474334862736\n",
      "Gradient Descent(262/999): loss=0.35689660519923194\n",
      "Gradient Descent(263/999): loss=0.3568487283696644\n",
      "Gradient Descent(264/999): loss=0.35680110967454143\n",
      "Gradient Descent(265/999): loss=0.35675374598473397\n",
      "Gradient Descent(266/999): loss=0.35670663422625015\n",
      "Gradient Descent(267/999): loss=0.3566597713791422\n",
      "Gradient Descent(268/999): loss=0.35661315447643593\n",
      "Gradient Descent(269/999): loss=0.3565667806030839\n",
      "Gradient Descent(270/999): loss=0.3565206468949402\n",
      "Gradient Descent(271/999): loss=0.35647475053775607\n",
      "Gradient Descent(272/999): loss=0.35642908876619833\n",
      "Gradient Descent(273/999): loss=0.3563836588628875\n",
      "Gradient Descent(274/999): loss=0.3563384581574552\n",
      "Gradient Descent(275/999): loss=0.35629348402562394\n",
      "Gradient Descent(276/999): loss=0.3562487338883035\n",
      "Gradient Descent(277/999): loss=0.3562042052107074\n",
      "Gradient Descent(278/999): loss=0.35615989550148786\n",
      "Gradient Descent(279/999): loss=0.35611580231188905\n",
      "Gradient Descent(280/999): loss=0.35607192323491677\n",
      "Gradient Descent(281/999): loss=0.35602825590452614\n",
      "Gradient Descent(282/999): loss=0.35598479799482746\n",
      "Gradient Descent(283/999): loss=0.3559415472193049\n",
      "Gradient Descent(284/999): loss=0.35589850133005607\n",
      "Gradient Descent(285/999): loss=0.35585565811704317\n",
      "Gradient Descent(286/999): loss=0.35581301540736215\n",
      "Gradient Descent(287/999): loss=0.35577057106452603\n",
      "Gradient Descent(288/999): loss=0.3557283229877625\n",
      "Gradient Descent(289/999): loss=0.35568626911132706\n",
      "Gradient Descent(290/999): loss=0.3556444074038291\n",
      "Gradient Descent(291/999): loss=0.3556027358675728\n",
      "Gradient Descent(292/999): loss=0.35556125253791093\n",
      "Gradient Descent(293/999): loss=0.3555199554826118\n",
      "Gradient Descent(294/999): loss=0.3554788428012393\n",
      "Gradient Descent(295/999): loss=0.35543791262454616\n",
      "Gradient Descent(296/999): loss=0.35539716311387776\n",
      "Gradient Descent(297/999): loss=0.35535659246059076\n",
      "Gradient Descent(298/999): loss=0.3553161988854811\n",
      "Gradient Descent(299/999): loss=0.35527598063822463\n",
      "Gradient Descent(300/999): loss=0.3552359359968292\n",
      "Gradient Descent(301/999): loss=0.35519606326709785\n",
      "Gradient Descent(302/999): loss=0.35515636078210233\n",
      "Gradient Descent(303/999): loss=0.3551168269016672\n",
      "Gradient Descent(304/999): loss=0.35507746001186546\n",
      "Gradient Descent(305/999): loss=0.35503825852452336\n",
      "Gradient Descent(306/999): loss=0.35499922087673463\n",
      "Gradient Descent(307/999): loss=0.35496034553038647\n",
      "Gradient Descent(308/999): loss=0.3549216309716924\n",
      "Gradient Descent(309/999): loss=0.35488307571073713\n",
      "Gradient Descent(310/999): loss=0.3548446782810285\n",
      "Gradient Descent(311/999): loss=0.3548064372390596\n",
      "Gradient Descent(312/999): loss=0.3547683511638784\n",
      "Gradient Descent(313/999): loss=0.3547304186566685\n",
      "Gradient Descent(314/999): loss=0.35469263834033454\n",
      "Gradient Descent(315/999): loss=0.3546550088590989\n",
      "Gradient Descent(316/999): loss=0.3546175288781055\n",
      "Gradient Descent(317/999): loss=0.35458019708303123\n",
      "Gradient Descent(318/999): loss=0.3545430121797042\n",
      "Gradient Descent(319/999): loss=0.3545059728937328\n",
      "Gradient Descent(320/999): loss=0.3544690779701374\n",
      "Gradient Descent(321/999): loss=0.3544323261729936\n",
      "Gradient Descent(322/999): loss=0.3543957162850801\n",
      "Gradient Descent(323/999): loss=0.3543592471075344\n",
      "Gradient Descent(324/999): loss=0.35432291745951505\n",
      "Gradient Descent(325/999): loss=0.3542867261778701\n",
      "Gradient Descent(326/999): loss=0.3542506721168139\n",
      "Gradient Descent(327/999): loss=0.35421475414760745\n",
      "Gradient Descent(328/999): loss=0.35417897115824787\n",
      "Gradient Descent(329/999): loss=0.35414332205316146\n",
      "Gradient Descent(330/999): loss=0.3541078057529047\n",
      "Gradient Descent(331/999): loss=0.35407242119387083\n",
      "Gradient Descent(332/999): loss=0.3540371673280007\n",
      "Gradient Descent(333/999): loss=0.3540020431225006\n",
      "Gradient Descent(334/999): loss=0.3539670475595658\n",
      "Gradient Descent(335/999): loss=0.35393217963610896\n",
      "Gradient Descent(336/999): loss=0.35389743836349347\n",
      "Gradient Descent(337/999): loss=0.3538628227672725\n",
      "Gradient Descent(338/999): loss=0.3538283318869333\n",
      "Gradient Descent(339/999): loss=0.35379396477564634\n",
      "Gradient Descent(340/999): loss=0.3537597205000189\n",
      "Gradient Descent(341/999): loss=0.35372559813985394\n",
      "Gradient Descent(342/999): loss=0.35369159678791334\n",
      "Gradient Descent(343/999): loss=0.35365771554968634\n",
      "Gradient Descent(344/999): loss=0.35362395354316145\n",
      "Gradient Descent(345/999): loss=0.35359030989860346\n",
      "Gradient Descent(346/999): loss=0.353556783758335\n",
      "Gradient Descent(347/999): loss=0.35352337427652175\n",
      "Gradient Descent(348/999): loss=0.3534900806189613\n",
      "Gradient Descent(349/999): loss=0.3534569019628787\n",
      "Gradient Descent(350/999): loss=0.3534238374967217\n",
      "Gradient Descent(351/999): loss=0.35339088641996413\n",
      "Gradient Descent(352/999): loss=0.35335804794291004\n",
      "Gradient Descent(353/999): loss=0.35332532128650357\n",
      "Gradient Descent(354/999): loss=0.35329270568214116\n",
      "Gradient Descent(355/999): loss=0.3532602003714881\n",
      "Gradient Descent(356/999): loss=0.35322780460629866\n",
      "Gradient Descent(357/999): loss=0.35319551764823864\n",
      "Gradient Descent(358/999): loss=0.3531633387687132\n",
      "Gradient Descent(359/999): loss=0.3531312672486956\n",
      "Gradient Descent(360/999): loss=0.3530993023785613\n",
      "Gradient Descent(361/999): loss=0.3530674434579245\n",
      "Gradient Descent(362/999): loss=0.3530356897954769\n",
      "Gradient Descent(363/999): loss=0.35300404070883123\n",
      "Gradient Descent(364/999): loss=0.352972495524366\n",
      "Gradient Descent(365/999): loss=0.35294105357707517\n",
      "Gradient Descent(366/999): loss=0.3529097142104183\n",
      "Gradient Descent(367/999): loss=0.3528784767761758\n",
      "Gradient Descent(368/999): loss=0.35284734063430556\n",
      "Gradient Descent(369/999): loss=0.3528163051528026\n",
      "Gradient Descent(370/999): loss=0.3527853697075618\n",
      "Gradient Descent(371/999): loss=0.35275453368224263\n",
      "Gradient Descent(372/999): loss=0.3527237964681363\n",
      "Gradient Descent(373/999): loss=0.35269315746403723\n",
      "Gradient Descent(374/999): loss=0.35266261607611377\n",
      "Gradient Descent(375/999): loss=0.3526321717177843\n",
      "Gradient Descent(376/999): loss=0.3526018238095945\n",
      "Gradient Descent(377/999): loss=0.3525715717790957\n",
      "Gradient Descent(378/999): loss=0.3525414150607282\n",
      "Gradient Descent(379/999): loss=0.3525113530957052\n",
      "Gradient Descent(380/999): loss=0.3524813853318976\n",
      "Gradient Descent(381/999): loss=0.3524515112237242\n",
      "Gradient Descent(382/999): loss=0.35242173023204093\n",
      "Gradient Descent(383/999): loss=0.35239204182403383\n",
      "Gradient Descent(384/999): loss=0.3523624454731139\n",
      "Gradient Descent(385/999): loss=0.35233294065881293\n",
      "Gradient Descent(386/999): loss=0.352303526866682\n",
      "Gradient Descent(387/999): loss=0.3522742035881928\n",
      "Gradient Descent(388/999): loss=0.35224497032063845\n",
      "Gradient Descent(389/999): loss=0.3522158265670387\n",
      "Gradient Descent(390/999): loss=0.35218677183604463\n",
      "Gradient Descent(391/999): loss=0.3521578056418474\n",
      "Gradient Descent(392/999): loss=0.3521289275040861\n",
      "Gradient Descent(393/999): loss=0.3521001369477601\n",
      "Gradient Descent(394/999): loss=0.3520714335031412\n",
      "Gradient Descent(395/999): loss=0.35204281670568666\n",
      "Gradient Descent(396/999): loss=0.3520142860959571\n",
      "Gradient Descent(397/999): loss=0.35198584121953214\n",
      "Gradient Descent(398/999): loss=0.35195748162693036\n",
      "Gradient Descent(399/999): loss=0.35192920687352874\n",
      "Gradient Descent(400/999): loss=0.3519010165194856\n",
      "Gradient Descent(401/999): loss=0.35187291012966265\n",
      "Gradient Descent(402/999): loss=0.3518448872735513\n",
      "Gradient Descent(403/999): loss=0.3518169475251968\n",
      "Gradient Descent(404/999): loss=0.3517890904631274\n",
      "Gradient Descent(405/999): loss=0.3517613156702817\n",
      "Gradient Descent(406/999): loss=0.35173362273394027\n",
      "Gradient Descent(407/999): loss=0.3517060112456552\n",
      "Gradient Descent(408/999): loss=0.3516784808011846\n",
      "Gradient Descent(409/999): loss=0.35165103100042494\n",
      "Gradient Descent(410/999): loss=0.3516236614473468\n",
      "Gradient Descent(411/999): loss=0.35159637174993086\n",
      "Gradient Descent(412/999): loss=0.3515691615201055\n",
      "Gradient Descent(413/999): loss=0.35154203037368537\n",
      "Gradient Descent(414/999): loss=0.3515149779303101\n",
      "Gradient Descent(415/999): loss=0.35148800381338724\n",
      "Gradient Descent(416/999): loss=0.3514611076500311\n",
      "Gradient Descent(417/999): loss=0.35143428907100843\n",
      "Gradient Descent(418/999): loss=0.35140754771068017\n",
      "Gradient Descent(419/999): loss=0.35138088320694755\n",
      "Gradient Descent(420/999): loss=0.35135429520119793\n",
      "Gradient Descent(421/999): loss=0.35132778333825126\n",
      "Gradient Descent(422/999): loss=0.351301347266308\n",
      "Gradient Descent(423/999): loss=0.35127498663689793\n",
      "Gradient Descent(424/999): loss=0.35124870110483036\n",
      "Gradient Descent(425/999): loss=0.3512224903281439\n",
      "Gradient Descent(426/999): loss=0.3511963539680584\n",
      "Gradient Descent(427/999): loss=0.3511702916889271\n",
      "Gradient Descent(428/999): loss=0.3511443031581904\n",
      "Gradient Descent(429/999): loss=0.3511183880463287\n",
      "Gradient Descent(430/999): loss=0.351092546026819\n",
      "Gradient Descent(431/999): loss=0.3510667767760885\n",
      "Gradient Descent(432/999): loss=0.35104107997347306\n",
      "Gradient Descent(433/999): loss=0.3510154553011727\n",
      "Gradient Descent(434/999): loss=0.3509899024442112\n",
      "Gradient Descent(435/999): loss=0.35096442109039294\n",
      "Gradient Descent(436/999): loss=0.3509390109302644\n",
      "Gradient Descent(437/999): loss=0.3509136716570729\n",
      "Gradient Descent(438/999): loss=0.35088840296672785\n",
      "Gradient Descent(439/999): loss=0.3508632045577625\n",
      "Gradient Descent(440/999): loss=0.3508380761312959\n",
      "Gradient Descent(441/999): loss=0.35081301739099635\n",
      "Gradient Descent(442/999): loss=0.3507880280430443\n",
      "Gradient Descent(443/999): loss=0.35076310779609676\n",
      "Gradient Descent(444/999): loss=0.35073825636125233\n",
      "Gradient Descent(445/999): loss=0.35071347345201676\n",
      "Gradient Descent(446/999): loss=0.3506887587842679\n",
      "Gradient Descent(447/999): loss=0.3506641120762241\n",
      "Gradient Descent(448/999): loss=0.3506395330484102\n",
      "Gradient Descent(449/999): loss=0.3506150214236251\n",
      "Gradient Descent(450/999): loss=0.3505905769269115\n",
      "Gradient Descent(451/999): loss=0.35056619928552296\n",
      "Gradient Descent(452/999): loss=0.35054188822889515\n",
      "Gradient Descent(453/999): loss=0.3505176434886143\n",
      "Gradient Descent(454/999): loss=0.35049346479838855\n",
      "Gradient Descent(455/999): loss=0.35046935189401873\n",
      "Gradient Descent(456/999): loss=0.3504453045133699\n",
      "Gradient Descent(457/999): loss=0.3504213223963432\n",
      "Gradient Descent(458/999): loss=0.35039740528484825\n",
      "Gradient Descent(459/999): loss=0.35037355292277633\n",
      "Gradient Descent(460/999): loss=0.35034976505597315\n",
      "Gradient Descent(461/999): loss=0.35032604143221413\n",
      "Gradient Descent(462/999): loss=0.3503023818011773\n",
      "Gradient Descent(463/999): loss=0.3502787859144179\n",
      "Gradient Descent(464/999): loss=0.35025525352534487\n",
      "Gradient Descent(465/999): loss=0.3502317843891957\n",
      "Gradient Descent(466/999): loss=0.3502083782630117\n",
      "Gradient Descent(467/999): loss=0.3501850349056159\n",
      "Gradient Descent(468/999): loss=0.35016175407758837\n",
      "Gradient Descent(469/999): loss=0.3501385355412451\n",
      "Gradient Descent(470/999): loss=0.3501153790606133\n",
      "Gradient Descent(471/999): loss=0.35009228440141155\n",
      "Gradient Descent(472/999): loss=0.3500692513310271\n",
      "Gradient Descent(473/999): loss=0.35004627961849444\n",
      "Gradient Descent(474/999): loss=0.3500233690344746\n",
      "Gradient Descent(475/999): loss=0.3500005193512345\n",
      "Gradient Descent(476/999): loss=0.3499777303426269\n",
      "Gradient Descent(477/999): loss=0.34995500178407013\n",
      "Gradient Descent(478/999): loss=0.3499323334525286\n",
      "Gradient Descent(479/999): loss=0.3499097251264935\n",
      "Gradient Descent(480/999): loss=0.34988717658596435\n",
      "Gradient Descent(481/999): loss=0.349864687612429\n",
      "Gradient Descent(482/999): loss=0.34984225798884744\n",
      "Gradient Descent(483/999): loss=0.34981988749963155\n",
      "Gradient Descent(484/999): loss=0.34979757593062877\n",
      "Gradient Descent(485/999): loss=0.3497753230691042\n",
      "Gradient Descent(486/999): loss=0.34975312870372344\n",
      "Gradient Descent(487/999): loss=0.34973099262453583\n",
      "Gradient Descent(488/999): loss=0.34970891462295767\n",
      "Gradient Descent(489/999): loss=0.349686894491756\n",
      "Gradient Descent(490/999): loss=0.34966493202503274\n",
      "Gradient Descent(491/999): loss=0.34964302701820843\n",
      "Gradient Descent(492/999): loss=0.3496211792680067\n",
      "Gradient Descent(493/999): loss=0.34959938857243983\n",
      "Gradient Descent(494/999): loss=0.34957765473079233\n",
      "Gradient Descent(495/999): loss=0.3495559775436071\n",
      "Gradient Descent(496/999): loss=0.34953435681267087\n",
      "Gradient Descent(497/999): loss=0.34951279234099925\n",
      "Gradient Descent(498/999): loss=0.34949128393282286\n",
      "Gradient Descent(499/999): loss=0.34946983139357374\n",
      "Gradient Descent(500/999): loss=0.3494484345298713\n",
      "Gradient Descent(501/999): loss=0.34942709314950904\n",
      "Gradient Descent(502/999): loss=0.34940580706144153\n",
      "Gradient Descent(503/999): loss=0.349384576075771\n",
      "Gradient Descent(504/999): loss=0.3493634000037345\n",
      "Gradient Descent(505/999): loss=0.34934227865769135\n",
      "Gradient Descent(506/999): loss=0.3493212118511114\n",
      "Gradient Descent(507/999): loss=0.34930019939856155\n",
      "Gradient Descent(508/999): loss=0.34927924111569464\n",
      "Gradient Descent(509/999): loss=0.3492583368192376\n",
      "Gradient Descent(510/999): loss=0.3492374863269786\n",
      "Gradient Descent(511/999): loss=0.3492166894577575\n",
      "Gradient Descent(512/999): loss=0.3491959460314529\n",
      "Gradient Descent(513/999): loss=0.3491752558689716\n",
      "Gradient Descent(514/999): loss=0.34915461879223814\n",
      "Gradient Descent(515/999): loss=0.349134034624183\n",
      "Gradient Descent(516/999): loss=0.3491135031887326\n",
      "Gradient Descent(517/999): loss=0.34909302431079886\n",
      "Gradient Descent(518/999): loss=0.3490725978162685\n",
      "Gradient Descent(519/999): loss=0.34905222353199333\n",
      "Gradient Descent(520/999): loss=0.34903190128578016\n",
      "Gradient Descent(521/999): loss=0.34901163090638054\n",
      "Gradient Descent(522/999): loss=0.3489914122234815\n",
      "Gradient Descent(523/999): loss=0.3489712450676962\n",
      "Gradient Descent(524/999): loss=0.34895112927055344\n",
      "Gradient Descent(525/999): loss=0.3489310646644899\n",
      "Gradient Descent(526/999): loss=0.34891105108283993\n",
      "Gradient Descent(527/999): loss=0.348891088359827\n",
      "Gradient Descent(528/999): loss=0.34887117633055453\n",
      "Gradient Descent(529/999): loss=0.3488513148309977\n",
      "Gradient Descent(530/999): loss=0.34883150369799415\n",
      "Gradient Descent(531/999): loss=0.3488117427692362\n",
      "Gradient Descent(532/999): loss=0.3487920318832618\n",
      "Gradient Descent(533/999): loss=0.34877237087944724\n",
      "Gradient Descent(534/999): loss=0.34875275959799795\n",
      "Gradient Descent(535/999): loss=0.34873319787994145\n",
      "Gradient Descent(536/999): loss=0.3487136855671187\n",
      "Gradient Descent(537/999): loss=0.34869422250217713\n",
      "Gradient Descent(538/999): loss=0.3486748085285624\n",
      "Gradient Descent(539/999): loss=0.34865544349051114\n",
      "Gradient Descent(540/999): loss=0.3486361272330433\n",
      "Gradient Descent(541/999): loss=0.34861685960195504\n",
      "Gradient Descent(542/999): loss=0.3485976404438117\n",
      "Gradient Descent(543/999): loss=0.3485784696059403\n",
      "Gradient Descent(544/999): loss=0.3485593469364223\n",
      "Gradient Descent(545/999): loss=0.3485402722840878\n",
      "Gradient Descent(546/999): loss=0.3485212454985074\n",
      "Gradient Descent(547/999): loss=0.3485022664299864\n",
      "Gradient Descent(548/999): loss=0.34848333492955763\n",
      "Gradient Descent(549/999): loss=0.3484644508489754\n",
      "Gradient Descent(550/999): loss=0.34844561404070856\n",
      "Gradient Descent(551/999): loss=0.3484268243579347\n",
      "Gradient Descent(552/999): loss=0.34840808165453346\n",
      "Gradient Descent(553/999): loss=0.34838938578507994\n",
      "Gradient Descent(554/999): loss=0.34837073660484036\n",
      "Gradient Descent(555/999): loss=0.3483521339697636\n",
      "Gradient Descent(556/999): loss=0.34833357773647744\n",
      "Gradient Descent(557/999): loss=0.3483150677622813\n",
      "Gradient Descent(558/999): loss=0.34829660390514117\n",
      "Gradient Descent(559/999): loss=0.34827818602368366\n",
      "Gradient Descent(560/999): loss=0.3482598139771908\n",
      "Gradient Descent(561/999): loss=0.3482414876255934\n",
      "Gradient Descent(562/999): loss=0.34822320682946717\n",
      "Gradient Descent(563/999): loss=0.34820497145002643\n",
      "Gradient Descent(564/999): loss=0.3481867813491184\n",
      "Gradient Descent(565/999): loss=0.3481686363892189\n",
      "Gradient Descent(566/999): loss=0.34815053643342664\n",
      "Gradient Descent(567/999): loss=0.3481324813454584\n",
      "Gradient Descent(568/999): loss=0.34811447098964327\n",
      "Gradient Descent(569/999): loss=0.3480965052309186\n",
      "Gradient Descent(570/999): loss=0.34807858393482505\n",
      "Gradient Descent(571/999): loss=0.34806070696750063\n",
      "Gradient Descent(572/999): loss=0.34804287419567737\n",
      "Gradient Descent(573/999): loss=0.3480250854866755\n",
      "Gradient Descent(574/999): loss=0.3480073407083996\n",
      "Gradient Descent(575/999): loss=0.34798963972933317\n",
      "Gradient Descent(576/999): loss=0.3479719824185353\n",
      "Gradient Descent(577/999): loss=0.34795436864563467\n",
      "Gradient Descent(578/999): loss=0.347936798280826\n",
      "Gradient Descent(579/999): loss=0.34791927119486626\n",
      "Gradient Descent(580/999): loss=0.347901787259069\n",
      "Gradient Descent(581/999): loss=0.3478843463453012\n",
      "Gradient Descent(582/999): loss=0.3478669483259781\n",
      "Gradient Descent(583/999): loss=0.34784959307406027\n",
      "Gradient Descent(584/999): loss=0.3478322804630484\n",
      "Gradient Descent(585/999): loss=0.34781501036697987\n",
      "Gradient Descent(586/999): loss=0.34779778266042444\n",
      "Gradient Descent(587/999): loss=0.34778059721848037\n",
      "Gradient Descent(588/999): loss=0.34776345391677105\n",
      "Gradient Descent(589/999): loss=0.34774635263144016\n",
      "Gradient Descent(590/999): loss=0.3477292932391482\n",
      "Gradient Descent(591/999): loss=0.34771227561706985\n",
      "Gradient Descent(592/999): loss=0.34769529964288837\n",
      "Gradient Descent(593/999): loss=0.34767836519479334\n",
      "Gradient Descent(594/999): loss=0.3476614721514764\n",
      "Gradient Descent(595/999): loss=0.347644620392128\n",
      "Gradient Descent(596/999): loss=0.34762780979643354\n",
      "Gradient Descent(597/999): loss=0.3476110402445699\n",
      "Gradient Descent(598/999): loss=0.3475943116172025\n",
      "Gradient Descent(599/999): loss=0.3475776237954812\n",
      "Gradient Descent(600/999): loss=0.3475609766610371\n",
      "Gradient Descent(601/999): loss=0.34754437009597977\n",
      "Gradient Descent(602/999): loss=0.34752780398289335\n",
      "Gradient Descent(603/999): loss=0.3475112782048331\n",
      "Gradient Descent(604/999): loss=0.34749479264532274\n",
      "Gradient Descent(605/999): loss=0.3474783471883512\n",
      "Gradient Descent(606/999): loss=0.347461941718369\n",
      "Gradient Descent(607/999): loss=0.3474455761202861\n",
      "Gradient Descent(608/999): loss=0.3474292502794672\n",
      "Gradient Descent(609/999): loss=0.3474129640817305\n",
      "Gradient Descent(610/999): loss=0.3473967174133436\n",
      "Gradient Descent(611/999): loss=0.34738051016102045\n",
      "Gradient Descent(612/999): loss=0.3473643422119193\n",
      "Gradient Descent(613/999): loss=0.34734821345363875\n",
      "Gradient Descent(614/999): loss=0.3473321237742162\n",
      "Gradient Descent(615/999): loss=0.34731607306212287\n",
      "Gradient Descent(616/999): loss=0.34730006120626333\n",
      "Gradient Descent(617/999): loss=0.34728408809597144\n",
      "Gradient Descent(618/999): loss=0.3472681536210075\n",
      "Gradient Descent(619/999): loss=0.3472522576715562\n",
      "Gradient Descent(620/999): loss=0.3472364001382236\n",
      "Gradient Descent(621/999): loss=0.34722058091203417\n",
      "Gradient Descent(622/999): loss=0.3472047998844285\n",
      "Gradient Descent(623/999): loss=0.3471890569472611\n",
      "Gradient Descent(624/999): loss=0.34717335199279664\n",
      "Gradient Descent(625/999): loss=0.3471576849137088\n",
      "Gradient Descent(626/999): loss=0.34714205560307604\n",
      "Gradient Descent(627/999): loss=0.34712646395438146\n",
      "Gradient Descent(628/999): loss=0.34711090986150817\n",
      "Gradient Descent(629/999): loss=0.3470953932187375\n",
      "Gradient Descent(630/999): loss=0.34707991392074705\n",
      "Gradient Descent(631/999): loss=0.3470644718626082\n",
      "Gradient Descent(632/999): loss=0.347049066939783\n",
      "Gradient Descent(633/999): loss=0.34703369904812276\n",
      "Gradient Descent(634/999): loss=0.34701836808386494\n",
      "Gradient Descent(635/999): loss=0.34700307394363156\n",
      "Gradient Descent(636/999): loss=0.34698781652442623\n",
      "Gradient Descent(637/999): loss=0.34697259572363265\n",
      "Gradient Descent(638/999): loss=0.34695741143901143\n",
      "Gradient Descent(639/999): loss=0.3469422635686991\n",
      "Gradient Descent(640/999): loss=0.34692715201120466\n",
      "Gradient Descent(641/999): loss=0.34691207666540824\n",
      "Gradient Descent(642/999): loss=0.34689703743055883\n",
      "Gradient Descent(643/999): loss=0.3468820342062716\n",
      "Gradient Descent(644/999): loss=0.3468670668925267\n",
      "Gradient Descent(645/999): loss=0.34685213538966647\n",
      "Gradient Descent(646/999): loss=0.346837239598394\n",
      "Gradient Descent(647/999): loss=0.3468223794197702\n",
      "Gradient Descent(648/999): loss=0.3468075547552123\n",
      "Gradient Descent(649/999): loss=0.3467927655064922\n",
      "Gradient Descent(650/999): loss=0.34677801157573396\n",
      "Gradient Descent(651/999): loss=0.34676329286541197\n",
      "Gradient Descent(652/999): loss=0.3467486092783492\n",
      "Gradient Descent(653/999): loss=0.34673396071771484\n",
      "Gradient Descent(654/999): loss=0.34671934708702273\n",
      "Gradient Descent(655/999): loss=0.3467047682901295\n",
      "Gradient Descent(656/999): loss=0.34669022423123247\n",
      "Gradient Descent(657/999): loss=0.34667571481486786\n",
      "Gradient Descent(658/999): loss=0.3466612399459093\n",
      "Gradient Descent(659/999): loss=0.3466467995295653\n",
      "Gradient Descent(660/999): loss=0.34663239347137814\n",
      "Gradient Descent(661/999): loss=0.34661802167722155\n",
      "Gradient Descent(662/999): loss=0.34660368405329944\n",
      "Gradient Descent(663/999): loss=0.3465893805061437\n",
      "Gradient Descent(664/999): loss=0.34657511094261245\n",
      "Gradient Descent(665/999): loss=0.3465608752698888\n",
      "Gradient Descent(666/999): loss=0.3465466733954789\n",
      "Gradient Descent(667/999): loss=0.34653250522720974\n",
      "Gradient Descent(668/999): loss=0.3465183706732284\n",
      "Gradient Descent(669/999): loss=0.3465042696419995\n",
      "Gradient Descent(670/999): loss=0.3464902020423041\n",
      "Gradient Descent(671/999): loss=0.3464761677832377\n",
      "Gradient Descent(672/999): loss=0.3464621667742094\n",
      "Gradient Descent(673/999): loss=0.34644819892493867\n",
      "Gradient Descent(674/999): loss=0.3464342641454559\n",
      "Gradient Descent(675/999): loss=0.34642036234609885\n",
      "Gradient Descent(676/999): loss=0.3464064934375125\n",
      "Gradient Descent(677/999): loss=0.3463926573306465\n",
      "Gradient Descent(678/999): loss=0.34637885393675427\n",
      "Gradient Descent(679/999): loss=0.3463650831673914\n",
      "Gradient Descent(680/999): loss=0.34635134493441366\n",
      "Gradient Descent(681/999): loss=0.34633763914997606\n",
      "Gradient Descent(682/999): loss=0.3463239657265309\n",
      "Gradient Descent(683/999): loss=0.34631032457682664\n",
      "Gradient Descent(684/999): loss=0.34629671561390657\n",
      "Gradient Descent(685/999): loss=0.3462831387511065\n",
      "Gradient Descent(686/999): loss=0.34626959390205453\n",
      "Gradient Descent(687/999): loss=0.34625608098066823\n",
      "Gradient Descent(688/999): loss=0.34624259990115475\n",
      "Gradient Descent(689/999): loss=0.34622915057800857\n",
      "Gradient Descent(690/999): loss=0.34621573292600943\n",
      "Gradient Descent(691/999): loss=0.3462023468602226\n",
      "Gradient Descent(692/999): loss=0.3461889922959961\n",
      "Gradient Descent(693/999): loss=0.3461756691489597\n",
      "Gradient Descent(694/999): loss=0.3461623773350242\n",
      "Gradient Descent(695/999): loss=0.34614911677037913\n",
      "Gradient Descent(696/999): loss=0.3461358873714921\n",
      "Gradient Descent(697/999): loss=0.3461226890551072\n",
      "Gradient Descent(698/999): loss=0.3461095217382439\n",
      "Gradient Descent(699/999): loss=0.346096385338195\n",
      "Gradient Descent(700/999): loss=0.34608327977252684\n",
      "Gradient Descent(701/999): loss=0.34607020495907614\n",
      "Gradient Descent(702/999): loss=0.34605716081595056\n",
      "Gradient Descent(703/999): loss=0.34604414726152566\n",
      "Gradient Descent(704/999): loss=0.3460311642144455\n",
      "Gradient Descent(705/999): loss=0.3460182115936198\n",
      "Gradient Descent(706/999): loss=0.3460052893182238\n",
      "Gradient Descent(707/999): loss=0.3459923973076959\n",
      "Gradient Descent(708/999): loss=0.34597953548173765\n",
      "Gradient Descent(709/999): loss=0.34596670376031236\n",
      "Gradient Descent(710/999): loss=0.345953902063643\n",
      "Gradient Descent(711/999): loss=0.3459411303122115\n",
      "Gradient Descent(712/999): loss=0.34592838842675816\n",
      "Gradient Descent(713/999): loss=0.3459156763282796\n",
      "Gradient Descent(714/999): loss=0.3459029939380281\n",
      "Gradient Descent(715/999): loss=0.3458903411775103\n",
      "Gradient Descent(716/999): loss=0.3458777179684861\n",
      "Gradient Descent(717/999): loss=0.34586512423296717\n",
      "Gradient Descent(718/999): loss=0.34585255989321684\n",
      "Gradient Descent(719/999): loss=0.3458400248717475\n",
      "Gradient Descent(720/999): loss=0.3458275190913206\n",
      "Gradient Descent(721/999): loss=0.34581504247494543\n",
      "Gradient Descent(722/999): loss=0.3458025949458773\n",
      "Gradient Descent(723/999): loss=0.34579017642761745\n",
      "Gradient Descent(724/999): loss=0.3457777868439109\n",
      "Gradient Descent(725/999): loss=0.34576542611874644\n",
      "Gradient Descent(726/999): loss=0.34575309417635486\n",
      "Gradient Descent(727/999): loss=0.3457407909412081\n",
      "Gradient Descent(728/999): loss=0.34572851633801815\n",
      "Gradient Descent(729/999): loss=0.34571627029173596\n",
      "Gradient Descent(730/999): loss=0.3457040527275507\n",
      "Gradient Descent(731/999): loss=0.3456918635708883\n",
      "Gradient Descent(732/999): loss=0.3456797027474106\n",
      "Gradient Descent(733/999): loss=0.3456675701830147\n",
      "Gradient Descent(734/999): loss=0.34565546580383094\n",
      "Gradient Descent(735/999): loss=0.34564338953622353\n",
      "Gradient Descent(736/999): loss=0.3456313413067877\n",
      "Gradient Descent(737/999): loss=0.34561932104234994\n",
      "Gradient Descent(738/999): loss=0.3456073286699668\n",
      "Gradient Descent(739/999): loss=0.34559536411692343\n",
      "Gradient Descent(740/999): loss=0.3455834273107336\n",
      "Gradient Descent(741/999): loss=0.3455715181791371\n",
      "Gradient Descent(742/999): loss=0.34555963665010075\n",
      "Gradient Descent(743/999): loss=0.34554778265181607\n",
      "Gradient Descent(744/999): loss=0.3455359561126982\n",
      "Gradient Descent(745/999): loss=0.3455241569613867\n",
      "Gradient Descent(746/999): loss=0.34551238512674215\n",
      "Gradient Descent(747/999): loss=0.3455006405378476\n",
      "Gradient Descent(748/999): loss=0.34548892312400553\n",
      "Gradient Descent(749/999): loss=0.3454772328147384\n",
      "Gradient Descent(750/999): loss=0.34546556953978746\n",
      "Gradient Descent(751/999): loss=0.3454539332291114\n",
      "Gradient Descent(752/999): loss=0.34544232381288553\n",
      "Gradient Descent(753/999): loss=0.3454307412215015\n",
      "Gradient Descent(754/999): loss=0.3454191853855659\n",
      "Gradient Descent(755/999): loss=0.34540765623589875\n",
      "Gradient Descent(756/999): loss=0.34539615370353416\n",
      "Gradient Descent(757/999): loss=0.34538467771971826\n",
      "Gradient Descent(758/999): loss=0.34537322821590893\n",
      "Gradient Descent(759/999): loss=0.34536180512377423\n",
      "Gradient Descent(760/999): loss=0.3453504083751924\n",
      "Gradient Descent(761/999): loss=0.3453390379022508\n",
      "Gradient Descent(762/999): loss=0.34532769363724397\n",
      "Gradient Descent(763/999): loss=0.3453163755126747\n",
      "Gradient Descent(764/999): loss=0.34530508346125216\n",
      "Gradient Descent(765/999): loss=0.34529381741589016\n",
      "Gradient Descent(766/999): loss=0.34528257730970846\n",
      "Gradient Descent(767/999): loss=0.3452713630760298\n",
      "Gradient Descent(768/999): loss=0.3452601746483806\n",
      "Gradient Descent(769/999): loss=0.34524901196048957\n",
      "Gradient Descent(770/999): loss=0.3452378749462868\n",
      "Gradient Descent(771/999): loss=0.3452267635399029\n",
      "Gradient Descent(772/999): loss=0.3452156776756685\n",
      "Gradient Descent(773/999): loss=0.34520461728811397\n",
      "Gradient Descent(774/999): loss=0.3451935823119672\n",
      "Gradient Descent(775/999): loss=0.34518257268215347\n",
      "Gradient Descent(776/999): loss=0.34517158833379613\n",
      "Gradient Descent(777/999): loss=0.3451606292022132\n",
      "Gradient Descent(778/999): loss=0.3451496952229184\n",
      "Gradient Descent(779/999): loss=0.34513878633162\n",
      "Gradient Descent(780/999): loss=0.3451279024642201\n",
      "Gradient Descent(781/999): loss=0.3451170435568132\n",
      "Gradient Descent(782/999): loss=0.3451062095456864\n",
      "Gradient Descent(783/999): loss=0.3450954003673186\n",
      "Gradient Descent(784/999): loss=0.3450846159583785\n",
      "Gradient Descent(785/999): loss=0.3450738562557255\n",
      "Gradient Descent(786/999): loss=0.3450631211964082\n",
      "Gradient Descent(787/999): loss=0.3450524107176631\n",
      "Gradient Descent(788/999): loss=0.3450417247569152\n",
      "Gradient Descent(789/999): loss=0.3450310632517759\n",
      "Gradient Descent(790/999): loss=0.3450204261400436\n",
      "Gradient Descent(791/999): loss=0.3450098133597018\n",
      "Gradient Descent(792/999): loss=0.3449992248489192\n",
      "Gradient Descent(793/999): loss=0.3449886605460483\n",
      "Gradient Descent(794/999): loss=0.34497812038962583\n",
      "Gradient Descent(795/999): loss=0.3449676043183706\n",
      "Gradient Descent(796/999): loss=0.3449571122711838\n",
      "Gradient Descent(797/999): loss=0.34494664418714815\n",
      "Gradient Descent(798/999): loss=0.3449362000055268\n",
      "Gradient Descent(799/999): loss=0.3449257796657633\n",
      "Gradient Descent(800/999): loss=0.34491538310748027\n",
      "Gradient Descent(801/999): loss=0.344905010270479\n",
      "Gradient Descent(802/999): loss=0.34489466109473915\n",
      "Gradient Descent(803/999): loss=0.3448843355204174\n",
      "Gradient Descent(804/999): loss=0.34487403348784706\n",
      "Gradient Descent(805/999): loss=0.3448637549375378\n",
      "Gradient Descent(806/999): loss=0.3448534998101741\n",
      "Gradient Descent(807/999): loss=0.3448432680466158\n",
      "Gradient Descent(808/999): loss=0.3448330595878963\n",
      "Gradient Descent(809/999): loss=0.3448228743752225\n",
      "Gradient Descent(810/999): loss=0.34481271234997424\n",
      "Gradient Descent(811/999): loss=0.3448025734537031\n",
      "Gradient Descent(812/999): loss=0.3447924576281325\n",
      "Gradient Descent(813/999): loss=0.3447823648151566\n",
      "Gradient Descent(814/999): loss=0.3447722949568397\n",
      "Gradient Descent(815/999): loss=0.34476224799541566\n",
      "Gradient Descent(816/999): loss=0.34475222387328724\n",
      "Gradient Descent(817/999): loss=0.3447422225330258\n",
      "Gradient Descent(818/999): loss=0.34473224391737034\n",
      "Gradient Descent(819/999): loss=0.3447222879692268\n",
      "Gradient Descent(820/999): loss=0.3447123546316674\n",
      "Gradient Descent(821/999): loss=0.3447024438479308\n",
      "Gradient Descent(822/999): loss=0.3446925555614205\n",
      "Gradient Descent(823/999): loss=0.34468268971570487\n",
      "Gradient Descent(824/999): loss=0.3446728462545162\n",
      "Gradient Descent(825/999): loss=0.3446630251217505\n",
      "Gradient Descent(826/999): loss=0.3446532262614663\n",
      "Gradient Descent(827/999): loss=0.3446434496178845\n",
      "Gradient Descent(828/999): loss=0.34463369513538805\n",
      "Gradient Descent(829/999): loss=0.34462396275852075\n",
      "Gradient Descent(830/999): loss=0.34461425243198684\n",
      "Gradient Descent(831/999): loss=0.34460456410065055\n",
      "Gradient Descent(832/999): loss=0.34459489770953555\n",
      "Gradient Descent(833/999): loss=0.3445852532038244\n",
      "Gradient Descent(834/999): loss=0.3445756305288577\n",
      "Gradient Descent(835/999): loss=0.34456602963013383\n",
      "Gradient Descent(836/999): loss=0.34455645045330835\n",
      "Gradient Descent(837/999): loss=0.34454689294419316\n",
      "Gradient Descent(838/999): loss=0.3445373570487561\n",
      "Gradient Descent(839/999): loss=0.3445278427131207\n",
      "Gradient Descent(840/999): loss=0.3445183498835651\n",
      "Gradient Descent(841/999): loss=0.34450887850652206\n",
      "Gradient Descent(842/999): loss=0.3444994285285777\n",
      "Gradient Descent(843/999): loss=0.34448999989647133\n",
      "Gradient Descent(844/999): loss=0.34448059255709557\n",
      "Gradient Descent(845/999): loss=0.3444712064574947\n",
      "Gradient Descent(846/999): loss=0.34446184154486464\n",
      "Gradient Descent(847/999): loss=0.3444524977665521\n",
      "Gradient Descent(848/999): loss=0.34444317507005495\n",
      "Gradient Descent(849/999): loss=0.3444338734030207\n",
      "Gradient Descent(850/999): loss=0.34442459271324594\n",
      "Gradient Descent(851/999): loss=0.34441533294867693\n",
      "Gradient Descent(852/999): loss=0.3444060940574083\n",
      "Gradient Descent(853/999): loss=0.34439687598768176\n",
      "Gradient Descent(854/999): loss=0.34438767868788706\n",
      "Gradient Descent(855/999): loss=0.34437850210656085\n",
      "Gradient Descent(856/999): loss=0.3443693461923859\n",
      "Gradient Descent(857/999): loss=0.34436021089419083\n",
      "Gradient Descent(858/999): loss=0.34435109616094983\n",
      "Gradient Descent(859/999): loss=0.3443420019417817\n",
      "Gradient Descent(860/999): loss=0.3443329281859495\n",
      "Gradient Descent(861/999): loss=0.34432387484286037\n",
      "Gradient Descent(862/999): loss=0.3443148418620645\n",
      "Gradient Descent(863/999): loss=0.34430582919325503\n",
      "Gradient Descent(864/999): loss=0.3442968367862675\n",
      "Gradient Descent(865/999): loss=0.3442878645910793\n",
      "Gradient Descent(866/999): loss=0.34427891255780885\n",
      "Gradient Descent(867/999): loss=0.34426998063671593\n",
      "Gradient Descent(868/999): loss=0.3442610687782002\n",
      "Gradient Descent(869/999): loss=0.34425217693280175\n",
      "Gradient Descent(870/999): loss=0.34424330505119954\n",
      "Gradient Descent(871/999): loss=0.3442344530842119\n",
      "Gradient Descent(872/999): loss=0.3442256209827953\n",
      "Gradient Descent(873/999): loss=0.3442168086980443\n",
      "Gradient Descent(874/999): loss=0.3442080161811911\n",
      "Gradient Descent(875/999): loss=0.3441992433836047\n",
      "Gradient Descent(876/999): loss=0.344190490256791\n",
      "Gradient Descent(877/999): loss=0.34418175675239177\n",
      "Gradient Descent(878/999): loss=0.3441730428221842\n",
      "Gradient Descent(879/999): loss=0.34416434841808136\n",
      "Gradient Descent(880/999): loss=0.3441556734921303\n",
      "Gradient Descent(881/999): loss=0.3441470179965127\n",
      "Gradient Descent(882/999): loss=0.3441383818835442\n",
      "Gradient Descent(883/999): loss=0.3441297651056734\n",
      "Gradient Descent(884/999): loss=0.34412116761548195\n",
      "Gradient Descent(885/999): loss=0.344112589365684\n",
      "Gradient Descent(886/999): loss=0.34410403030912595\n",
      "Gradient Descent(887/999): loss=0.3440954903987854\n",
      "Gradient Descent(888/999): loss=0.3440869695877712\n",
      "Gradient Descent(889/999): loss=0.34407846782932267\n",
      "Gradient Descent(890/999): loss=0.3440699850768098\n",
      "Gradient Descent(891/999): loss=0.34406152128373196\n",
      "Gradient Descent(892/999): loss=0.3440530764037182\n",
      "Gradient Descent(893/999): loss=0.34404465039052645\n",
      "Gradient Descent(894/999): loss=0.3440362431980427\n",
      "Gradient Descent(895/999): loss=0.34402785478028186\n",
      "Gradient Descent(896/999): loss=0.3440194850913855\n",
      "Gradient Descent(897/999): loss=0.3440111340856233\n",
      "Gradient Descent(898/999): loss=0.344002801717391\n",
      "Gradient Descent(899/999): loss=0.34399448794121107\n",
      "Gradient Descent(900/999): loss=0.34398619271173214\n",
      "Gradient Descent(901/999): loss=0.34397791598372773\n",
      "Gradient Descent(902/999): loss=0.3439696577120969\n",
      "Gradient Descent(903/999): loss=0.3439614178518636\n",
      "Gradient Descent(904/999): loss=0.34395319635817573\n",
      "Gradient Descent(905/999): loss=0.34394499318630484\n",
      "Gradient Descent(906/999): loss=0.34393680829164647\n",
      "Gradient Descent(907/999): loss=0.343928641629719\n",
      "Gradient Descent(908/999): loss=0.3439204931561635\n",
      "Gradient Descent(909/999): loss=0.3439123628267429\n",
      "Gradient Descent(910/999): loss=0.3439042505973425\n",
      "Gradient Descent(911/999): loss=0.34389615642396865\n",
      "Gradient Descent(912/999): loss=0.34388808026274903\n",
      "Gradient Descent(913/999): loss=0.3438800220699318\n",
      "Gradient Descent(914/999): loss=0.34387198180188505\n",
      "Gradient Descent(915/999): loss=0.3438639594150976\n",
      "Gradient Descent(916/999): loss=0.3438559548661767\n",
      "Gradient Descent(917/999): loss=0.34384796811184914\n",
      "Gradient Descent(918/999): loss=0.3438399991089606\n",
      "Gradient Descent(919/999): loss=0.3438320478144748\n",
      "Gradient Descent(920/999): loss=0.34382411418547304\n",
      "Gradient Descent(921/999): loss=0.3438161981791549\n",
      "Gradient Descent(922/999): loss=0.343808299752836\n",
      "Gradient Descent(923/999): loss=0.3438004188639501\n",
      "Gradient Descent(924/999): loss=0.34379255547004606\n",
      "Gradient Descent(925/999): loss=0.3437847095287892\n",
      "Gradient Descent(926/999): loss=0.343776880997961\n",
      "Gradient Descent(927/999): loss=0.3437690698354571\n",
      "Gradient Descent(928/999): loss=0.3437612759992888\n",
      "Gradient Descent(929/999): loss=0.34375349944758155\n",
      "Gradient Descent(930/999): loss=0.3437457401385753\n",
      "Gradient Descent(931/999): loss=0.3437379980306232\n",
      "Gradient Descent(932/999): loss=0.3437302730821922\n",
      "Gradient Descent(933/999): loss=0.34372256525186196\n",
      "Gradient Descent(934/999): loss=0.34371487449832516\n",
      "Gradient Descent(935/999): loss=0.34370720078038647\n",
      "Gradient Descent(936/999): loss=0.3436995440569626\n",
      "Gradient Descent(937/999): loss=0.34369190428708174\n",
      "Gradient Descent(938/999): loss=0.3436842814298835\n",
      "Gradient Descent(939/999): loss=0.343676675444618\n",
      "Gradient Descent(940/999): loss=0.34366908629064613\n",
      "Gradient Descent(941/999): loss=0.3436615139274388\n",
      "Gradient Descent(942/999): loss=0.34365395831457696\n",
      "Gradient Descent(943/999): loss=0.34364641941175045\n",
      "Gradient Descent(944/999): loss=0.3436388971787587\n",
      "Gradient Descent(945/999): loss=0.3436313915755097\n",
      "Gradient Descent(946/999): loss=0.3436239025620199\n",
      "Gradient Descent(947/999): loss=0.34361643009841364\n",
      "Gradient Descent(948/999): loss=0.343608974144923\n",
      "Gradient Descent(949/999): loss=0.34360153466188753\n",
      "Gradient Descent(950/999): loss=0.3435941116097536\n",
      "Gradient Descent(951/999): loss=0.34358670494907445\n",
      "Gradient Descent(952/999): loss=0.34357931464050984\n",
      "Gradient Descent(953/999): loss=0.3435719406448249\n",
      "Gradient Descent(954/999): loss=0.3435645829228909\n",
      "Gradient Descent(955/999): loss=0.34355724143568445\n",
      "Gradient Descent(956/999): loss=0.3435499161442868\n",
      "Gradient Descent(957/999): loss=0.34354260700988426\n",
      "Gradient Descent(958/999): loss=0.3435353139937674\n",
      "Gradient Descent(959/999): loss=0.34352803705733037\n",
      "Gradient Descent(960/999): loss=0.3435207761620716\n",
      "Gradient Descent(961/999): loss=0.34351353126959233\n",
      "Gradient Descent(962/999): loss=0.3435063023415973\n",
      "Gradient Descent(963/999): loss=0.3434990893398935\n",
      "Gradient Descent(964/999): loss=0.34349189222639065\n",
      "Gradient Descent(965/999): loss=0.3434847109631002\n",
      "Gradient Descent(966/999): loss=0.34347754551213566\n",
      "Gradient Descent(967/999): loss=0.3434703958357115\n",
      "Gradient Descent(968/999): loss=0.3434632618961438\n",
      "Gradient Descent(969/999): loss=0.3434561436558493\n",
      "Gradient Descent(970/999): loss=0.3434490410773447\n",
      "Gradient Descent(971/999): loss=0.3434419541232473\n",
      "Gradient Descent(972/999): loss=0.3434348827562744\n",
      "Gradient Descent(973/999): loss=0.3434278269392422\n",
      "Gradient Descent(974/999): loss=0.3434207866350666\n",
      "Gradient Descent(975/999): loss=0.34341376180676225\n",
      "Gradient Descent(976/999): loss=0.34340675241744256\n",
      "Gradient Descent(977/999): loss=0.3433997584303188\n",
      "Gradient Descent(978/999): loss=0.34339277980870064\n",
      "Gradient Descent(979/999): loss=0.3433858165159951\n",
      "Gradient Descent(980/999): loss=0.34337886851570676\n",
      "Gradient Descent(981/999): loss=0.3433719357714373\n",
      "Gradient Descent(982/999): loss=0.343365018246885\n",
      "Gradient Descent(983/999): loss=0.34335811590584464\n",
      "Gradient Descent(984/999): loss=0.3433512287122073\n",
      "Gradient Descent(985/999): loss=0.34334435662996\n",
      "Gradient Descent(986/999): loss=0.3433374996231846\n",
      "Gradient Descent(987/999): loss=0.34333065765605947\n",
      "Gradient Descent(988/999): loss=0.343323830692857\n",
      "Gradient Descent(989/999): loss=0.3433170186979443\n",
      "Gradient Descent(990/999): loss=0.34331022163578345\n",
      "Gradient Descent(991/999): loss=0.34330343947093006\n",
      "Gradient Descent(992/999): loss=0.3432966721680339\n",
      "Gradient Descent(993/999): loss=0.34328991969183814\n",
      "Gradient Descent(994/999): loss=0.3432831820071792\n",
      "Gradient Descent(995/999): loss=0.3432764590789867\n",
      "Gradient Descent(996/999): loss=0.34326975087228206\n",
      "Gradient Descent(997/999): loss=0.34326305735218005\n",
      "Gradient Descent(998/999): loss=0.34325637848388735\n",
      "Gradient Descent(999/999): loss=0.3432497142327018\n",
      "parameters:  [-0.31465042  0.04197621 -0.23534325 -0.17314583  0.02012779 -0.01187588\n",
      "  0.26245187 -0.01749717  0.20186582 -0.02009936 -0.00310099 -0.12085995\n",
      "  0.1184256  -0.01337476  0.18706606 -0.00096492 -0.00162282  0.18152135\n",
      " -0.00082432  0.00249176  0.08814838  0.00115061 -0.07005346 -0.10721263\n",
      "  0.02615664  0.02936407  0.02937945 -0.01606007 -0.01384844 -0.01391513\n",
      " -0.08729817]\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "# from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "# start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tX, w_initial, max_iters, gamma)\n",
    "# end_time = datetime.datetime.now()\n",
    "\n",
    "print('parameters: ',gradient_ws[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/999): loss=0.5\n",
      "Stochastic Gradient Descent(1/999): loss=0.49097577769773404\n",
      "Stochastic Gradient Descent(2/999): loss=0.4787077336190901\n",
      "Stochastic Gradient Descent(3/999): loss=0.48202057420798816\n",
      "Stochastic Gradient Descent(4/999): loss=0.4801342403316257\n",
      "Stochastic Gradient Descent(5/999): loss=0.4857553603991821\n",
      "Stochastic Gradient Descent(6/999): loss=0.4898959193756118\n",
      "Stochastic Gradient Descent(7/999): loss=0.4459445011035137\n",
      "Stochastic Gradient Descent(8/999): loss=0.4617479985429819\n",
      "Stochastic Gradient Descent(9/999): loss=0.4541295788246645\n",
      "Stochastic Gradient Descent(10/999): loss=0.44883650469220304\n",
      "Stochastic Gradient Descent(11/999): loss=0.45098555849902766\n",
      "Stochastic Gradient Descent(12/999): loss=0.42430456118339266\n",
      "Stochastic Gradient Descent(13/999): loss=0.48637765682007533\n",
      "Stochastic Gradient Descent(14/999): loss=0.43396794229063546\n",
      "Stochastic Gradient Descent(15/999): loss=0.4396159616559118\n",
      "Stochastic Gradient Descent(16/999): loss=0.45001427767209423\n",
      "Stochastic Gradient Descent(17/999): loss=0.43263183059865473\n",
      "Stochastic Gradient Descent(18/999): loss=0.4706189195456963\n",
      "Stochastic Gradient Descent(19/999): loss=0.4404668984080369\n",
      "Stochastic Gradient Descent(20/999): loss=0.48836216782713315\n",
      "Stochastic Gradient Descent(21/999): loss=0.43162718529737837\n",
      "Stochastic Gradient Descent(22/999): loss=0.4633118228704214\n",
      "Stochastic Gradient Descent(23/999): loss=0.41904430807948223\n",
      "Stochastic Gradient Descent(24/999): loss=0.41354839382324665\n",
      "Stochastic Gradient Descent(25/999): loss=0.40984874615725936\n",
      "Stochastic Gradient Descent(26/999): loss=0.4625237030804767\n",
      "Stochastic Gradient Descent(27/999): loss=0.4324704294492148\n",
      "Stochastic Gradient Descent(28/999): loss=0.36867724059771667\n",
      "Stochastic Gradient Descent(29/999): loss=0.4442981152974058\n",
      "Stochastic Gradient Descent(30/999): loss=0.42088904169029023\n",
      "Stochastic Gradient Descent(31/999): loss=0.37523180085975083\n",
      "Stochastic Gradient Descent(32/999): loss=0.3930528476069699\n",
      "Stochastic Gradient Descent(33/999): loss=0.39300885436672817\n",
      "Stochastic Gradient Descent(34/999): loss=0.37247790933969666\n",
      "Stochastic Gradient Descent(35/999): loss=0.3864815451361854\n",
      "Stochastic Gradient Descent(36/999): loss=0.448005938623779\n",
      "Stochastic Gradient Descent(37/999): loss=0.43268782952520285\n",
      "Stochastic Gradient Descent(38/999): loss=0.4510988219439721\n",
      "Stochastic Gradient Descent(39/999): loss=0.42759231564928935\n",
      "Stochastic Gradient Descent(40/999): loss=0.4001105000924741\n",
      "Stochastic Gradient Descent(41/999): loss=0.4367953683066767\n",
      "Stochastic Gradient Descent(42/999): loss=0.36448842204752274\n",
      "Stochastic Gradient Descent(43/999): loss=0.4215149406866059\n",
      "Stochastic Gradient Descent(44/999): loss=0.41553409748434306\n",
      "Stochastic Gradient Descent(45/999): loss=0.4384493720312518\n",
      "Stochastic Gradient Descent(46/999): loss=0.43689000515059967\n",
      "Stochastic Gradient Descent(47/999): loss=0.36952649703523305\n",
      "Stochastic Gradient Descent(48/999): loss=0.4396075713079764\n",
      "Stochastic Gradient Descent(49/999): loss=0.42330104174008176\n",
      "Stochastic Gradient Descent(50/999): loss=0.37075740048701017\n",
      "Stochastic Gradient Descent(51/999): loss=0.3852824420313765\n",
      "Stochastic Gradient Descent(52/999): loss=0.42798422168925304\n",
      "Stochastic Gradient Descent(53/999): loss=0.38749758497363657\n",
      "Stochastic Gradient Descent(54/999): loss=0.38335536851320934\n",
      "Stochastic Gradient Descent(55/999): loss=0.44065500101078053\n",
      "Stochastic Gradient Descent(56/999): loss=0.36659188337187026\n",
      "Stochastic Gradient Descent(57/999): loss=0.4273280309332755\n",
      "Stochastic Gradient Descent(58/999): loss=0.43370028434217106\n",
      "Stochastic Gradient Descent(59/999): loss=0.38771267217020877\n",
      "Stochastic Gradient Descent(60/999): loss=0.4151630032227259\n",
      "Stochastic Gradient Descent(61/999): loss=0.4141763806942082\n",
      "Stochastic Gradient Descent(62/999): loss=0.3670721724698121\n",
      "Stochastic Gradient Descent(63/999): loss=0.4245864079332074\n",
      "Stochastic Gradient Descent(64/999): loss=0.3715545760874669\n",
      "Stochastic Gradient Descent(65/999): loss=0.4305487025213165\n",
      "Stochastic Gradient Descent(66/999): loss=0.3847983875846175\n",
      "Stochastic Gradient Descent(67/999): loss=0.3805513511708877\n",
      "Stochastic Gradient Descent(68/999): loss=0.4100816158272136\n",
      "Stochastic Gradient Descent(69/999): loss=0.3980674701283116\n",
      "Stochastic Gradient Descent(70/999): loss=0.39026220955812463\n",
      "Stochastic Gradient Descent(71/999): loss=0.3147047495322832\n",
      "Stochastic Gradient Descent(72/999): loss=0.35472034838206595\n",
      "Stochastic Gradient Descent(73/999): loss=0.35748882408267546\n",
      "Stochastic Gradient Descent(74/999): loss=0.38746853204097315\n",
      "Stochastic Gradient Descent(75/999): loss=0.36981699955614156\n",
      "Stochastic Gradient Descent(76/999): loss=0.35432042447886125\n",
      "Stochastic Gradient Descent(77/999): loss=0.3674350398588944\n",
      "Stochastic Gradient Descent(78/999): loss=0.407572180760741\n",
      "Stochastic Gradient Descent(79/999): loss=0.3503002014326351\n",
      "Stochastic Gradient Descent(80/999): loss=0.4105951714260634\n",
      "Stochastic Gradient Descent(81/999): loss=0.39167240977416723\n",
      "Stochastic Gradient Descent(82/999): loss=0.36935832830447846\n",
      "Stochastic Gradient Descent(83/999): loss=0.3936687970132492\n",
      "Stochastic Gradient Descent(84/999): loss=0.36013951495448604\n",
      "Stochastic Gradient Descent(85/999): loss=0.33832581828703373\n",
      "Stochastic Gradient Descent(86/999): loss=0.3751540803001654\n",
      "Stochastic Gradient Descent(87/999): loss=0.3482037577914728\n",
      "Stochastic Gradient Descent(88/999): loss=0.43088437291286197\n",
      "Stochastic Gradient Descent(89/999): loss=0.4807950952151207\n",
      "Stochastic Gradient Descent(90/999): loss=0.38949490828313893\n",
      "Stochastic Gradient Descent(91/999): loss=0.4093675569965894\n",
      "Stochastic Gradient Descent(92/999): loss=0.4287452790007156\n",
      "Stochastic Gradient Descent(93/999): loss=0.3812038634492586\n",
      "Stochastic Gradient Descent(94/999): loss=0.3721020330663602\n",
      "Stochastic Gradient Descent(95/999): loss=0.39832284943888696\n",
      "Stochastic Gradient Descent(96/999): loss=0.35937753835348935\n",
      "Stochastic Gradient Descent(97/999): loss=0.34695813069542203\n",
      "Stochastic Gradient Descent(98/999): loss=0.3494691206238285\n",
      "Stochastic Gradient Descent(99/999): loss=0.3932650016729547\n",
      "Stochastic Gradient Descent(100/999): loss=0.36139441528009714\n",
      "Stochastic Gradient Descent(101/999): loss=0.35617445822824434\n",
      "Stochastic Gradient Descent(102/999): loss=0.3522202481057692\n",
      "Stochastic Gradient Descent(103/999): loss=0.3808743773265197\n",
      "Stochastic Gradient Descent(104/999): loss=0.31584245563841695\n",
      "Stochastic Gradient Descent(105/999): loss=0.44030696084767035\n",
      "Stochastic Gradient Descent(106/999): loss=0.34590237318724065\n",
      "Stochastic Gradient Descent(107/999): loss=0.3943592526091414\n",
      "Stochastic Gradient Descent(108/999): loss=0.4237013690166862\n",
      "Stochastic Gradient Descent(109/999): loss=0.36378435484604715\n",
      "Stochastic Gradient Descent(110/999): loss=0.3506968349529867\n",
      "Stochastic Gradient Descent(111/999): loss=0.46417450305453933\n",
      "Stochastic Gradient Descent(112/999): loss=0.3773048416788518\n",
      "Stochastic Gradient Descent(113/999): loss=0.3499281739827218\n",
      "Stochastic Gradient Descent(114/999): loss=0.34302616271927305\n",
      "Stochastic Gradient Descent(115/999): loss=0.29342747360913324\n",
      "Stochastic Gradient Descent(116/999): loss=0.31932008810431295\n",
      "Stochastic Gradient Descent(117/999): loss=0.5514595813302813\n",
      "Stochastic Gradient Descent(118/999): loss=0.3284193883325991\n",
      "Stochastic Gradient Descent(119/999): loss=0.35681556861148217\n",
      "Stochastic Gradient Descent(120/999): loss=0.30365415265374257\n",
      "Stochastic Gradient Descent(121/999): loss=0.347299677642707\n",
      "Stochastic Gradient Descent(122/999): loss=0.3864380517483448\n",
      "Stochastic Gradient Descent(123/999): loss=0.37751965986807323\n",
      "Stochastic Gradient Descent(124/999): loss=0.3292895417359143\n",
      "Stochastic Gradient Descent(125/999): loss=0.34637239166187733\n",
      "Stochastic Gradient Descent(126/999): loss=0.32463913164121977\n",
      "Stochastic Gradient Descent(127/999): loss=0.3065604517968737\n",
      "Stochastic Gradient Descent(128/999): loss=0.310879678271308\n",
      "Stochastic Gradient Descent(129/999): loss=0.40644001438542565\n",
      "Stochastic Gradient Descent(130/999): loss=0.41879731056094605\n",
      "Stochastic Gradient Descent(131/999): loss=0.42343951142808733\n",
      "Stochastic Gradient Descent(132/999): loss=0.3166215517368733\n",
      "Stochastic Gradient Descent(133/999): loss=0.4109504722672917\n",
      "Stochastic Gradient Descent(134/999): loss=0.37549160365491907\n",
      "Stochastic Gradient Descent(135/999): loss=0.417557386129354\n",
      "Stochastic Gradient Descent(136/999): loss=0.3073008777634066\n",
      "Stochastic Gradient Descent(137/999): loss=0.3841158257939014\n",
      "Stochastic Gradient Descent(138/999): loss=0.3636484614406718\n",
      "Stochastic Gradient Descent(139/999): loss=0.38698446601691133\n",
      "Stochastic Gradient Descent(140/999): loss=0.4337598148078039\n",
      "Stochastic Gradient Descent(141/999): loss=0.36787952621852954\n",
      "Stochastic Gradient Descent(142/999): loss=0.3783967862978153\n",
      "Stochastic Gradient Descent(143/999): loss=0.32981953784015716\n",
      "Stochastic Gradient Descent(144/999): loss=0.3530867456034088\n",
      "Stochastic Gradient Descent(145/999): loss=0.3404424659029711\n",
      "Stochastic Gradient Descent(146/999): loss=0.39417241666729347\n",
      "Stochastic Gradient Descent(147/999): loss=0.3571927312613955\n",
      "Stochastic Gradient Descent(148/999): loss=0.36088243040866386\n",
      "Stochastic Gradient Descent(149/999): loss=0.4259381265077109\n",
      "Stochastic Gradient Descent(150/999): loss=0.33637173774313645\n",
      "Stochastic Gradient Descent(151/999): loss=0.34176261157019927\n",
      "Stochastic Gradient Descent(152/999): loss=0.36822085017057604\n",
      "Stochastic Gradient Descent(153/999): loss=0.4068368195966114\n",
      "Stochastic Gradient Descent(154/999): loss=0.31958804058134954\n",
      "Stochastic Gradient Descent(155/999): loss=0.36582603790004353\n",
      "Stochastic Gradient Descent(156/999): loss=0.3716566801122788\n",
      "Stochastic Gradient Descent(157/999): loss=0.37242123140540434\n",
      "Stochastic Gradient Descent(158/999): loss=0.28552928913909015\n",
      "Stochastic Gradient Descent(159/999): loss=0.3235831942795653\n",
      "Stochastic Gradient Descent(160/999): loss=0.3180187258618595\n",
      "Stochastic Gradient Descent(161/999): loss=0.3590917488509717\n",
      "Stochastic Gradient Descent(162/999): loss=0.36833901596867996\n",
      "Stochastic Gradient Descent(163/999): loss=0.3646077349050688\n",
      "Stochastic Gradient Descent(164/999): loss=0.30914939466015007\n",
      "Stochastic Gradient Descent(165/999): loss=0.3250230791250257\n",
      "Stochastic Gradient Descent(166/999): loss=0.3709132565566302\n",
      "Stochastic Gradient Descent(167/999): loss=0.3829738856579801\n",
      "Stochastic Gradient Descent(168/999): loss=0.42359828715033815\n",
      "Stochastic Gradient Descent(169/999): loss=0.324518638773737\n",
      "Stochastic Gradient Descent(170/999): loss=0.3711969203274094\n",
      "Stochastic Gradient Descent(171/999): loss=0.3804752494635732\n",
      "Stochastic Gradient Descent(172/999): loss=0.30254623989563545\n",
      "Stochastic Gradient Descent(173/999): loss=0.4059164973585955\n",
      "Stochastic Gradient Descent(174/999): loss=0.3986124944994048\n",
      "Stochastic Gradient Descent(175/999): loss=0.37044541717730844\n",
      "Stochastic Gradient Descent(176/999): loss=0.3152864755258005\n",
      "Stochastic Gradient Descent(177/999): loss=0.3208776247134619\n",
      "Stochastic Gradient Descent(178/999): loss=0.3456936126218053\n",
      "Stochastic Gradient Descent(179/999): loss=0.29567543983700767\n",
      "Stochastic Gradient Descent(180/999): loss=0.3039603552339802\n",
      "Stochastic Gradient Descent(181/999): loss=0.3447889081751594\n",
      "Stochastic Gradient Descent(182/999): loss=0.43557715194437335\n",
      "Stochastic Gradient Descent(183/999): loss=0.26075415831338167\n",
      "Stochastic Gradient Descent(184/999): loss=0.37901225815452233\n",
      "Stochastic Gradient Descent(185/999): loss=0.35966581830384525\n",
      "Stochastic Gradient Descent(186/999): loss=0.39989810204247606\n",
      "Stochastic Gradient Descent(187/999): loss=0.37719917142928744\n",
      "Stochastic Gradient Descent(188/999): loss=0.355482717376995\n",
      "Stochastic Gradient Descent(189/999): loss=0.34452463533711025\n",
      "Stochastic Gradient Descent(190/999): loss=0.31218756896643884\n",
      "Stochastic Gradient Descent(191/999): loss=0.3195055298138202\n",
      "Stochastic Gradient Descent(192/999): loss=0.39042303474222273\n",
      "Stochastic Gradient Descent(193/999): loss=0.3574370531101203\n",
      "Stochastic Gradient Descent(194/999): loss=0.37573973463137345\n",
      "Stochastic Gradient Descent(195/999): loss=0.34419356784826294\n",
      "Stochastic Gradient Descent(196/999): loss=0.40243122282727767\n",
      "Stochastic Gradient Descent(197/999): loss=0.38026160988477437\n",
      "Stochastic Gradient Descent(198/999): loss=0.3606825534797795\n",
      "Stochastic Gradient Descent(199/999): loss=0.341141213562368\n",
      "Stochastic Gradient Descent(200/999): loss=0.27059441529287964\n",
      "Stochastic Gradient Descent(201/999): loss=0.3629872733651655\n",
      "Stochastic Gradient Descent(202/999): loss=0.35795817946178377\n",
      "Stochastic Gradient Descent(203/999): loss=0.3182843716687489\n",
      "Stochastic Gradient Descent(204/999): loss=0.3615711528286568\n",
      "Stochastic Gradient Descent(205/999): loss=0.4463160997549226\n",
      "Stochastic Gradient Descent(206/999): loss=0.31626851830134084\n",
      "Stochastic Gradient Descent(207/999): loss=0.3982755020455723\n",
      "Stochastic Gradient Descent(208/999): loss=0.35163888588254694\n",
      "Stochastic Gradient Descent(209/999): loss=0.3489070356347873\n",
      "Stochastic Gradient Descent(210/999): loss=0.4068618399538548\n",
      "Stochastic Gradient Descent(211/999): loss=0.32398482569437226\n",
      "Stochastic Gradient Descent(212/999): loss=0.37049406706000865\n",
      "Stochastic Gradient Descent(213/999): loss=0.38305896404675976\n",
      "Stochastic Gradient Descent(214/999): loss=0.4380702466132588\n",
      "Stochastic Gradient Descent(215/999): loss=0.34639708892684323\n",
      "Stochastic Gradient Descent(216/999): loss=0.4206530118577577\n",
      "Stochastic Gradient Descent(217/999): loss=0.3165073419591101\n",
      "Stochastic Gradient Descent(218/999): loss=0.3815521525461949\n",
      "Stochastic Gradient Descent(219/999): loss=0.372320030387248\n",
      "Stochastic Gradient Descent(220/999): loss=0.4032898724571688\n",
      "Stochastic Gradient Descent(221/999): loss=0.32122854926117234\n",
      "Stochastic Gradient Descent(222/999): loss=0.4112767183684893\n",
      "Stochastic Gradient Descent(223/999): loss=0.42737699695376963\n",
      "Stochastic Gradient Descent(224/999): loss=0.3226385682246231\n",
      "Stochastic Gradient Descent(225/999): loss=0.3019474935212249\n",
      "Stochastic Gradient Descent(226/999): loss=0.34630958636877657\n",
      "Stochastic Gradient Descent(227/999): loss=0.4435960676556148\n",
      "Stochastic Gradient Descent(228/999): loss=0.3570631848799619\n",
      "Stochastic Gradient Descent(229/999): loss=0.4588669944782127\n",
      "Stochastic Gradient Descent(230/999): loss=0.3522304220170357\n",
      "Stochastic Gradient Descent(231/999): loss=0.3274984549975969\n",
      "Stochastic Gradient Descent(232/999): loss=0.4008167871405803\n",
      "Stochastic Gradient Descent(233/999): loss=0.3260952638016945\n",
      "Stochastic Gradient Descent(234/999): loss=0.41877094238848206\n",
      "Stochastic Gradient Descent(235/999): loss=0.35826449676136285\n",
      "Stochastic Gradient Descent(236/999): loss=0.35515821446161466\n",
      "Stochastic Gradient Descent(237/999): loss=0.2561849872000027\n",
      "Stochastic Gradient Descent(238/999): loss=0.41885293865548157\n",
      "Stochastic Gradient Descent(239/999): loss=0.3928954161632856\n",
      "Stochastic Gradient Descent(240/999): loss=0.2956707413728701\n",
      "Stochastic Gradient Descent(241/999): loss=0.37875646904048027\n",
      "Stochastic Gradient Descent(242/999): loss=0.33813074639757157\n",
      "Stochastic Gradient Descent(243/999): loss=0.35443103507535123\n",
      "Stochastic Gradient Descent(244/999): loss=0.40832981281660546\n",
      "Stochastic Gradient Descent(245/999): loss=0.31142988224458945\n",
      "Stochastic Gradient Descent(246/999): loss=0.40593560101580645\n",
      "Stochastic Gradient Descent(247/999): loss=0.3086092830487933\n",
      "Stochastic Gradient Descent(248/999): loss=0.37459334341251344\n",
      "Stochastic Gradient Descent(249/999): loss=0.28665022274854185\n",
      "Stochastic Gradient Descent(250/999): loss=0.31909759350077216\n",
      "Stochastic Gradient Descent(251/999): loss=0.3516491514198439\n",
      "Stochastic Gradient Descent(252/999): loss=0.4280231461913558\n",
      "Stochastic Gradient Descent(253/999): loss=0.2739515902307696\n",
      "Stochastic Gradient Descent(254/999): loss=0.326757543226122\n",
      "Stochastic Gradient Descent(255/999): loss=0.28949094527847263\n",
      "Stochastic Gradient Descent(256/999): loss=0.3736815392220976\n",
      "Stochastic Gradient Descent(257/999): loss=0.3756636558636181\n",
      "Stochastic Gradient Descent(258/999): loss=0.364918062202715\n",
      "Stochastic Gradient Descent(259/999): loss=0.42486939192514056\n",
      "Stochastic Gradient Descent(260/999): loss=0.3964119041867222\n",
      "Stochastic Gradient Descent(261/999): loss=0.2831135010662076\n",
      "Stochastic Gradient Descent(262/999): loss=0.34163692610367064\n",
      "Stochastic Gradient Descent(263/999): loss=0.40738737943182096\n",
      "Stochastic Gradient Descent(264/999): loss=0.3277837583878151\n",
      "Stochastic Gradient Descent(265/999): loss=0.4063229033970556\n",
      "Stochastic Gradient Descent(266/999): loss=0.3994404297679904\n",
      "Stochastic Gradient Descent(267/999): loss=0.42996190934989104\n",
      "Stochastic Gradient Descent(268/999): loss=0.40042237107923356\n",
      "Stochastic Gradient Descent(269/999): loss=0.3619879342079693\n",
      "Stochastic Gradient Descent(270/999): loss=0.5057571306507258\n",
      "Stochastic Gradient Descent(271/999): loss=0.3455830146450238\n",
      "Stochastic Gradient Descent(272/999): loss=0.3577235821011229\n",
      "Stochastic Gradient Descent(273/999): loss=0.37823155873339465\n",
      "Stochastic Gradient Descent(274/999): loss=0.35088793678325914\n",
      "Stochastic Gradient Descent(275/999): loss=0.3966942506738288\n",
      "Stochastic Gradient Descent(276/999): loss=0.35275982105558756\n",
      "Stochastic Gradient Descent(277/999): loss=0.38754719014497074\n",
      "Stochastic Gradient Descent(278/999): loss=0.23182673238292315\n",
      "Stochastic Gradient Descent(279/999): loss=0.30130909732299005\n",
      "Stochastic Gradient Descent(280/999): loss=0.4953312552689424\n",
      "Stochastic Gradient Descent(281/999): loss=0.3035951676230737\n",
      "Stochastic Gradient Descent(282/999): loss=0.25321380312942926\n",
      "Stochastic Gradient Descent(283/999): loss=0.3021096650112068\n",
      "Stochastic Gradient Descent(284/999): loss=0.37254049742712575\n",
      "Stochastic Gradient Descent(285/999): loss=0.4721945059502145\n",
      "Stochastic Gradient Descent(286/999): loss=0.2885619138636964\n",
      "Stochastic Gradient Descent(287/999): loss=0.2828673023294926\n",
      "Stochastic Gradient Descent(288/999): loss=0.40927355510591357\n",
      "Stochastic Gradient Descent(289/999): loss=0.3640938856282166\n",
      "Stochastic Gradient Descent(290/999): loss=0.4025784996528808\n",
      "Stochastic Gradient Descent(291/999): loss=0.3668797522310806\n",
      "Stochastic Gradient Descent(292/999): loss=0.3372705809409333\n",
      "Stochastic Gradient Descent(293/999): loss=0.36716374224101905\n",
      "Stochastic Gradient Descent(294/999): loss=0.35944784652514267\n",
      "Stochastic Gradient Descent(295/999): loss=0.3368022281832402\n",
      "Stochastic Gradient Descent(296/999): loss=0.31895638610999777\n",
      "Stochastic Gradient Descent(297/999): loss=0.3700914917094348\n",
      "Stochastic Gradient Descent(298/999): loss=0.3792500046805531\n",
      "Stochastic Gradient Descent(299/999): loss=0.3971166114586805\n",
      "Stochastic Gradient Descent(300/999): loss=0.2804684705083146\n",
      "Stochastic Gradient Descent(301/999): loss=0.49402502765453243\n",
      "Stochastic Gradient Descent(302/999): loss=0.40008650523449746\n",
      "Stochastic Gradient Descent(303/999): loss=0.4253030797660442\n",
      "Stochastic Gradient Descent(304/999): loss=0.305089480918158\n",
      "Stochastic Gradient Descent(305/999): loss=0.40631321680664273\n",
      "Stochastic Gradient Descent(306/999): loss=0.3486177804008661\n",
      "Stochastic Gradient Descent(307/999): loss=0.31472007772946314\n",
      "Stochastic Gradient Descent(308/999): loss=0.37822177859825645\n",
      "Stochastic Gradient Descent(309/999): loss=0.3163307477101889\n",
      "Stochastic Gradient Descent(310/999): loss=0.3330605011241271\n",
      "Stochastic Gradient Descent(311/999): loss=0.3750148662851811\n",
      "Stochastic Gradient Descent(312/999): loss=0.3738299638100996\n",
      "Stochastic Gradient Descent(313/999): loss=0.42150978607713874\n",
      "Stochastic Gradient Descent(314/999): loss=0.32510811383529953\n",
      "Stochastic Gradient Descent(315/999): loss=0.313853242732103\n",
      "Stochastic Gradient Descent(316/999): loss=0.37080230597767927\n",
      "Stochastic Gradient Descent(317/999): loss=0.36823733476080456\n",
      "Stochastic Gradient Descent(318/999): loss=0.33589720885240715\n",
      "Stochastic Gradient Descent(319/999): loss=0.34324571933603815\n",
      "Stochastic Gradient Descent(320/999): loss=0.3563483344781575\n",
      "Stochastic Gradient Descent(321/999): loss=0.3533782825152654\n",
      "Stochastic Gradient Descent(322/999): loss=0.34813625960603944\n",
      "Stochastic Gradient Descent(323/999): loss=0.30776957674424454\n",
      "Stochastic Gradient Descent(324/999): loss=0.38997348522980424\n",
      "Stochastic Gradient Descent(325/999): loss=0.2921187100710527\n",
      "Stochastic Gradient Descent(326/999): loss=0.3599333692609875\n",
      "Stochastic Gradient Descent(327/999): loss=0.34543466516192745\n",
      "Stochastic Gradient Descent(328/999): loss=0.4429810838823854\n",
      "Stochastic Gradient Descent(329/999): loss=0.3090572000061169\n",
      "Stochastic Gradient Descent(330/999): loss=0.3515058772270724\n",
      "Stochastic Gradient Descent(331/999): loss=0.34592027845650364\n",
      "Stochastic Gradient Descent(332/999): loss=0.2730134386692051\n",
      "Stochastic Gradient Descent(333/999): loss=0.33307530207522773\n",
      "Stochastic Gradient Descent(334/999): loss=0.3210151320221529\n",
      "Stochastic Gradient Descent(335/999): loss=0.3755611043235874\n",
      "Stochastic Gradient Descent(336/999): loss=0.3635606844909593\n",
      "Stochastic Gradient Descent(337/999): loss=0.3470566344661515\n",
      "Stochastic Gradient Descent(338/999): loss=0.4361032647117488\n",
      "Stochastic Gradient Descent(339/999): loss=0.3647740207631647\n",
      "Stochastic Gradient Descent(340/999): loss=0.3499182150305491\n",
      "Stochastic Gradient Descent(341/999): loss=0.3754765835285381\n",
      "Stochastic Gradient Descent(342/999): loss=0.3227121115870378\n",
      "Stochastic Gradient Descent(343/999): loss=0.3519531874960349\n",
      "Stochastic Gradient Descent(344/999): loss=0.34980465348206735\n",
      "Stochastic Gradient Descent(345/999): loss=0.3738954907777803\n",
      "Stochastic Gradient Descent(346/999): loss=0.3811226155112805\n",
      "Stochastic Gradient Descent(347/999): loss=0.41327199181132296\n",
      "Stochastic Gradient Descent(348/999): loss=0.36427053918381597\n",
      "Stochastic Gradient Descent(349/999): loss=0.3098272595339315\n",
      "Stochastic Gradient Descent(350/999): loss=0.4274311129952202\n",
      "Stochastic Gradient Descent(351/999): loss=0.31242220196469944\n",
      "Stochastic Gradient Descent(352/999): loss=0.3149839312932761\n",
      "Stochastic Gradient Descent(353/999): loss=0.3299046045633542\n",
      "Stochastic Gradient Descent(354/999): loss=0.2999021824089207\n",
      "Stochastic Gradient Descent(355/999): loss=0.47846655507860925\n",
      "Stochastic Gradient Descent(356/999): loss=0.3733230343354488\n",
      "Stochastic Gradient Descent(357/999): loss=0.4039957719207648\n",
      "Stochastic Gradient Descent(358/999): loss=0.4004930914992745\n",
      "Stochastic Gradient Descent(359/999): loss=0.35476634428361065\n",
      "Stochastic Gradient Descent(360/999): loss=0.3382156962193584\n",
      "Stochastic Gradient Descent(361/999): loss=0.2833254431919038\n",
      "Stochastic Gradient Descent(362/999): loss=0.4427654975494534\n",
      "Stochastic Gradient Descent(363/999): loss=0.24968063711405222\n",
      "Stochastic Gradient Descent(364/999): loss=0.3992964667163586\n",
      "Stochastic Gradient Descent(365/999): loss=0.4143784927318866\n",
      "Stochastic Gradient Descent(366/999): loss=0.3404375444482744\n",
      "Stochastic Gradient Descent(367/999): loss=0.3614793420954203\n",
      "Stochastic Gradient Descent(368/999): loss=0.33855836204962303\n",
      "Stochastic Gradient Descent(369/999): loss=0.39956116630501237\n",
      "Stochastic Gradient Descent(370/999): loss=0.3519429296582362\n",
      "Stochastic Gradient Descent(371/999): loss=0.39447026914089567\n",
      "Stochastic Gradient Descent(372/999): loss=0.44457469509253544\n",
      "Stochastic Gradient Descent(373/999): loss=0.3699480205838763\n",
      "Stochastic Gradient Descent(374/999): loss=0.42065824918820227\n",
      "Stochastic Gradient Descent(375/999): loss=0.4376526783444355\n",
      "Stochastic Gradient Descent(376/999): loss=0.30661888472812326\n",
      "Stochastic Gradient Descent(377/999): loss=0.2555692984089082\n",
      "Stochastic Gradient Descent(378/999): loss=0.36855143285690695\n",
      "Stochastic Gradient Descent(379/999): loss=0.3907718665420723\n",
      "Stochastic Gradient Descent(380/999): loss=0.3911414151846083\n",
      "Stochastic Gradient Descent(381/999): loss=0.3950829686073235\n",
      "Stochastic Gradient Descent(382/999): loss=0.3376762163181267\n",
      "Stochastic Gradient Descent(383/999): loss=0.3686681259873583\n",
      "Stochastic Gradient Descent(384/999): loss=0.3702396885366243\n",
      "Stochastic Gradient Descent(385/999): loss=0.32253700796377516\n",
      "Stochastic Gradient Descent(386/999): loss=0.4021432426241826\n",
      "Stochastic Gradient Descent(387/999): loss=0.4107176867685215\n",
      "Stochastic Gradient Descent(388/999): loss=0.3666270295538673\n",
      "Stochastic Gradient Descent(389/999): loss=0.3762749971257453\n",
      "Stochastic Gradient Descent(390/999): loss=0.38982187144281705\n",
      "Stochastic Gradient Descent(391/999): loss=0.427336136486229\n",
      "Stochastic Gradient Descent(392/999): loss=0.2860608663721596\n",
      "Stochastic Gradient Descent(393/999): loss=0.5103154949925441\n",
      "Stochastic Gradient Descent(394/999): loss=0.4013298636705518\n",
      "Stochastic Gradient Descent(395/999): loss=0.3539993493030948\n",
      "Stochastic Gradient Descent(396/999): loss=0.3755473114412521\n",
      "Stochastic Gradient Descent(397/999): loss=0.3100522445265092\n",
      "Stochastic Gradient Descent(398/999): loss=0.38600642298253923\n",
      "Stochastic Gradient Descent(399/999): loss=0.3874198850276109\n",
      "Stochastic Gradient Descent(400/999): loss=0.287940860408446\n",
      "Stochastic Gradient Descent(401/999): loss=0.3910843051396426\n",
      "Stochastic Gradient Descent(402/999): loss=0.36280192726694827\n",
      "Stochastic Gradient Descent(403/999): loss=0.335023605798167\n",
      "Stochastic Gradient Descent(404/999): loss=0.29147411242814975\n",
      "Stochastic Gradient Descent(405/999): loss=0.363961028459268\n",
      "Stochastic Gradient Descent(406/999): loss=0.3378234833651602\n",
      "Stochastic Gradient Descent(407/999): loss=0.31203099526511735\n",
      "Stochastic Gradient Descent(408/999): loss=0.2759402987264957\n",
      "Stochastic Gradient Descent(409/999): loss=0.3678601160430814\n",
      "Stochastic Gradient Descent(410/999): loss=0.34672502338688915\n",
      "Stochastic Gradient Descent(411/999): loss=0.32555966274652304\n",
      "Stochastic Gradient Descent(412/999): loss=0.3772226231299535\n",
      "Stochastic Gradient Descent(413/999): loss=0.37771040184663673\n",
      "Stochastic Gradient Descent(414/999): loss=0.32495393282530394\n",
      "Stochastic Gradient Descent(415/999): loss=0.24903845651870063\n",
      "Stochastic Gradient Descent(416/999): loss=0.3089848493283641\n",
      "Stochastic Gradient Descent(417/999): loss=0.2766097952964925\n",
      "Stochastic Gradient Descent(418/999): loss=0.3649764189239534\n",
      "Stochastic Gradient Descent(419/999): loss=0.4244378112080933\n",
      "Stochastic Gradient Descent(420/999): loss=0.27137034050409453\n",
      "Stochastic Gradient Descent(421/999): loss=0.2862014922611924\n",
      "Stochastic Gradient Descent(422/999): loss=0.3432235508134697\n",
      "Stochastic Gradient Descent(423/999): loss=0.30994872201891405\n",
      "Stochastic Gradient Descent(424/999): loss=0.32206810197493035\n",
      "Stochastic Gradient Descent(425/999): loss=0.3224164794259019\n",
      "Stochastic Gradient Descent(426/999): loss=0.3687939866338125\n",
      "Stochastic Gradient Descent(427/999): loss=0.38483401988874133\n",
      "Stochastic Gradient Descent(428/999): loss=0.3154771961611037\n",
      "Stochastic Gradient Descent(429/999): loss=0.39384925603073434\n",
      "Stochastic Gradient Descent(430/999): loss=0.32925099757831783\n",
      "Stochastic Gradient Descent(431/999): loss=0.4113094942850256\n",
      "Stochastic Gradient Descent(432/999): loss=0.3612458443009943\n",
      "Stochastic Gradient Descent(433/999): loss=0.3014778294752307\n",
      "Stochastic Gradient Descent(434/999): loss=0.5165380739341731\n",
      "Stochastic Gradient Descent(435/999): loss=0.3849525066506846\n",
      "Stochastic Gradient Descent(436/999): loss=0.33985865036084634\n",
      "Stochastic Gradient Descent(437/999): loss=0.33621540613567147\n",
      "Stochastic Gradient Descent(438/999): loss=0.40269325872759587\n",
      "Stochastic Gradient Descent(439/999): loss=0.3184007770513502\n",
      "Stochastic Gradient Descent(440/999): loss=0.2983109859255297\n",
      "Stochastic Gradient Descent(441/999): loss=0.39862787501390334\n",
      "Stochastic Gradient Descent(442/999): loss=0.41625084433750525\n",
      "Stochastic Gradient Descent(443/999): loss=0.3237539239100411\n",
      "Stochastic Gradient Descent(444/999): loss=0.34461101708354236\n",
      "Stochastic Gradient Descent(445/999): loss=0.3164810634560542\n",
      "Stochastic Gradient Descent(446/999): loss=0.4279588197319566\n",
      "Stochastic Gradient Descent(447/999): loss=0.4142176634780796\n",
      "Stochastic Gradient Descent(448/999): loss=0.33798329828468887\n",
      "Stochastic Gradient Descent(449/999): loss=0.33066025241492225\n",
      "Stochastic Gradient Descent(450/999): loss=0.2780969544844013\n",
      "Stochastic Gradient Descent(451/999): loss=0.386533509549379\n",
      "Stochastic Gradient Descent(452/999): loss=0.35072803360035676\n",
      "Stochastic Gradient Descent(453/999): loss=0.3310457151000781\n",
      "Stochastic Gradient Descent(454/999): loss=0.3836536978265523\n",
      "Stochastic Gradient Descent(455/999): loss=0.3409952962357618\n",
      "Stochastic Gradient Descent(456/999): loss=0.4135496620275638\n",
      "Stochastic Gradient Descent(457/999): loss=0.3963752605984127\n",
      "Stochastic Gradient Descent(458/999): loss=0.35114272349901987\n",
      "Stochastic Gradient Descent(459/999): loss=0.37365842727665466\n",
      "Stochastic Gradient Descent(460/999): loss=0.3292948385623368\n",
      "Stochastic Gradient Descent(461/999): loss=0.36385383086145395\n",
      "Stochastic Gradient Descent(462/999): loss=0.3171498153611231\n",
      "Stochastic Gradient Descent(463/999): loss=0.2924664663631539\n",
      "Stochastic Gradient Descent(464/999): loss=0.391565076669235\n",
      "Stochastic Gradient Descent(465/999): loss=0.3413727294710312\n",
      "Stochastic Gradient Descent(466/999): loss=0.350487208825704\n",
      "Stochastic Gradient Descent(467/999): loss=0.38327703281721986\n",
      "Stochastic Gradient Descent(468/999): loss=0.388746496986929\n",
      "Stochastic Gradient Descent(469/999): loss=0.26909036606770265\n",
      "Stochastic Gradient Descent(470/999): loss=0.34532781616565167\n",
      "Stochastic Gradient Descent(471/999): loss=0.33580043818778527\n",
      "Stochastic Gradient Descent(472/999): loss=0.34349272380478674\n",
      "Stochastic Gradient Descent(473/999): loss=0.41100032859327124\n",
      "Stochastic Gradient Descent(474/999): loss=0.38093655098668916\n",
      "Stochastic Gradient Descent(475/999): loss=0.3134206903256498\n",
      "Stochastic Gradient Descent(476/999): loss=0.3580634472568891\n",
      "Stochastic Gradient Descent(477/999): loss=0.3637466168819911\n",
      "Stochastic Gradient Descent(478/999): loss=0.4001197664892364\n",
      "Stochastic Gradient Descent(479/999): loss=0.34427111965338375\n",
      "Stochastic Gradient Descent(480/999): loss=0.3533896701725317\n",
      "Stochastic Gradient Descent(481/999): loss=0.30788740999586184\n",
      "Stochastic Gradient Descent(482/999): loss=0.3975354954816869\n",
      "Stochastic Gradient Descent(483/999): loss=0.3878464100375432\n",
      "Stochastic Gradient Descent(484/999): loss=0.24545740913916153\n",
      "Stochastic Gradient Descent(485/999): loss=0.34659501432511375\n",
      "Stochastic Gradient Descent(486/999): loss=0.29648440739987136\n",
      "Stochastic Gradient Descent(487/999): loss=0.2953507411787165\n",
      "Stochastic Gradient Descent(488/999): loss=0.3355307301500979\n",
      "Stochastic Gradient Descent(489/999): loss=0.30611154474377433\n",
      "Stochastic Gradient Descent(490/999): loss=0.3904987625739749\n",
      "Stochastic Gradient Descent(491/999): loss=0.30279704957174475\n",
      "Stochastic Gradient Descent(492/999): loss=0.33681715084443753\n",
      "Stochastic Gradient Descent(493/999): loss=0.338137725480085\n",
      "Stochastic Gradient Descent(494/999): loss=0.31064368689773847\n",
      "Stochastic Gradient Descent(495/999): loss=0.34603328917953924\n",
      "Stochastic Gradient Descent(496/999): loss=0.46227311918205205\n",
      "Stochastic Gradient Descent(497/999): loss=0.3973972082426864\n",
      "Stochastic Gradient Descent(498/999): loss=0.3572227656850572\n",
      "Stochastic Gradient Descent(499/999): loss=0.4198930682724423\n",
      "Stochastic Gradient Descent(500/999): loss=0.3141898623059853\n",
      "Stochastic Gradient Descent(501/999): loss=0.31084936736808966\n",
      "Stochastic Gradient Descent(502/999): loss=0.38157816095940644\n",
      "Stochastic Gradient Descent(503/999): loss=0.3521578636571082\n",
      "Stochastic Gradient Descent(504/999): loss=0.34212984160350274\n",
      "Stochastic Gradient Descent(505/999): loss=0.36214618845505997\n",
      "Stochastic Gradient Descent(506/999): loss=0.33249919808691586\n",
      "Stochastic Gradient Descent(507/999): loss=0.3866897962766268\n",
      "Stochastic Gradient Descent(508/999): loss=0.4854704693909306\n",
      "Stochastic Gradient Descent(509/999): loss=0.3620197223277929\n",
      "Stochastic Gradient Descent(510/999): loss=0.3745504076704148\n",
      "Stochastic Gradient Descent(511/999): loss=0.3142402380943361\n",
      "Stochastic Gradient Descent(512/999): loss=0.3638007013557508\n",
      "Stochastic Gradient Descent(513/999): loss=0.3509177229005093\n",
      "Stochastic Gradient Descent(514/999): loss=0.3052971790092434\n",
      "Stochastic Gradient Descent(515/999): loss=0.32951550366657734\n",
      "Stochastic Gradient Descent(516/999): loss=0.3867498799410756\n",
      "Stochastic Gradient Descent(517/999): loss=0.3424922821354609\n",
      "Stochastic Gradient Descent(518/999): loss=0.31716161144729377\n",
      "Stochastic Gradient Descent(519/999): loss=0.31352430956460414\n",
      "Stochastic Gradient Descent(520/999): loss=0.2790949148299908\n",
      "Stochastic Gradient Descent(521/999): loss=0.3834675164355943\n",
      "Stochastic Gradient Descent(522/999): loss=0.3221907788479577\n",
      "Stochastic Gradient Descent(523/999): loss=0.3349188207442528\n",
      "Stochastic Gradient Descent(524/999): loss=0.3930384796842094\n",
      "Stochastic Gradient Descent(525/999): loss=0.3470079493784548\n",
      "Stochastic Gradient Descent(526/999): loss=0.293779569257849\n",
      "Stochastic Gradient Descent(527/999): loss=0.43215340222986603\n",
      "Stochastic Gradient Descent(528/999): loss=0.4475554383302074\n",
      "Stochastic Gradient Descent(529/999): loss=0.2941670503031016\n",
      "Stochastic Gradient Descent(530/999): loss=0.30428752347444193\n",
      "Stochastic Gradient Descent(531/999): loss=0.3347095821407465\n",
      "Stochastic Gradient Descent(532/999): loss=0.4433535853838091\n",
      "Stochastic Gradient Descent(533/999): loss=0.3122891296930344\n",
      "Stochastic Gradient Descent(534/999): loss=0.3744458029188958\n",
      "Stochastic Gradient Descent(535/999): loss=0.6363492410215779\n",
      "Stochastic Gradient Descent(536/999): loss=0.5274891941406528\n",
      "Stochastic Gradient Descent(537/999): loss=0.33743036631132806\n",
      "Stochastic Gradient Descent(538/999): loss=0.33054862636192917\n",
      "Stochastic Gradient Descent(539/999): loss=0.39977415524678617\n",
      "Stochastic Gradient Descent(540/999): loss=0.2858574939633594\n",
      "Stochastic Gradient Descent(541/999): loss=0.4340772521095076\n",
      "Stochastic Gradient Descent(542/999): loss=0.3343579435814536\n",
      "Stochastic Gradient Descent(543/999): loss=0.36512452851555516\n",
      "Stochastic Gradient Descent(544/999): loss=0.314260580726976\n",
      "Stochastic Gradient Descent(545/999): loss=0.3462174955340454\n",
      "Stochastic Gradient Descent(546/999): loss=0.259062369950568\n",
      "Stochastic Gradient Descent(547/999): loss=0.32437397435777165\n",
      "Stochastic Gradient Descent(548/999): loss=0.32673555487462436\n",
      "Stochastic Gradient Descent(549/999): loss=0.32560398836279303\n",
      "Stochastic Gradient Descent(550/999): loss=0.3184508769464605\n",
      "Stochastic Gradient Descent(551/999): loss=0.3565840264575244\n",
      "Stochastic Gradient Descent(552/999): loss=0.33831025351521354\n",
      "Stochastic Gradient Descent(553/999): loss=0.3648557150035158\n",
      "Stochastic Gradient Descent(554/999): loss=0.26077712637432776\n",
      "Stochastic Gradient Descent(555/999): loss=0.29968042298724473\n",
      "Stochastic Gradient Descent(556/999): loss=0.22449898354770112\n",
      "Stochastic Gradient Descent(557/999): loss=0.26633223824411695\n",
      "Stochastic Gradient Descent(558/999): loss=0.28495968112495984\n",
      "Stochastic Gradient Descent(559/999): loss=0.3016967321427443\n",
      "Stochastic Gradient Descent(560/999): loss=0.34685010617317175\n",
      "Stochastic Gradient Descent(561/999): loss=0.3101331745351385\n",
      "Stochastic Gradient Descent(562/999): loss=0.38245608186910046\n",
      "Stochastic Gradient Descent(563/999): loss=0.299537644253251\n",
      "Stochastic Gradient Descent(564/999): loss=0.3779023234637507\n",
      "Stochastic Gradient Descent(565/999): loss=0.3698775651495363\n",
      "Stochastic Gradient Descent(566/999): loss=0.3747761240628875\n",
      "Stochastic Gradient Descent(567/999): loss=0.3638080867081447\n",
      "Stochastic Gradient Descent(568/999): loss=0.3149948441216267\n",
      "Stochastic Gradient Descent(569/999): loss=0.3325833789644605\n",
      "Stochastic Gradient Descent(570/999): loss=0.3267201374757218\n",
      "Stochastic Gradient Descent(571/999): loss=0.350188921306548\n",
      "Stochastic Gradient Descent(572/999): loss=0.40745761134896213\n",
      "Stochastic Gradient Descent(573/999): loss=0.2870916231993887\n",
      "Stochastic Gradient Descent(574/999): loss=0.30479144609348574\n",
      "Stochastic Gradient Descent(575/999): loss=0.3372408669669469\n",
      "Stochastic Gradient Descent(576/999): loss=0.37160007636865494\n",
      "Stochastic Gradient Descent(577/999): loss=0.30363152240427743\n",
      "Stochastic Gradient Descent(578/999): loss=0.34322313698961127\n",
      "Stochastic Gradient Descent(579/999): loss=0.385342082987608\n",
      "Stochastic Gradient Descent(580/999): loss=0.3793856303891196\n",
      "Stochastic Gradient Descent(581/999): loss=0.2744285607884778\n",
      "Stochastic Gradient Descent(582/999): loss=0.3697677910822661\n",
      "Stochastic Gradient Descent(583/999): loss=0.27775072922049193\n",
      "Stochastic Gradient Descent(584/999): loss=0.28100881109590775\n",
      "Stochastic Gradient Descent(585/999): loss=0.3879081583832483\n",
      "Stochastic Gradient Descent(586/999): loss=0.294204701708742\n",
      "Stochastic Gradient Descent(587/999): loss=0.381941446419001\n",
      "Stochastic Gradient Descent(588/999): loss=0.37225266164043336\n",
      "Stochastic Gradient Descent(589/999): loss=0.4204983627301143\n",
      "Stochastic Gradient Descent(590/999): loss=0.40057764245927013\n",
      "Stochastic Gradient Descent(591/999): loss=0.289758541130129\n",
      "Stochastic Gradient Descent(592/999): loss=0.39720452493115865\n",
      "Stochastic Gradient Descent(593/999): loss=0.403157853284855\n",
      "Stochastic Gradient Descent(594/999): loss=0.375663380314019\n",
      "Stochastic Gradient Descent(595/999): loss=0.39719896389819004\n",
      "Stochastic Gradient Descent(596/999): loss=0.44752385922978677\n",
      "Stochastic Gradient Descent(597/999): loss=0.4264098088278904\n",
      "Stochastic Gradient Descent(598/999): loss=0.32404585013079634\n",
      "Stochastic Gradient Descent(599/999): loss=0.4192592738867765\n",
      "Stochastic Gradient Descent(600/999): loss=0.335937848952501\n",
      "Stochastic Gradient Descent(601/999): loss=0.38234889393509186\n",
      "Stochastic Gradient Descent(602/999): loss=0.37181129014127756\n",
      "Stochastic Gradient Descent(603/999): loss=0.3236695555661886\n",
      "Stochastic Gradient Descent(604/999): loss=0.30640179272206064\n",
      "Stochastic Gradient Descent(605/999): loss=0.42440474226371416\n",
      "Stochastic Gradient Descent(606/999): loss=0.25224434050323785\n",
      "Stochastic Gradient Descent(607/999): loss=0.44246074511010747\n",
      "Stochastic Gradient Descent(608/999): loss=0.3271680070030263\n",
      "Stochastic Gradient Descent(609/999): loss=0.39022216481275585\n",
      "Stochastic Gradient Descent(610/999): loss=0.3018071488934224\n",
      "Stochastic Gradient Descent(611/999): loss=0.33312041816493543\n",
      "Stochastic Gradient Descent(612/999): loss=0.3523450193367751\n",
      "Stochastic Gradient Descent(613/999): loss=0.4191215260222511\n",
      "Stochastic Gradient Descent(614/999): loss=0.3513799143726558\n",
      "Stochastic Gradient Descent(615/999): loss=0.38764567455136484\n",
      "Stochastic Gradient Descent(616/999): loss=0.3705082140629595\n",
      "Stochastic Gradient Descent(617/999): loss=0.38636816332568563\n",
      "Stochastic Gradient Descent(618/999): loss=0.33087877469592586\n",
      "Stochastic Gradient Descent(619/999): loss=0.34185381937455633\n",
      "Stochastic Gradient Descent(620/999): loss=0.33997212307768054\n",
      "Stochastic Gradient Descent(621/999): loss=0.325812743303638\n",
      "Stochastic Gradient Descent(622/999): loss=0.31064793205344926\n",
      "Stochastic Gradient Descent(623/999): loss=0.352658802915701\n",
      "Stochastic Gradient Descent(624/999): loss=0.38280613906971916\n",
      "Stochastic Gradient Descent(625/999): loss=0.39686570167624907\n",
      "Stochastic Gradient Descent(626/999): loss=0.2797442923468781\n",
      "Stochastic Gradient Descent(627/999): loss=0.2847811300070844\n",
      "Stochastic Gradient Descent(628/999): loss=0.3345677118077618\n",
      "Stochastic Gradient Descent(629/999): loss=0.3304633857108586\n",
      "Stochastic Gradient Descent(630/999): loss=0.2971970075022619\n",
      "Stochastic Gradient Descent(631/999): loss=0.31015979967602736\n",
      "Stochastic Gradient Descent(632/999): loss=0.3267941349708698\n",
      "Stochastic Gradient Descent(633/999): loss=0.34447668486126837\n",
      "Stochastic Gradient Descent(634/999): loss=0.26688213504314207\n",
      "Stochastic Gradient Descent(635/999): loss=0.33475391032669877\n",
      "Stochastic Gradient Descent(636/999): loss=0.35886281904817097\n",
      "Stochastic Gradient Descent(637/999): loss=0.4801337744956098\n",
      "Stochastic Gradient Descent(638/999): loss=0.3587313490696786\n",
      "Stochastic Gradient Descent(639/999): loss=0.31297630136328336\n",
      "Stochastic Gradient Descent(640/999): loss=0.337360656124292\n",
      "Stochastic Gradient Descent(641/999): loss=0.36977485341457594\n",
      "Stochastic Gradient Descent(642/999): loss=0.31733129017749595\n",
      "Stochastic Gradient Descent(643/999): loss=0.2927564525612315\n",
      "Stochastic Gradient Descent(644/999): loss=0.38294421412898044\n",
      "Stochastic Gradient Descent(645/999): loss=0.24124883971957226\n",
      "Stochastic Gradient Descent(646/999): loss=0.3466144275853573\n",
      "Stochastic Gradient Descent(647/999): loss=0.3522813800567035\n",
      "Stochastic Gradient Descent(648/999): loss=0.2802267464734128\n",
      "Stochastic Gradient Descent(649/999): loss=0.35519183032231266\n",
      "Stochastic Gradient Descent(650/999): loss=0.3685039298138728\n",
      "Stochastic Gradient Descent(651/999): loss=0.36124216989515867\n",
      "Stochastic Gradient Descent(652/999): loss=0.34443940034804643\n",
      "Stochastic Gradient Descent(653/999): loss=0.4459492117838589\n",
      "Stochastic Gradient Descent(654/999): loss=0.3460534956521483\n",
      "Stochastic Gradient Descent(655/999): loss=0.366372134514225\n",
      "Stochastic Gradient Descent(656/999): loss=0.38536873896713625\n",
      "Stochastic Gradient Descent(657/999): loss=0.3444370117588431\n",
      "Stochastic Gradient Descent(658/999): loss=0.35165476000105506\n",
      "Stochastic Gradient Descent(659/999): loss=0.3465523494128404\n",
      "Stochastic Gradient Descent(660/999): loss=0.3029864391414888\n",
      "Stochastic Gradient Descent(661/999): loss=0.3487169723738748\n",
      "Stochastic Gradient Descent(662/999): loss=0.3343676043873881\n",
      "Stochastic Gradient Descent(663/999): loss=0.33292459789929835\n",
      "Stochastic Gradient Descent(664/999): loss=0.43384540758766726\n",
      "Stochastic Gradient Descent(665/999): loss=0.35942362096211516\n",
      "Stochastic Gradient Descent(666/999): loss=0.36859719207051983\n",
      "Stochastic Gradient Descent(667/999): loss=0.3118452638915401\n",
      "Stochastic Gradient Descent(668/999): loss=0.43793630469439276\n",
      "Stochastic Gradient Descent(669/999): loss=0.31995688409229833\n",
      "Stochastic Gradient Descent(670/999): loss=0.3975301687397454\n",
      "Stochastic Gradient Descent(671/999): loss=0.2947443874794099\n",
      "Stochastic Gradient Descent(672/999): loss=0.3033407373186032\n",
      "Stochastic Gradient Descent(673/999): loss=0.3298833833076835\n",
      "Stochastic Gradient Descent(674/999): loss=0.3317235194625367\n",
      "Stochastic Gradient Descent(675/999): loss=0.38451981742386543\n",
      "Stochastic Gradient Descent(676/999): loss=0.37322601678880246\n",
      "Stochastic Gradient Descent(677/999): loss=0.286757030635113\n",
      "Stochastic Gradient Descent(678/999): loss=0.2823729256683437\n",
      "Stochastic Gradient Descent(679/999): loss=0.33628590533164326\n",
      "Stochastic Gradient Descent(680/999): loss=0.41738769085432764\n",
      "Stochastic Gradient Descent(681/999): loss=0.26253470762631215\n",
      "Stochastic Gradient Descent(682/999): loss=0.2689427484215962\n",
      "Stochastic Gradient Descent(683/999): loss=0.2955807594564346\n",
      "Stochastic Gradient Descent(684/999): loss=0.3407780484031768\n",
      "Stochastic Gradient Descent(685/999): loss=0.3295374907506685\n",
      "Stochastic Gradient Descent(686/999): loss=0.4015580076549296\n",
      "Stochastic Gradient Descent(687/999): loss=0.397792561124088\n",
      "Stochastic Gradient Descent(688/999): loss=0.4367663091288223\n",
      "Stochastic Gradient Descent(689/999): loss=0.26728311422803575\n",
      "Stochastic Gradient Descent(690/999): loss=0.37316291527899703\n",
      "Stochastic Gradient Descent(691/999): loss=0.4287365568155602\n",
      "Stochastic Gradient Descent(692/999): loss=0.33276900869032433\n",
      "Stochastic Gradient Descent(693/999): loss=0.3424159048235368\n",
      "Stochastic Gradient Descent(694/999): loss=0.34814776724839225\n",
      "Stochastic Gradient Descent(695/999): loss=0.29229015946947007\n",
      "Stochastic Gradient Descent(696/999): loss=0.28919349830589125\n",
      "Stochastic Gradient Descent(697/999): loss=0.46473825280048475\n",
      "Stochastic Gradient Descent(698/999): loss=0.36045507073956684\n",
      "Stochastic Gradient Descent(699/999): loss=0.30158265378206667\n",
      "Stochastic Gradient Descent(700/999): loss=0.3510632987874082\n",
      "Stochastic Gradient Descent(701/999): loss=0.30495128679131933\n",
      "Stochastic Gradient Descent(702/999): loss=0.31795328120231775\n",
      "Stochastic Gradient Descent(703/999): loss=0.41376980406283326\n",
      "Stochastic Gradient Descent(704/999): loss=0.3358949238406005\n",
      "Stochastic Gradient Descent(705/999): loss=0.3346214384787096\n",
      "Stochastic Gradient Descent(706/999): loss=0.3000398676024562\n",
      "Stochastic Gradient Descent(707/999): loss=0.4148222728711099\n",
      "Stochastic Gradient Descent(708/999): loss=0.3935856937977206\n",
      "Stochastic Gradient Descent(709/999): loss=0.3448503050820543\n",
      "Stochastic Gradient Descent(710/999): loss=0.3283906940281705\n",
      "Stochastic Gradient Descent(711/999): loss=0.36169521218954515\n",
      "Stochastic Gradient Descent(712/999): loss=0.2929029094390132\n",
      "Stochastic Gradient Descent(713/999): loss=0.45467110794224974\n",
      "Stochastic Gradient Descent(714/999): loss=0.338090929389208\n",
      "Stochastic Gradient Descent(715/999): loss=0.35325825098767505\n",
      "Stochastic Gradient Descent(716/999): loss=0.4109433157746327\n",
      "Stochastic Gradient Descent(717/999): loss=0.2532781865714337\n",
      "Stochastic Gradient Descent(718/999): loss=0.3121722577412029\n",
      "Stochastic Gradient Descent(719/999): loss=0.35748754571283514\n",
      "Stochastic Gradient Descent(720/999): loss=0.42571735754535694\n",
      "Stochastic Gradient Descent(721/999): loss=0.31004304236065927\n",
      "Stochastic Gradient Descent(722/999): loss=0.34168198548802914\n",
      "Stochastic Gradient Descent(723/999): loss=0.36635856875849954\n",
      "Stochastic Gradient Descent(724/999): loss=0.343298900429883\n",
      "Stochastic Gradient Descent(725/999): loss=0.3927985947052921\n",
      "Stochastic Gradient Descent(726/999): loss=0.2776797185416124\n",
      "Stochastic Gradient Descent(727/999): loss=0.328454696959432\n",
      "Stochastic Gradient Descent(728/999): loss=0.33766262029765487\n",
      "Stochastic Gradient Descent(729/999): loss=0.3616734404166673\n",
      "Stochastic Gradient Descent(730/999): loss=0.38048556089918834\n",
      "Stochastic Gradient Descent(731/999): loss=0.3470544195873797\n",
      "Stochastic Gradient Descent(732/999): loss=0.3452988224450379\n",
      "Stochastic Gradient Descent(733/999): loss=0.3016088184282567\n",
      "Stochastic Gradient Descent(734/999): loss=0.3543059576813725\n",
      "Stochastic Gradient Descent(735/999): loss=0.3150315602075641\n",
      "Stochastic Gradient Descent(736/999): loss=0.43657186176204993\n",
      "Stochastic Gradient Descent(737/999): loss=0.3208816815663911\n",
      "Stochastic Gradient Descent(738/999): loss=0.3908024256536142\n",
      "Stochastic Gradient Descent(739/999): loss=0.2617043610835268\n",
      "Stochastic Gradient Descent(740/999): loss=0.324816121736519\n",
      "Stochastic Gradient Descent(741/999): loss=0.3751836857041831\n",
      "Stochastic Gradient Descent(742/999): loss=0.32099729829153945\n",
      "Stochastic Gradient Descent(743/999): loss=0.3802548201410517\n",
      "Stochastic Gradient Descent(744/999): loss=0.34862974162019683\n",
      "Stochastic Gradient Descent(745/999): loss=0.35496236627298555\n",
      "Stochastic Gradient Descent(746/999): loss=0.40524462301238595\n",
      "Stochastic Gradient Descent(747/999): loss=0.41127957643383894\n",
      "Stochastic Gradient Descent(748/999): loss=0.2504424324600704\n",
      "Stochastic Gradient Descent(749/999): loss=0.4038740112858429\n",
      "Stochastic Gradient Descent(750/999): loss=0.31362222785407257\n",
      "Stochastic Gradient Descent(751/999): loss=0.345645370642494\n",
      "Stochastic Gradient Descent(752/999): loss=0.36748002426174253\n",
      "Stochastic Gradient Descent(753/999): loss=0.3683862486157898\n",
      "Stochastic Gradient Descent(754/999): loss=0.34582852307498557\n",
      "Stochastic Gradient Descent(755/999): loss=0.37062260374924605\n",
      "Stochastic Gradient Descent(756/999): loss=0.377831386632522\n",
      "Stochastic Gradient Descent(757/999): loss=0.3488628607201632\n",
      "Stochastic Gradient Descent(758/999): loss=0.3528146391833596\n",
      "Stochastic Gradient Descent(759/999): loss=0.30964833402638064\n",
      "Stochastic Gradient Descent(760/999): loss=0.3691331289359113\n",
      "Stochastic Gradient Descent(761/999): loss=0.2878800611979529\n",
      "Stochastic Gradient Descent(762/999): loss=0.3430527967602497\n",
      "Stochastic Gradient Descent(763/999): loss=0.3645742668467481\n",
      "Stochastic Gradient Descent(764/999): loss=0.3265990097585811\n",
      "Stochastic Gradient Descent(765/999): loss=0.3160567302580265\n",
      "Stochastic Gradient Descent(766/999): loss=0.2748910061102882\n",
      "Stochastic Gradient Descent(767/999): loss=0.3173668133863614\n",
      "Stochastic Gradient Descent(768/999): loss=0.3554181876111904\n",
      "Stochastic Gradient Descent(769/999): loss=0.4179342471125937\n",
      "Stochastic Gradient Descent(770/999): loss=0.31476776053692274\n",
      "Stochastic Gradient Descent(771/999): loss=0.3208567459303506\n",
      "Stochastic Gradient Descent(772/999): loss=0.33069674393283927\n",
      "Stochastic Gradient Descent(773/999): loss=0.28672629938198624\n",
      "Stochastic Gradient Descent(774/999): loss=0.3638202375994674\n",
      "Stochastic Gradient Descent(775/999): loss=0.363383668315514\n",
      "Stochastic Gradient Descent(776/999): loss=0.24060093099701785\n",
      "Stochastic Gradient Descent(777/999): loss=0.3991065277184533\n",
      "Stochastic Gradient Descent(778/999): loss=0.41168805795657115\n",
      "Stochastic Gradient Descent(779/999): loss=0.3288396615695912\n",
      "Stochastic Gradient Descent(780/999): loss=0.36819114638946976\n",
      "Stochastic Gradient Descent(781/999): loss=0.30092044747315594\n",
      "Stochastic Gradient Descent(782/999): loss=0.36254922464726375\n",
      "Stochastic Gradient Descent(783/999): loss=0.3260203590869902\n",
      "Stochastic Gradient Descent(784/999): loss=0.23370483475119602\n",
      "Stochastic Gradient Descent(785/999): loss=0.3747199373397257\n",
      "Stochastic Gradient Descent(786/999): loss=0.32171335357623126\n",
      "Stochastic Gradient Descent(787/999): loss=0.32694086235769937\n",
      "Stochastic Gradient Descent(788/999): loss=0.34806446215595754\n",
      "Stochastic Gradient Descent(789/999): loss=0.3196505719667165\n",
      "Stochastic Gradient Descent(790/999): loss=0.34113064979703117\n",
      "Stochastic Gradient Descent(791/999): loss=0.3890027845932068\n",
      "Stochastic Gradient Descent(792/999): loss=0.3776785217562037\n",
      "Stochastic Gradient Descent(793/999): loss=0.4243687665980707\n",
      "Stochastic Gradient Descent(794/999): loss=0.3151682363926809\n",
      "Stochastic Gradient Descent(795/999): loss=0.3785538253411312\n",
      "Stochastic Gradient Descent(796/999): loss=0.3746342529815424\n",
      "Stochastic Gradient Descent(797/999): loss=0.35056492658110394\n",
      "Stochastic Gradient Descent(798/999): loss=0.3297669184096982\n",
      "Stochastic Gradient Descent(799/999): loss=0.4435747766759522\n",
      "Stochastic Gradient Descent(800/999): loss=0.30201019583755984\n",
      "Stochastic Gradient Descent(801/999): loss=0.36438964575591526\n",
      "Stochastic Gradient Descent(802/999): loss=0.3637801103863396\n",
      "Stochastic Gradient Descent(803/999): loss=0.2851157573466606\n",
      "Stochastic Gradient Descent(804/999): loss=0.31178209249156447\n",
      "Stochastic Gradient Descent(805/999): loss=0.40322208489729783\n",
      "Stochastic Gradient Descent(806/999): loss=0.32912293723657415\n",
      "Stochastic Gradient Descent(807/999): loss=0.3795208288673224\n",
      "Stochastic Gradient Descent(808/999): loss=0.3535815318365179\n",
      "Stochastic Gradient Descent(809/999): loss=0.3391897819832922\n",
      "Stochastic Gradient Descent(810/999): loss=0.35738652444892155\n",
      "Stochastic Gradient Descent(811/999): loss=0.31081782034154676\n",
      "Stochastic Gradient Descent(812/999): loss=0.3105511225942429\n",
      "Stochastic Gradient Descent(813/999): loss=0.3422780464063888\n",
      "Stochastic Gradient Descent(814/999): loss=0.46730066274941395\n",
      "Stochastic Gradient Descent(815/999): loss=0.45254012573907104\n",
      "Stochastic Gradient Descent(816/999): loss=0.277855188165712\n",
      "Stochastic Gradient Descent(817/999): loss=0.37437995720410217\n",
      "Stochastic Gradient Descent(818/999): loss=0.37482302726230265\n",
      "Stochastic Gradient Descent(819/999): loss=0.36904252188116726\n",
      "Stochastic Gradient Descent(820/999): loss=0.365144277165831\n",
      "Stochastic Gradient Descent(821/999): loss=0.38134996124997145\n",
      "Stochastic Gradient Descent(822/999): loss=0.32009264382964403\n",
      "Stochastic Gradient Descent(823/999): loss=0.3458014468912558\n",
      "Stochastic Gradient Descent(824/999): loss=0.30346661474011755\n",
      "Stochastic Gradient Descent(825/999): loss=0.3698988313667083\n",
      "Stochastic Gradient Descent(826/999): loss=0.3935110827879898\n",
      "Stochastic Gradient Descent(827/999): loss=0.34692996827172523\n",
      "Stochastic Gradient Descent(828/999): loss=0.334592012663846\n",
      "Stochastic Gradient Descent(829/999): loss=0.3444823314161772\n",
      "Stochastic Gradient Descent(830/999): loss=0.39782494049563577\n",
      "Stochastic Gradient Descent(831/999): loss=0.37122637180890355\n",
      "Stochastic Gradient Descent(832/999): loss=0.3854683441212672\n",
      "Stochastic Gradient Descent(833/999): loss=0.37851340689826446\n",
      "Stochastic Gradient Descent(834/999): loss=0.3781057577534662\n",
      "Stochastic Gradient Descent(835/999): loss=0.2940500357019181\n",
      "Stochastic Gradient Descent(836/999): loss=0.3192257206187062\n",
      "Stochastic Gradient Descent(837/999): loss=0.3756909991601562\n",
      "Stochastic Gradient Descent(838/999): loss=0.40674896893257917\n",
      "Stochastic Gradient Descent(839/999): loss=0.34750226532367406\n",
      "Stochastic Gradient Descent(840/999): loss=0.3066837949343403\n",
      "Stochastic Gradient Descent(841/999): loss=0.4299006423043303\n",
      "Stochastic Gradient Descent(842/999): loss=0.36011782776964535\n",
      "Stochastic Gradient Descent(843/999): loss=0.3836616713594507\n",
      "Stochastic Gradient Descent(844/999): loss=0.3430955281801419\n",
      "Stochastic Gradient Descent(845/999): loss=0.3191916277628781\n",
      "Stochastic Gradient Descent(846/999): loss=0.32376923473133623\n",
      "Stochastic Gradient Descent(847/999): loss=0.35717438187658845\n",
      "Stochastic Gradient Descent(848/999): loss=0.3164737800877787\n",
      "Stochastic Gradient Descent(849/999): loss=0.34624442616776285\n",
      "Stochastic Gradient Descent(850/999): loss=0.3208626177788973\n",
      "Stochastic Gradient Descent(851/999): loss=0.3252218659081833\n",
      "Stochastic Gradient Descent(852/999): loss=0.3190140692042283\n",
      "Stochastic Gradient Descent(853/999): loss=0.3263005423233913\n",
      "Stochastic Gradient Descent(854/999): loss=0.2719183232939807\n",
      "Stochastic Gradient Descent(855/999): loss=0.3287727773803744\n",
      "Stochastic Gradient Descent(856/999): loss=0.3987126169736542\n",
      "Stochastic Gradient Descent(857/999): loss=0.4284425742719591\n",
      "Stochastic Gradient Descent(858/999): loss=0.3712541788464543\n",
      "Stochastic Gradient Descent(859/999): loss=0.3065284514003485\n",
      "Stochastic Gradient Descent(860/999): loss=0.324493372845612\n",
      "Stochastic Gradient Descent(861/999): loss=0.24980455912884736\n",
      "Stochastic Gradient Descent(862/999): loss=0.38575894597271526\n",
      "Stochastic Gradient Descent(863/999): loss=0.3443522773109494\n",
      "Stochastic Gradient Descent(864/999): loss=0.3889649827619921\n",
      "Stochastic Gradient Descent(865/999): loss=0.3268855212385621\n",
      "Stochastic Gradient Descent(866/999): loss=0.33395300290210644\n",
      "Stochastic Gradient Descent(867/999): loss=0.35858659448772895\n",
      "Stochastic Gradient Descent(868/999): loss=0.3919591383502173\n",
      "Stochastic Gradient Descent(869/999): loss=0.34047366156171355\n",
      "Stochastic Gradient Descent(870/999): loss=0.2581368152671368\n",
      "Stochastic Gradient Descent(871/999): loss=0.42821006343732554\n",
      "Stochastic Gradient Descent(872/999): loss=0.3481390115549071\n",
      "Stochastic Gradient Descent(873/999): loss=0.26991843129644244\n",
      "Stochastic Gradient Descent(874/999): loss=0.3519028632280643\n",
      "Stochastic Gradient Descent(875/999): loss=0.35128165275776146\n",
      "Stochastic Gradient Descent(876/999): loss=0.34133002725059663\n",
      "Stochastic Gradient Descent(877/999): loss=0.2727027390430943\n",
      "Stochastic Gradient Descent(878/999): loss=0.3052472656300997\n",
      "Stochastic Gradient Descent(879/999): loss=0.35682013044012595\n",
      "Stochastic Gradient Descent(880/999): loss=0.3210689384676839\n",
      "Stochastic Gradient Descent(881/999): loss=0.37815988596149636\n",
      "Stochastic Gradient Descent(882/999): loss=0.379429448967907\n",
      "Stochastic Gradient Descent(883/999): loss=0.352292479630756\n",
      "Stochastic Gradient Descent(884/999): loss=0.3229046515699807\n",
      "Stochastic Gradient Descent(885/999): loss=0.2593216382579634\n",
      "Stochastic Gradient Descent(886/999): loss=0.39504724946883746\n",
      "Stochastic Gradient Descent(887/999): loss=0.3546626049924491\n",
      "Stochastic Gradient Descent(888/999): loss=0.3230965809900562\n",
      "Stochastic Gradient Descent(889/999): loss=0.2879700669276952\n",
      "Stochastic Gradient Descent(890/999): loss=0.306720429921764\n",
      "Stochastic Gradient Descent(891/999): loss=0.3377191816356533\n",
      "Stochastic Gradient Descent(892/999): loss=0.3415198121379013\n",
      "Stochastic Gradient Descent(893/999): loss=0.37319565207277683\n",
      "Stochastic Gradient Descent(894/999): loss=0.3835607972256728\n",
      "Stochastic Gradient Descent(895/999): loss=0.37014747069267206\n",
      "Stochastic Gradient Descent(896/999): loss=0.3221677026390293\n",
      "Stochastic Gradient Descent(897/999): loss=0.3693190309751841\n",
      "Stochastic Gradient Descent(898/999): loss=0.3714459910962294\n",
      "Stochastic Gradient Descent(899/999): loss=0.36756296692091567\n",
      "Stochastic Gradient Descent(900/999): loss=0.2624700736267764\n",
      "Stochastic Gradient Descent(901/999): loss=0.3662844949943468\n",
      "Stochastic Gradient Descent(902/999): loss=0.3136579719492923\n",
      "Stochastic Gradient Descent(903/999): loss=0.32083173564913586\n",
      "Stochastic Gradient Descent(904/999): loss=0.27103469260344293\n",
      "Stochastic Gradient Descent(905/999): loss=0.3833188978751304\n",
      "Stochastic Gradient Descent(906/999): loss=0.3156127759949489\n",
      "Stochastic Gradient Descent(907/999): loss=0.30022471242938215\n",
      "Stochastic Gradient Descent(908/999): loss=0.3487789063751376\n",
      "Stochastic Gradient Descent(909/999): loss=0.3679049676154606\n",
      "Stochastic Gradient Descent(910/999): loss=0.3536929070827633\n",
      "Stochastic Gradient Descent(911/999): loss=0.28029709151237203\n",
      "Stochastic Gradient Descent(912/999): loss=0.3901680941119585\n",
      "Stochastic Gradient Descent(913/999): loss=0.4068295228287716\n",
      "Stochastic Gradient Descent(914/999): loss=0.35976087537553303\n",
      "Stochastic Gradient Descent(915/999): loss=0.35091877957540774\n",
      "Stochastic Gradient Descent(916/999): loss=0.3513418077040084\n",
      "Stochastic Gradient Descent(917/999): loss=0.36722703602943385\n",
      "Stochastic Gradient Descent(918/999): loss=0.3249018218203598\n",
      "Stochastic Gradient Descent(919/999): loss=0.28844301326968\n",
      "Stochastic Gradient Descent(920/999): loss=0.38465508413466687\n",
      "Stochastic Gradient Descent(921/999): loss=0.3762906495027342\n",
      "Stochastic Gradient Descent(922/999): loss=0.3705867714190607\n",
      "Stochastic Gradient Descent(923/999): loss=0.3344282853795596\n",
      "Stochastic Gradient Descent(924/999): loss=0.3815948797891784\n",
      "Stochastic Gradient Descent(925/999): loss=0.3144560986180201\n",
      "Stochastic Gradient Descent(926/999): loss=0.33487035853350944\n",
      "Stochastic Gradient Descent(927/999): loss=0.3245946629794688\n",
      "Stochastic Gradient Descent(928/999): loss=0.40355813554180076\n",
      "Stochastic Gradient Descent(929/999): loss=0.448157257125941\n",
      "Stochastic Gradient Descent(930/999): loss=0.2995572431846746\n",
      "Stochastic Gradient Descent(931/999): loss=0.3805249669639633\n",
      "Stochastic Gradient Descent(932/999): loss=0.31113549934696927\n",
      "Stochastic Gradient Descent(933/999): loss=0.35672020051260295\n",
      "Stochastic Gradient Descent(934/999): loss=0.2608821340083542\n",
      "Stochastic Gradient Descent(935/999): loss=0.3813958327791107\n",
      "Stochastic Gradient Descent(936/999): loss=0.36961615310420554\n",
      "Stochastic Gradient Descent(937/999): loss=0.3203528571854439\n",
      "Stochastic Gradient Descent(938/999): loss=0.3811476567498259\n",
      "Stochastic Gradient Descent(939/999): loss=0.3555789491059137\n",
      "Stochastic Gradient Descent(940/999): loss=0.28887423609856083\n",
      "Stochastic Gradient Descent(941/999): loss=0.42415685358262684\n",
      "Stochastic Gradient Descent(942/999): loss=0.31579490194793547\n",
      "Stochastic Gradient Descent(943/999): loss=0.3467117480972572\n",
      "Stochastic Gradient Descent(944/999): loss=0.39309982333044646\n",
      "Stochastic Gradient Descent(945/999): loss=0.35409000668772345\n",
      "Stochastic Gradient Descent(946/999): loss=0.3551701808895411\n",
      "Stochastic Gradient Descent(947/999): loss=0.32181519974150996\n",
      "Stochastic Gradient Descent(948/999): loss=0.3053313421168796\n",
      "Stochastic Gradient Descent(949/999): loss=0.28208728386415133\n",
      "Stochastic Gradient Descent(950/999): loss=0.3382931882427468\n",
      "Stochastic Gradient Descent(951/999): loss=0.32195615679086664\n",
      "Stochastic Gradient Descent(952/999): loss=0.3105766030936088\n",
      "Stochastic Gradient Descent(953/999): loss=0.3513102453737028\n",
      "Stochastic Gradient Descent(954/999): loss=0.315377167380775\n",
      "Stochastic Gradient Descent(955/999): loss=0.28834766864337597\n",
      "Stochastic Gradient Descent(956/999): loss=0.24429605818599756\n",
      "Stochastic Gradient Descent(957/999): loss=0.39900179302283934\n",
      "Stochastic Gradient Descent(958/999): loss=0.4574575846949927\n",
      "Stochastic Gradient Descent(959/999): loss=0.3060375339373294\n",
      "Stochastic Gradient Descent(960/999): loss=0.3098950571392945\n",
      "Stochastic Gradient Descent(961/999): loss=0.30580272722543644\n",
      "Stochastic Gradient Descent(962/999): loss=0.27524164603759255\n",
      "Stochastic Gradient Descent(963/999): loss=0.2782744539862531\n",
      "Stochastic Gradient Descent(964/999): loss=0.3139055399995769\n",
      "Stochastic Gradient Descent(965/999): loss=0.3461535223579672\n",
      "Stochastic Gradient Descent(966/999): loss=0.49069323916446494\n",
      "Stochastic Gradient Descent(967/999): loss=0.3665230276421414\n",
      "Stochastic Gradient Descent(968/999): loss=0.35399731450151495\n",
      "Stochastic Gradient Descent(969/999): loss=0.35081117753745533\n",
      "Stochastic Gradient Descent(970/999): loss=0.40121071057839797\n",
      "Stochastic Gradient Descent(971/999): loss=0.3674733353225788\n",
      "Stochastic Gradient Descent(972/999): loss=0.39020426748634596\n",
      "Stochastic Gradient Descent(973/999): loss=0.32095222010634034\n",
      "Stochastic Gradient Descent(974/999): loss=0.381895098496734\n",
      "Stochastic Gradient Descent(975/999): loss=0.34135113269981304\n",
      "Stochastic Gradient Descent(976/999): loss=0.34041226206931724\n",
      "Stochastic Gradient Descent(977/999): loss=0.31125554155460383\n",
      "Stochastic Gradient Descent(978/999): loss=0.3383555584473739\n",
      "Stochastic Gradient Descent(979/999): loss=0.3651043695871963\n",
      "Stochastic Gradient Descent(980/999): loss=0.32144765644214757\n",
      "Stochastic Gradient Descent(981/999): loss=0.3647587180573399\n",
      "Stochastic Gradient Descent(982/999): loss=0.4466804538562247\n",
      "Stochastic Gradient Descent(983/999): loss=0.28163044935986187\n",
      "Stochastic Gradient Descent(984/999): loss=0.3590621636247328\n",
      "Stochastic Gradient Descent(985/999): loss=0.38370717879677085\n",
      "Stochastic Gradient Descent(986/999): loss=0.29151293740222434\n",
      "Stochastic Gradient Descent(987/999): loss=0.3655097693646649\n",
      "Stochastic Gradient Descent(988/999): loss=0.390292417717523\n",
      "Stochastic Gradient Descent(989/999): loss=0.3698724665163101\n",
      "Stochastic Gradient Descent(990/999): loss=0.34070175129118657\n",
      "Stochastic Gradient Descent(991/999): loss=0.4107438162007241\n",
      "Stochastic Gradient Descent(992/999): loss=0.2803772444771562\n",
      "Stochastic Gradient Descent(993/999): loss=0.32095699182339743\n",
      "Stochastic Gradient Descent(994/999): loss=0.3639620333194123\n",
      "Stochastic Gradient Descent(995/999): loss=0.3204270081794219\n",
      "Stochastic Gradient Descent(996/999): loss=0.23983448115249467\n",
      "Stochastic Gradient Descent(997/999): loss=0.27072711390669985\n",
      "Stochastic Gradient Descent(998/999): loss=0.3601944952120815\n",
      "Stochastic Gradient Descent(999/999): loss=0.35091407843455225\n",
      "[-0.3031079   0.04172662 -0.24364712 -0.17609974  0.02699585 -0.00239365\n",
      "  0.26400878 -0.0078386   0.19894408  0.01126035 -0.00273747 -0.12059403\n",
      "  0.12047113 -0.0038364   0.2063264  -0.00035222 -0.00732184  0.18496773\n",
      " -0.00656018  0.00394357  0.0949545   0.00170345 -0.08277771 -0.11245229\n",
      "  0.02885788  0.03308292  0.0330643  -0.00738899 -0.00445602 -0.0044607\n",
      " -0.09204884]\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "batch_size = 50\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "# start_time = datetime.datetime.now()\n",
    "stoch_gradient_losses, stoch_gradient_ws = stochastic_gradient_descent(y, tX, w_initial, batch_size, max_iters, gamma)\n",
    "# end_time = datetime.datetime.now()\n",
    "\n",
    "print(stoch_gradient_ws[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.339686809908\n",
      "[  8.03908656e-05  -7.20112725e-03  -6.05470621e-03  -5.47503706e-04\n",
      "  -1.93917581e-02   4.73466159e-04  -2.60377150e-02   3.25108506e-01\n",
      "  -3.80835171e-05  -2.72729929e+00  -2.21218331e-01   9.50817492e-02\n",
      "   6.40392496e-02   2.73555898e+00  -3.31801082e-04  -9.54327750e-04\n",
      "   2.74031566e+00  -5.34164922e-04   9.73498609e-04   3.69225052e-03\n",
      "   3.54487428e-04  -5.43344599e-04  -3.30448035e-01  -1.40800497e-03\n",
      "   8.31432882e-04   1.02117272e-03  -1.68047416e-03  -5.83664815e-03\n",
      "  -1.11087997e-02   2.72775918e+00]\n"
     ]
    }
   ],
   "source": [
    "# start_ls_time = datetime.datetime.now()\n",
    "ls_wopt, ls_loss = least_squares(y,tX)\n",
    "print(ls_loss)\n",
    "print(ls_wopt)\n",
    "# end_ls_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.30849412  0.03572402 -0.24077855 -0.21610016 -0.01054923 -0.01919922\n",
      "  0.34814992 -0.03018983  0.23504977 -0.01013674 -0.00087247 -0.15674991\n",
      "  0.11455458 -0.02161131  0.18503225 -0.00076269 -0.00130455  0.23850622\n",
      " -0.00086979  0.00249252  0.10383837  0.00113401 -0.06198121 -0.14991608\n",
      "  0.03645409  0.04367213  0.0436772  -0.02328488 -0.02309077 -0.0234825\n",
      " -0.09703028]\n"
     ]
    }
   ],
   "source": [
    "# lambdas = np.logspace(-3, 1, 10)      \n",
    "# φ_x = build_poly(x, degree)\n",
    "# x_train, x_test, y_train, y_test = split_data(tX, y, ratio, seed)\n",
    "    \n",
    "#     for lamb in lambdas:\n",
    "\n",
    "w_ridge = ridge_regression(y, tX, 0.01)\n",
    "\n",
    "print(w_ridge)\n",
    "\n",
    "# rmse_tr = np.sqrt(2*compute_loss(y, tX, w_ridge))\n",
    "# rmse_te = np.sqrt(2*compute_loss(y, tX, w_ridge))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '/home/sarigian/Desktop/project1/Data/test.csv' # TODO: download test data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
