{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCML Project-1 ~ Team #60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from costs import compute_loss\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "\n",
    "DATA_TRAIN_PATH = \"../Data/train.csv\" # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "#print the shape of the offset x matrix.\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#standardization\n",
    "tX, mean_x, std_x = standardize(tX, mean_x=None, std_x=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.5\n",
      "Gradient Descent(1/999): loss=0.4930403211757793\n",
      "Gradient Descent(2/999): loss=0.4870098501022072\n",
      "Gradient Descent(3/999): loss=0.4817168268664753\n",
      "Gradient Descent(4/999): loss=0.4770128047663221\n",
      "Gradient Descent(5/999): loss=0.4727827104481938\n",
      "Gradient Descent(6/999): loss=0.4689371916527971\n",
      "Gradient Descent(7/999): loss=0.4654067258061777\n",
      "Gradient Descent(8/999): loss=0.4621370840030196\n",
      "Gradient Descent(9/999): loss=0.45908583830059513\n",
      "Gradient Descent(10/999): loss=0.45621967211044767\n",
      "Gradient Descent(11/999): loss=0.4535123087925472\n",
      "Gradient Descent(12/999): loss=0.45094291613495796\n",
      "Gradient Descent(13/999): loss=0.4484948771748208\n",
      "Gradient Descent(14/999): loss=0.4461548430418665\n",
      "Gradient Descent(15/999): loss=0.4439120029218537\n",
      "Gradient Descent(16/999): loss=0.4417575211821269\n",
      "Gradient Descent(17/999): loss=0.4396841032046329\n",
      "Gradient Descent(18/999): loss=0.43768566032584444\n",
      "Gradient Descent(19/999): loss=0.43575705109814233\n",
      "Gradient Descent(20/999): loss=0.4338938813329179\n",
      "Gradient Descent(21/999): loss=0.4320923494233507\n",
      "Gradient Descent(22/999): loss=0.4303491265527386\n",
      "Gradient Descent(23/999): loss=0.4286612637864871\n",
      "Gradient Descent(24/999): loss=0.4270261198872494\n",
      "Gradient Descent(25/999): loss=0.42544130511008876\n",
      "Gradient Descent(26/999): loss=0.42390463732556116\n",
      "Gradient Descent(27/999): loss=0.4224141076584347\n",
      "Gradient Descent(28/999): loss=0.4209678534762368\n",
      "Gradient Descent(29/999): loss=0.4195641370594624\n",
      "Gradient Descent(30/999): loss=0.4182013286683655\n",
      "Gradient Descent(31/999): loss=0.41687789301616646\n",
      "Gradient Descent(32/999): loss=0.4155923783855599\n",
      "Gradient Descent(33/999): loss=0.4143434078001904\n",
      "Gradient Descent(34/999): loss=0.41312967179736826\n",
      "Gradient Descent(35/999): loss=0.4119499224519139\n",
      "Gradient Descent(36/999): loss=0.4108029683808321\n",
      "Gradient Descent(37/999): loss=0.4096876705199781\n",
      "Gradient Descent(38/999): loss=0.4086029385112189\n",
      "Gradient Descent(39/999): loss=0.4075477275750668\n",
      "Gradient Descent(40/999): loss=0.40652103577187293\n",
      "Gradient Descent(41/999): loss=0.4055219015763233\n",
      "Gradient Descent(42/999): loss=0.4045494017066949\n",
      "Gradient Descent(43/999): loss=0.4036026491632055\n",
      "Gradient Descent(44/999): loss=0.4026807914397397\n",
      "Gradient Descent(45/999): loss=0.40178300888091195\n",
      "Gradient Descent(46/999): loss=0.4009085131623653\n",
      "Gradient Descent(47/999): loss=0.40005654587679157\n",
      "Gradient Descent(48/999): loss=0.3992263772117204\n",
      "Gradient Descent(49/999): loss=0.3984173047078789\n",
      "Gradient Descent(50/999): loss=0.3976286520890715\n",
      "Gradient Descent(51/999): loss=0.3968597681561937\n",
      "Gradient Descent(52/999): loss=0.3961100257393025\n",
      "Gradient Descent(53/999): loss=0.3953788207026833\n",
      "Gradient Descent(54/999): loss=0.3946655709986549\n",
      "Gradient Descent(55/999): loss=0.3939697157664929\n",
      "Gradient Descent(56/999): loss=0.3932907144733497\n",
      "Gradient Descent(57/999): loss=0.3926280460944537\n",
      "Gradient Descent(58/999): loss=0.39198120833019573\n",
      "Gradient Descent(59/999): loss=0.39134971685796766\n",
      "Gradient Descent(60/999): loss=0.3907331046168399\n",
      "Gradient Descent(61/999): loss=0.3901309211233352\n",
      "Gradient Descent(62/999): loss=0.38954273181671084\n",
      "Gradient Descent(63/999): loss=0.38896811743228027\n",
      "Gradient Descent(64/999): loss=0.38840667340141793\n",
      "Gradient Descent(65/999): loss=0.3878580092769802\n",
      "Gradient Descent(66/999): loss=0.3873217481829583\n",
      "Gradient Descent(67/999): loss=0.3867975262872486\n",
      "Gradient Descent(68/999): loss=0.3862849922964908\n",
      "Gradient Descent(69/999): loss=0.38578380697198617\n",
      "Gradient Descent(70/999): loss=0.3852936426657501\n",
      "Gradient Descent(71/999): loss=0.38481418287581554\n",
      "Gradient Descent(72/999): loss=0.38434512181993413\n",
      "Gradient Descent(73/999): loss=0.3838861640268767\n",
      "Gradient Descent(74/999): loss=0.3834370239445601\n",
      "Gradient Descent(75/999): loss=0.3829974255642747\n",
      "Gradient Descent(76/999): loss=0.38256710206031536\n",
      "Gradient Descent(77/999): loss=0.382145795444348\n",
      "Gradient Descent(78/999): loss=0.3817332562338833\n",
      "Gradient Descent(79/999): loss=0.38132924313424604\n",
      "Gradient Descent(80/999): loss=0.38093352273346764\n",
      "Gradient Descent(81/999): loss=0.38054586920954475\n",
      "Gradient Descent(82/999): loss=0.38016606404953845\n",
      "Gradient Descent(83/999): loss=0.37979389578000927\n",
      "Gradient Descent(84/999): loss=0.379429159708306\n",
      "Gradient Descent(85/999): loss=0.3790716576742464\n",
      "Gradient Descent(86/999): loss=0.3787211978117502\n",
      "Gradient Descent(87/999): loss=0.37837759432000334\n",
      "Gradient Descent(88/999): loss=0.37804066724374963\n",
      "Gradient Descent(89/999): loss=0.37771024226232447\n",
      "Gradient Descent(90/999): loss=0.3773861504870649\n",
      "Gradient Descent(91/999): loss=0.37706822826673775\n",
      "Gradient Descent(92/999): loss=0.37675631700065787\n",
      "Gradient Descent(93/999): loss=0.3764502629591642\n",
      "Gradient Descent(94/999): loss=0.376149917111152\n",
      "Gradient Descent(95/999): loss=0.37585513495836603\n",
      "Gradient Descent(96/999): loss=0.37556577637616756\n",
      "Gradient Descent(97/999): loss=0.3752817054605107\n",
      "Gradient Descent(98/999): loss=0.37500279038086753\n",
      "Gradient Descent(99/999): loss=0.3747289032388546\n",
      "Gradient Descent(100/999): loss=0.3744599199323246\n",
      "Gradient Descent(101/999): loss=0.3741957200246976\n",
      "Gradient Descent(102/999): loss=0.3739361866193146\n",
      "Gradient Descent(103/999): loss=0.37368120623860707\n",
      "Gradient Descent(104/999): loss=0.373430668707883\n",
      "Gradient Descent(105/999): loss=0.37318446704354036\n",
      "Gradient Descent(106/999): loss=0.3729424973455276\n",
      "Gradient Descent(107/999): loss=0.3727046586938736\n",
      "Gradient Descent(108/999): loss=0.3724708530491261\n",
      "Gradient Descent(109/999): loss=0.3722409851565345\n",
      "Gradient Descent(110/999): loss=0.3720149624538261\n",
      "Gradient Descent(111/999): loss=0.3717926949824323\n",
      "Gradient Descent(112/999): loss=0.3715740953020202\n",
      "Gradient Descent(113/999): loss=0.37135907840820054\n",
      "Gradient Descent(114/999): loss=0.3711475616532802\n",
      "Gradient Descent(115/999): loss=0.37093946466993916\n",
      "Gradient Descent(116/999): loss=0.370734709297712\n",
      "Gradient Descent(117/999): loss=0.37053321951216506\n",
      "Gradient Descent(118/999): loss=0.3703349213566558\n",
      "Gradient Descent(119/999): loss=0.37013974287657847\n",
      "Gradient Descent(120/999): loss=0.3699476140559895\n",
      "Gradient Descent(121/999): loss=0.3697584667565246\n",
      "Gradient Descent(122/999): loss=0.3695722346585125\n",
      "Gradient Descent(123/999): loss=0.36938885320419956\n",
      "Gradient Descent(124/999): loss=0.36920825954300374\n",
      "Gradient Descent(125/999): loss=0.3690303924787152\n",
      "Gradient Descent(126/999): loss=0.368855192418569\n",
      "Gradient Descent(127/999): loss=0.368682601324117\n",
      "Gradient Descent(128/999): loss=0.3685125626638262\n",
      "Gradient Descent(129/999): loss=0.36834502136734015\n",
      "Gradient Descent(130/999): loss=0.3681799237813348\n",
      "Gradient Descent(131/999): loss=0.3680172176269095\n",
      "Gradient Descent(132/999): loss=0.3678568519584542\n",
      "Gradient Descent(133/999): loss=0.3676987771239343\n",
      "Gradient Descent(134/999): loss=0.36754294472653914\n",
      "Gradient Descent(135/999): loss=0.3673893075876433\n",
      "Gradient Descent(136/999): loss=0.36723781971102837\n",
      "Gradient Descent(137/999): loss=0.3670884362483197\n",
      "Gradient Descent(138/999): loss=0.3669411134655903\n",
      "Gradient Descent(139/999): loss=0.36679580871108647\n",
      "Gradient Descent(140/999): loss=0.36665248038403814\n",
      "Gradient Descent(141/999): loss=0.36651108790450554\n",
      "Gradient Descent(142/999): loss=0.36637159168423006\n",
      "Gradient Descent(143/999): loss=0.36623395309844803\n",
      "Gradient Descent(144/999): loss=0.3660981344586321\n",
      "Gradient Descent(145/999): loss=0.3659640989861277\n",
      "Gradient Descent(146/999): loss=0.36583181078664845\n",
      "Gradient Descent(147/999): loss=0.36570123482560074\n",
      "Gradient Descent(148/999): loss=0.3655723369042077\n",
      "Gradient Descent(149/999): loss=0.36544508363640094\n",
      "Gradient Descent(150/999): loss=0.3653194424264537\n",
      "Gradient Descent(151/999): loss=0.36519538144732827\n",
      "Gradient Descent(152/999): loss=0.3650728696197119\n",
      "Gradient Descent(153/999): loss=0.36495187659171485\n",
      "Gradient Descent(154/999): loss=0.3648323727192085\n",
      "Gradient Descent(155/999): loss=0.36471432904678064\n",
      "Gradient Descent(156/999): loss=0.3645977172892844\n",
      "Gradient Descent(157/999): loss=0.36448250981395985\n",
      "Gradient Descent(158/999): loss=0.3643686796231107\n",
      "Gradient Descent(159/999): loss=0.3642562003373118\n",
      "Gradient Descent(160/999): loss=0.3641450461791334\n",
      "Gradient Descent(161/999): loss=0.3640351919573606\n",
      "Gradient Descent(162/999): loss=0.3639266130516928\n",
      "Gradient Descent(163/999): loss=0.36381928539790576\n",
      "Gradient Descent(164/999): loss=0.3637131854734592\n",
      "Gradient Descent(165/999): loss=0.36360829028353725\n",
      "Gradient Descent(166/999): loss=0.363504577347503\n",
      "Gradient Descent(167/999): loss=0.36340202468575744\n",
      "Gradient Descent(168/999): loss=0.3633006108069846\n",
      "Gradient Descent(169/999): loss=0.36320031469577313\n",
      "Gradient Descent(170/999): loss=0.36310111580059984\n",
      "Gradient Descent(171/999): loss=0.36300299402216324\n",
      "Gradient Descent(172/999): loss=0.3629059297020566\n",
      "Gradient Descent(173/999): loss=0.36280990361176635\n",
      "Gradient Descent(174/999): loss=0.36271489694198844\n",
      "Gradient Descent(175/999): loss=0.36262089129224945\n",
      "Gradient Descent(176/999): loss=0.3625278686608239\n",
      "Gradient Descent(177/999): loss=0.3624358114349372\n",
      "Gradient Descent(178/999): loss=0.36234470238124566\n",
      "Gradient Descent(179/999): loss=0.3622545246365834\n",
      "Gradient Descent(180/999): loss=0.36216526169896857\n",
      "Gradient Descent(181/999): loss=0.36207689741886084\n",
      "Gradient Descent(182/999): loss=0.3619894159906598\n",
      "Gradient Descent(183/999): loss=0.3619028019444396\n",
      "Gradient Descent(184/999): loss=0.36181704013790955\n",
      "Gradient Descent(185/999): loss=0.3617321157485961\n",
      "Gradient Descent(186/999): loss=0.36164801426623705\n",
      "Gradient Descent(187/999): loss=0.361564721485382\n",
      "Gradient Descent(188/999): loss=0.36148222349819314\n",
      "Gradient Descent(189/999): loss=0.36140050668744006\n",
      "Gradient Descent(190/999): loss=0.36131955771968066\n",
      "Gradient Descent(191/999): loss=0.36123936353862673\n",
      "Gradient Descent(192/999): loss=0.3611599113586822\n",
      "Gradient Descent(193/999): loss=0.3610811886586557\n",
      "Gradient Descent(194/999): loss=0.36100318317563623\n",
      "Gradient Descent(195/999): loss=0.360925882899031\n",
      "Gradient Descent(196/999): loss=0.36084927606475836\n",
      "Gradient Descent(197/999): loss=0.36077335114959186\n",
      "Gradient Descent(198/999): loss=0.3606980968656512\n",
      "Gradient Descent(199/999): loss=0.3606235021550349\n",
      "Gradient Descent(200/999): loss=0.36054955618459134\n",
      "Gradient Descent(201/999): loss=0.3604762483408235\n",
      "Gradient Descent(202/999): loss=0.36040356822492364\n",
      "Gradient Descent(203/999): loss=0.3603315056479341\n",
      "Gradient Descent(204/999): loss=0.360260050626031\n",
      "Gradient Descent(205/999): loss=0.36018919337592575\n",
      "Gradient Descent(206/999): loss=0.36011892431038384\n",
      "Gradient Descent(207/999): loss=0.360049234033853\n",
      "Gradient Descent(208/999): loss=0.3599801133382045\n",
      "Gradient Descent(209/999): loss=0.35991155319857493\n",
      "Gradient Descent(210/999): loss=0.3598435447693159\n",
      "Gradient Descent(211/999): loss=0.3597760793800403\n",
      "Gradient Descent(212/999): loss=0.3597091485317665\n",
      "Gradient Descent(213/999): loss=0.3596427438931587\n",
      "Gradient Descent(214/999): loss=0.3595768572968554\n",
      "Gradient Descent(215/999): loss=0.359511480735891\n",
      "Gradient Descent(216/999): loss=0.3594466063602014\n",
      "Gradient Descent(217/999): loss=0.3593822264732148\n",
      "Gradient Descent(218/999): loss=0.35931833352852505\n",
      "Gradient Descent(219/999): loss=0.35925492012664373\n",
      "Gradient Descent(220/999): loss=0.3591919790118313\n",
      "Gradient Descent(221/999): loss=0.35912950306900165\n",
      "Gradient Descent(222/999): loss=0.359067485320703\n",
      "Gradient Descent(223/999): loss=0.35900591892416667\n",
      "Gradient Descent(224/999): loss=0.35894479716842903\n",
      "Gradient Descent(225/999): loss=0.3588841134715191\n",
      "Gradient Descent(226/999): loss=0.3588238613777123\n",
      "Gradient Descent(227/999): loss=0.3587640345548489\n",
      "Gradient Descent(228/999): loss=0.3587046267917148\n",
      "Gradient Descent(229/999): loss=0.35864563199548294\n",
      "Gradient Descent(230/999): loss=0.3585870441892144\n",
      "Gradient Descent(231/999): loss=0.358528857509417\n",
      "Gradient Descent(232/999): loss=0.3584710662036595\n",
      "Gradient Descent(233/999): loss=0.35841366462824226\n",
      "Gradient Descent(234/999): loss=0.3583566472459196\n",
      "Gradient Descent(235/999): loss=0.3583000086236748\n",
      "Gradient Descent(236/999): loss=0.35824374343054627\n",
      "Gradient Descent(237/999): loss=0.3581878464355023\n",
      "Gradient Descent(238/999): loss=0.3581323125053643\n",
      "Gradient Descent(239/999): loss=0.35807713660277707\n",
      "Gradient Descent(240/999): loss=0.35802231378422483\n",
      "Gradient Descent(241/999): loss=0.35796783919809194\n",
      "Gradient Descent(242/999): loss=0.35791370808276646\n",
      "Gradient Descent(243/999): loss=0.3578599157647867\n",
      "Gradient Descent(244/999): loss=0.3578064576570296\n",
      "Gradient Descent(245/999): loss=0.3577533292569382\n",
      "Gradient Descent(246/999): loss=0.35770052614478975\n",
      "Gradient Descent(247/999): loss=0.35764804398200095\n",
      "Gradient Descent(248/999): loss=0.35759587850947183\n",
      "Gradient Descent(249/999): loss=0.3575440255459655\n",
      "Gradient Descent(250/999): loss=0.35749248098652353\n",
      "Gradient Descent(251/999): loss=0.35744124080091616\n",
      "Gradient Descent(252/999): loss=0.3573903010321269\n",
      "Gradient Descent(253/999): loss=0.35733965779486965\n",
      "Gradient Descent(254/999): loss=0.3572893072741385\n",
      "Gradient Descent(255/999): loss=0.35723924572378896\n",
      "Gradient Descent(256/999): loss=0.35718946946514973\n",
      "Gradient Descent(257/999): loss=0.35713997488566607\n",
      "Gradient Descent(258/999): loss=0.3570907584375692\n",
      "Gradient Descent(259/999): loss=0.3570418166365786\n",
      "Gradient Descent(260/999): loss=0.3569931460606292\n",
      "Gradient Descent(261/999): loss=0.35694474334862736\n",
      "Gradient Descent(262/999): loss=0.35689660519923194\n",
      "Gradient Descent(263/999): loss=0.3568487283696644\n",
      "Gradient Descent(264/999): loss=0.35680110967454143\n",
      "Gradient Descent(265/999): loss=0.35675374598473397\n",
      "Gradient Descent(266/999): loss=0.35670663422625015\n",
      "Gradient Descent(267/999): loss=0.3566597713791422\n",
      "Gradient Descent(268/999): loss=0.35661315447643593\n",
      "Gradient Descent(269/999): loss=0.3565667806030839\n",
      "Gradient Descent(270/999): loss=0.3565206468949402\n",
      "Gradient Descent(271/999): loss=0.35647475053775607\n",
      "Gradient Descent(272/999): loss=0.35642908876619833\n",
      "Gradient Descent(273/999): loss=0.3563836588628875\n",
      "Gradient Descent(274/999): loss=0.3563384581574552\n",
      "Gradient Descent(275/999): loss=0.35629348402562394\n",
      "Gradient Descent(276/999): loss=0.3562487338883035\n",
      "Gradient Descent(277/999): loss=0.3562042052107074\n",
      "Gradient Descent(278/999): loss=0.35615989550148786\n",
      "Gradient Descent(279/999): loss=0.35611580231188905\n",
      "Gradient Descent(280/999): loss=0.35607192323491677\n",
      "Gradient Descent(281/999): loss=0.35602825590452614\n",
      "Gradient Descent(282/999): loss=0.35598479799482746\n",
      "Gradient Descent(283/999): loss=0.3559415472193049\n",
      "Gradient Descent(284/999): loss=0.35589850133005607\n",
      "Gradient Descent(285/999): loss=0.35585565811704317\n",
      "Gradient Descent(286/999): loss=0.35581301540736215\n",
      "Gradient Descent(287/999): loss=0.35577057106452603\n",
      "Gradient Descent(288/999): loss=0.3557283229877625\n",
      "Gradient Descent(289/999): loss=0.35568626911132706\n",
      "Gradient Descent(290/999): loss=0.3556444074038291\n",
      "Gradient Descent(291/999): loss=0.3556027358675728\n",
      "Gradient Descent(292/999): loss=0.35556125253791093\n",
      "Gradient Descent(293/999): loss=0.3555199554826118\n",
      "Gradient Descent(294/999): loss=0.3554788428012393\n",
      "Gradient Descent(295/999): loss=0.35543791262454616\n",
      "Gradient Descent(296/999): loss=0.35539716311387776\n",
      "Gradient Descent(297/999): loss=0.35535659246059076\n",
      "Gradient Descent(298/999): loss=0.3553161988854811\n",
      "Gradient Descent(299/999): loss=0.35527598063822463\n",
      "Gradient Descent(300/999): loss=0.3552359359968292\n",
      "Gradient Descent(301/999): loss=0.35519606326709785\n",
      "Gradient Descent(302/999): loss=0.35515636078210233\n",
      "Gradient Descent(303/999): loss=0.3551168269016672\n",
      "Gradient Descent(304/999): loss=0.35507746001186546\n",
      "Gradient Descent(305/999): loss=0.35503825852452336\n",
      "Gradient Descent(306/999): loss=0.35499922087673463\n",
      "Gradient Descent(307/999): loss=0.35496034553038647\n",
      "Gradient Descent(308/999): loss=0.3549216309716924\n",
      "Gradient Descent(309/999): loss=0.35488307571073713\n",
      "Gradient Descent(310/999): loss=0.3548446782810285\n",
      "Gradient Descent(311/999): loss=0.3548064372390596\n",
      "Gradient Descent(312/999): loss=0.3547683511638784\n",
      "Gradient Descent(313/999): loss=0.3547304186566685\n",
      "Gradient Descent(314/999): loss=0.35469263834033454\n",
      "Gradient Descent(315/999): loss=0.3546550088590989\n",
      "Gradient Descent(316/999): loss=0.3546175288781055\n",
      "Gradient Descent(317/999): loss=0.35458019708303123\n",
      "Gradient Descent(318/999): loss=0.3545430121797042\n",
      "Gradient Descent(319/999): loss=0.3545059728937328\n",
      "Gradient Descent(320/999): loss=0.3544690779701374\n",
      "Gradient Descent(321/999): loss=0.3544323261729936\n",
      "Gradient Descent(322/999): loss=0.3543957162850801\n",
      "Gradient Descent(323/999): loss=0.3543592471075344\n",
      "Gradient Descent(324/999): loss=0.35432291745951505\n",
      "Gradient Descent(325/999): loss=0.3542867261778701\n",
      "Gradient Descent(326/999): loss=0.3542506721168139\n",
      "Gradient Descent(327/999): loss=0.35421475414760745\n",
      "Gradient Descent(328/999): loss=0.35417897115824787\n",
      "Gradient Descent(329/999): loss=0.35414332205316146\n",
      "Gradient Descent(330/999): loss=0.3541078057529047\n",
      "Gradient Descent(331/999): loss=0.35407242119387083\n",
      "Gradient Descent(332/999): loss=0.3540371673280007\n",
      "Gradient Descent(333/999): loss=0.3540020431225006\n",
      "Gradient Descent(334/999): loss=0.3539670475595658\n",
      "Gradient Descent(335/999): loss=0.35393217963610896\n",
      "Gradient Descent(336/999): loss=0.35389743836349347\n",
      "Gradient Descent(337/999): loss=0.3538628227672725\n",
      "Gradient Descent(338/999): loss=0.3538283318869333\n",
      "Gradient Descent(339/999): loss=0.35379396477564634\n",
      "Gradient Descent(340/999): loss=0.3537597205000189\n",
      "Gradient Descent(341/999): loss=0.35372559813985394\n",
      "Gradient Descent(342/999): loss=0.35369159678791334\n",
      "Gradient Descent(343/999): loss=0.35365771554968634\n",
      "Gradient Descent(344/999): loss=0.35362395354316145\n",
      "Gradient Descent(345/999): loss=0.35359030989860346\n",
      "Gradient Descent(346/999): loss=0.353556783758335\n",
      "Gradient Descent(347/999): loss=0.35352337427652175\n",
      "Gradient Descent(348/999): loss=0.3534900806189613\n",
      "Gradient Descent(349/999): loss=0.3534569019628787\n",
      "Gradient Descent(350/999): loss=0.3534238374967217\n",
      "Gradient Descent(351/999): loss=0.35339088641996413\n",
      "Gradient Descent(352/999): loss=0.35335804794291004\n",
      "Gradient Descent(353/999): loss=0.35332532128650357\n",
      "Gradient Descent(354/999): loss=0.35329270568214116\n",
      "Gradient Descent(355/999): loss=0.3532602003714881\n",
      "Gradient Descent(356/999): loss=0.35322780460629866\n",
      "Gradient Descent(357/999): loss=0.35319551764823864\n",
      "Gradient Descent(358/999): loss=0.3531633387687132\n",
      "Gradient Descent(359/999): loss=0.3531312672486956\n",
      "Gradient Descent(360/999): loss=0.3530993023785613\n",
      "Gradient Descent(361/999): loss=0.3530674434579245\n",
      "Gradient Descent(362/999): loss=0.3530356897954769\n",
      "Gradient Descent(363/999): loss=0.35300404070883123\n",
      "Gradient Descent(364/999): loss=0.352972495524366\n",
      "Gradient Descent(365/999): loss=0.35294105357707517\n",
      "Gradient Descent(366/999): loss=0.3529097142104183\n",
      "Gradient Descent(367/999): loss=0.3528784767761758\n",
      "Gradient Descent(368/999): loss=0.35284734063430556\n",
      "Gradient Descent(369/999): loss=0.3528163051528026\n",
      "Gradient Descent(370/999): loss=0.3527853697075618\n",
      "Gradient Descent(371/999): loss=0.35275453368224263\n",
      "Gradient Descent(372/999): loss=0.3527237964681363\n",
      "Gradient Descent(373/999): loss=0.35269315746403723\n",
      "Gradient Descent(374/999): loss=0.35266261607611377\n",
      "Gradient Descent(375/999): loss=0.3526321717177843\n",
      "Gradient Descent(376/999): loss=0.3526018238095945\n",
      "Gradient Descent(377/999): loss=0.3525715717790957\n",
      "Gradient Descent(378/999): loss=0.3525414150607282\n",
      "Gradient Descent(379/999): loss=0.3525113530957052\n",
      "Gradient Descent(380/999): loss=0.3524813853318976\n",
      "Gradient Descent(381/999): loss=0.3524515112237242\n",
      "Gradient Descent(382/999): loss=0.35242173023204093\n",
      "Gradient Descent(383/999): loss=0.35239204182403383\n",
      "Gradient Descent(384/999): loss=0.3523624454731139\n",
      "Gradient Descent(385/999): loss=0.35233294065881293\n",
      "Gradient Descent(386/999): loss=0.352303526866682\n",
      "Gradient Descent(387/999): loss=0.3522742035881928\n",
      "Gradient Descent(388/999): loss=0.35224497032063845\n",
      "Gradient Descent(389/999): loss=0.3522158265670387\n",
      "Gradient Descent(390/999): loss=0.35218677183604463\n",
      "Gradient Descent(391/999): loss=0.3521578056418474\n",
      "Gradient Descent(392/999): loss=0.3521289275040861\n",
      "Gradient Descent(393/999): loss=0.3521001369477601\n",
      "Gradient Descent(394/999): loss=0.3520714335031412\n",
      "Gradient Descent(395/999): loss=0.35204281670568666\n",
      "Gradient Descent(396/999): loss=0.3520142860959571\n",
      "Gradient Descent(397/999): loss=0.35198584121953214\n",
      "Gradient Descent(398/999): loss=0.35195748162693036\n",
      "Gradient Descent(399/999): loss=0.35192920687352874\n",
      "Gradient Descent(400/999): loss=0.3519010165194856\n",
      "Gradient Descent(401/999): loss=0.35187291012966265\n",
      "Gradient Descent(402/999): loss=0.3518448872735513\n",
      "Gradient Descent(403/999): loss=0.3518169475251968\n",
      "Gradient Descent(404/999): loss=0.3517890904631274\n",
      "Gradient Descent(405/999): loss=0.3517613156702817\n",
      "Gradient Descent(406/999): loss=0.35173362273394027\n",
      "Gradient Descent(407/999): loss=0.3517060112456552\n",
      "Gradient Descent(408/999): loss=0.3516784808011846\n",
      "Gradient Descent(409/999): loss=0.35165103100042494\n",
      "Gradient Descent(410/999): loss=0.3516236614473468\n",
      "Gradient Descent(411/999): loss=0.35159637174993086\n",
      "Gradient Descent(412/999): loss=0.3515691615201055\n",
      "Gradient Descent(413/999): loss=0.35154203037368537\n",
      "Gradient Descent(414/999): loss=0.3515149779303101\n",
      "Gradient Descent(415/999): loss=0.35148800381338724\n",
      "Gradient Descent(416/999): loss=0.3514611076500311\n",
      "Gradient Descent(417/999): loss=0.35143428907100843\n",
      "Gradient Descent(418/999): loss=0.35140754771068017\n",
      "Gradient Descent(419/999): loss=0.35138088320694755\n",
      "Gradient Descent(420/999): loss=0.35135429520119793\n",
      "Gradient Descent(421/999): loss=0.35132778333825126\n",
      "Gradient Descent(422/999): loss=0.351301347266308\n",
      "Gradient Descent(423/999): loss=0.35127498663689793\n",
      "Gradient Descent(424/999): loss=0.35124870110483036\n",
      "Gradient Descent(425/999): loss=0.3512224903281439\n",
      "Gradient Descent(426/999): loss=0.3511963539680584\n",
      "Gradient Descent(427/999): loss=0.3511702916889271\n",
      "Gradient Descent(428/999): loss=0.3511443031581904\n",
      "Gradient Descent(429/999): loss=0.3511183880463287\n",
      "Gradient Descent(430/999): loss=0.351092546026819\n",
      "Gradient Descent(431/999): loss=0.3510667767760885\n",
      "Gradient Descent(432/999): loss=0.35104107997347306\n",
      "Gradient Descent(433/999): loss=0.3510154553011727\n",
      "Gradient Descent(434/999): loss=0.3509899024442112\n",
      "Gradient Descent(435/999): loss=0.35096442109039294\n",
      "Gradient Descent(436/999): loss=0.3509390109302644\n",
      "Gradient Descent(437/999): loss=0.3509136716570729\n",
      "Gradient Descent(438/999): loss=0.35088840296672785\n",
      "Gradient Descent(439/999): loss=0.3508632045577625\n",
      "Gradient Descent(440/999): loss=0.3508380761312959\n",
      "Gradient Descent(441/999): loss=0.35081301739099635\n",
      "Gradient Descent(442/999): loss=0.3507880280430443\n",
      "Gradient Descent(443/999): loss=0.35076310779609676\n",
      "Gradient Descent(444/999): loss=0.35073825636125233\n",
      "Gradient Descent(445/999): loss=0.35071347345201676\n",
      "Gradient Descent(446/999): loss=0.3506887587842679\n",
      "Gradient Descent(447/999): loss=0.3506641120762241\n",
      "Gradient Descent(448/999): loss=0.3506395330484102\n",
      "Gradient Descent(449/999): loss=0.3506150214236251\n",
      "Gradient Descent(450/999): loss=0.3505905769269115\n",
      "Gradient Descent(451/999): loss=0.35056619928552296\n",
      "Gradient Descent(452/999): loss=0.35054188822889515\n",
      "Gradient Descent(453/999): loss=0.3505176434886143\n",
      "Gradient Descent(454/999): loss=0.35049346479838855\n",
      "Gradient Descent(455/999): loss=0.35046935189401873\n",
      "Gradient Descent(456/999): loss=0.3504453045133699\n",
      "Gradient Descent(457/999): loss=0.3504213223963432\n",
      "Gradient Descent(458/999): loss=0.35039740528484825\n",
      "Gradient Descent(459/999): loss=0.35037355292277633\n",
      "Gradient Descent(460/999): loss=0.35034976505597315\n",
      "Gradient Descent(461/999): loss=0.35032604143221413\n",
      "Gradient Descent(462/999): loss=0.3503023818011773\n",
      "Gradient Descent(463/999): loss=0.3502787859144179\n",
      "Gradient Descent(464/999): loss=0.35025525352534487\n",
      "Gradient Descent(465/999): loss=0.3502317843891957\n",
      "Gradient Descent(466/999): loss=0.3502083782630117\n",
      "Gradient Descent(467/999): loss=0.3501850349056159\n",
      "Gradient Descent(468/999): loss=0.35016175407758837\n",
      "Gradient Descent(469/999): loss=0.3501385355412451\n",
      "Gradient Descent(470/999): loss=0.3501153790606133\n",
      "Gradient Descent(471/999): loss=0.35009228440141155\n",
      "Gradient Descent(472/999): loss=0.3500692513310271\n",
      "Gradient Descent(473/999): loss=0.35004627961849444\n",
      "Gradient Descent(474/999): loss=0.3500233690344746\n",
      "Gradient Descent(475/999): loss=0.3500005193512345\n",
      "Gradient Descent(476/999): loss=0.3499777303426269\n",
      "Gradient Descent(477/999): loss=0.34995500178407013\n",
      "Gradient Descent(478/999): loss=0.3499323334525286\n",
      "Gradient Descent(479/999): loss=0.3499097251264935\n",
      "Gradient Descent(480/999): loss=0.34988717658596435\n",
      "Gradient Descent(481/999): loss=0.349864687612429\n",
      "Gradient Descent(482/999): loss=0.34984225798884744\n",
      "Gradient Descent(483/999): loss=0.34981988749963155\n",
      "Gradient Descent(484/999): loss=0.34979757593062877\n",
      "Gradient Descent(485/999): loss=0.3497753230691042\n",
      "Gradient Descent(486/999): loss=0.34975312870372344\n",
      "Gradient Descent(487/999): loss=0.34973099262453583\n",
      "Gradient Descent(488/999): loss=0.34970891462295767\n",
      "Gradient Descent(489/999): loss=0.349686894491756\n",
      "Gradient Descent(490/999): loss=0.34966493202503274\n",
      "Gradient Descent(491/999): loss=0.34964302701820843\n",
      "Gradient Descent(492/999): loss=0.3496211792680067\n",
      "Gradient Descent(493/999): loss=0.34959938857243983\n",
      "Gradient Descent(494/999): loss=0.34957765473079233\n",
      "Gradient Descent(495/999): loss=0.3495559775436071\n",
      "Gradient Descent(496/999): loss=0.34953435681267087\n",
      "Gradient Descent(497/999): loss=0.34951279234099925\n",
      "Gradient Descent(498/999): loss=0.34949128393282286\n",
      "Gradient Descent(499/999): loss=0.34946983139357374\n",
      "Gradient Descent(500/999): loss=0.3494484345298713\n",
      "Gradient Descent(501/999): loss=0.34942709314950904\n",
      "Gradient Descent(502/999): loss=0.34940580706144153\n",
      "Gradient Descent(503/999): loss=0.349384576075771\n",
      "Gradient Descent(504/999): loss=0.3493634000037345\n",
      "Gradient Descent(505/999): loss=0.34934227865769135\n",
      "Gradient Descent(506/999): loss=0.3493212118511114\n",
      "Gradient Descent(507/999): loss=0.34930019939856155\n",
      "Gradient Descent(508/999): loss=0.34927924111569464\n",
      "Gradient Descent(509/999): loss=0.3492583368192376\n",
      "Gradient Descent(510/999): loss=0.3492374863269786\n",
      "Gradient Descent(511/999): loss=0.3492166894577575\n",
      "Gradient Descent(512/999): loss=0.3491959460314529\n",
      "Gradient Descent(513/999): loss=0.3491752558689716\n",
      "Gradient Descent(514/999): loss=0.34915461879223814\n",
      "Gradient Descent(515/999): loss=0.349134034624183\n",
      "Gradient Descent(516/999): loss=0.3491135031887326\n",
      "Gradient Descent(517/999): loss=0.34909302431079886\n",
      "Gradient Descent(518/999): loss=0.3490725978162685\n",
      "Gradient Descent(519/999): loss=0.34905222353199333\n",
      "Gradient Descent(520/999): loss=0.34903190128578016\n",
      "Gradient Descent(521/999): loss=0.34901163090638054\n",
      "Gradient Descent(522/999): loss=0.3489914122234815\n",
      "Gradient Descent(523/999): loss=0.3489712450676962\n",
      "Gradient Descent(524/999): loss=0.34895112927055344\n",
      "Gradient Descent(525/999): loss=0.3489310646644899\n",
      "Gradient Descent(526/999): loss=0.34891105108283993\n",
      "Gradient Descent(527/999): loss=0.348891088359827\n",
      "Gradient Descent(528/999): loss=0.34887117633055453\n",
      "Gradient Descent(529/999): loss=0.3488513148309977\n",
      "Gradient Descent(530/999): loss=0.34883150369799415\n",
      "Gradient Descent(531/999): loss=0.3488117427692362\n",
      "Gradient Descent(532/999): loss=0.3487920318832618\n",
      "Gradient Descent(533/999): loss=0.34877237087944724\n",
      "Gradient Descent(534/999): loss=0.34875275959799795\n",
      "Gradient Descent(535/999): loss=0.34873319787994145\n",
      "Gradient Descent(536/999): loss=0.3487136855671187\n",
      "Gradient Descent(537/999): loss=0.34869422250217713\n",
      "Gradient Descent(538/999): loss=0.3486748085285624\n",
      "Gradient Descent(539/999): loss=0.34865544349051114\n",
      "Gradient Descent(540/999): loss=0.3486361272330433\n",
      "Gradient Descent(541/999): loss=0.34861685960195504\n",
      "Gradient Descent(542/999): loss=0.3485976404438117\n",
      "Gradient Descent(543/999): loss=0.3485784696059403\n",
      "Gradient Descent(544/999): loss=0.3485593469364223\n",
      "Gradient Descent(545/999): loss=0.3485402722840878\n",
      "Gradient Descent(546/999): loss=0.3485212454985074\n",
      "Gradient Descent(547/999): loss=0.3485022664299864\n",
      "Gradient Descent(548/999): loss=0.34848333492955763\n",
      "Gradient Descent(549/999): loss=0.3484644508489754\n",
      "Gradient Descent(550/999): loss=0.34844561404070856\n",
      "Gradient Descent(551/999): loss=0.3484268243579347\n",
      "Gradient Descent(552/999): loss=0.34840808165453346\n",
      "Gradient Descent(553/999): loss=0.34838938578507994\n",
      "Gradient Descent(554/999): loss=0.34837073660484036\n",
      "Gradient Descent(555/999): loss=0.3483521339697636\n",
      "Gradient Descent(556/999): loss=0.34833357773647744\n",
      "Gradient Descent(557/999): loss=0.3483150677622813\n",
      "Gradient Descent(558/999): loss=0.34829660390514117\n",
      "Gradient Descent(559/999): loss=0.34827818602368366\n",
      "Gradient Descent(560/999): loss=0.3482598139771908\n",
      "Gradient Descent(561/999): loss=0.3482414876255934\n",
      "Gradient Descent(562/999): loss=0.34822320682946717\n",
      "Gradient Descent(563/999): loss=0.34820497145002643\n",
      "Gradient Descent(564/999): loss=0.3481867813491184\n",
      "Gradient Descent(565/999): loss=0.3481686363892189\n",
      "Gradient Descent(566/999): loss=0.34815053643342664\n",
      "Gradient Descent(567/999): loss=0.3481324813454584\n",
      "Gradient Descent(568/999): loss=0.34811447098964327\n",
      "Gradient Descent(569/999): loss=0.3480965052309186\n",
      "Gradient Descent(570/999): loss=0.34807858393482505\n",
      "Gradient Descent(571/999): loss=0.34806070696750063\n",
      "Gradient Descent(572/999): loss=0.34804287419567737\n",
      "Gradient Descent(573/999): loss=0.3480250854866755\n",
      "Gradient Descent(574/999): loss=0.3480073407083996\n",
      "Gradient Descent(575/999): loss=0.34798963972933317\n",
      "Gradient Descent(576/999): loss=0.3479719824185353\n",
      "Gradient Descent(577/999): loss=0.34795436864563467\n",
      "Gradient Descent(578/999): loss=0.347936798280826\n",
      "Gradient Descent(579/999): loss=0.34791927119486626\n",
      "Gradient Descent(580/999): loss=0.347901787259069\n",
      "Gradient Descent(581/999): loss=0.3478843463453012\n",
      "Gradient Descent(582/999): loss=0.3478669483259781\n",
      "Gradient Descent(583/999): loss=0.34784959307406027\n",
      "Gradient Descent(584/999): loss=0.3478322804630484\n",
      "Gradient Descent(585/999): loss=0.34781501036697987\n",
      "Gradient Descent(586/999): loss=0.34779778266042444\n",
      "Gradient Descent(587/999): loss=0.34778059721848037\n",
      "Gradient Descent(588/999): loss=0.34776345391677105\n",
      "Gradient Descent(589/999): loss=0.34774635263144016\n",
      "Gradient Descent(590/999): loss=0.3477292932391482\n",
      "Gradient Descent(591/999): loss=0.34771227561706985\n",
      "Gradient Descent(592/999): loss=0.34769529964288837\n",
      "Gradient Descent(593/999): loss=0.34767836519479334\n",
      "Gradient Descent(594/999): loss=0.3476614721514764\n",
      "Gradient Descent(595/999): loss=0.347644620392128\n",
      "Gradient Descent(596/999): loss=0.34762780979643354\n",
      "Gradient Descent(597/999): loss=0.3476110402445699\n",
      "Gradient Descent(598/999): loss=0.3475943116172025\n",
      "Gradient Descent(599/999): loss=0.3475776237954812\n",
      "Gradient Descent(600/999): loss=0.3475609766610371\n",
      "Gradient Descent(601/999): loss=0.34754437009597977\n",
      "Gradient Descent(602/999): loss=0.34752780398289335\n",
      "Gradient Descent(603/999): loss=0.3475112782048331\n",
      "Gradient Descent(604/999): loss=0.34749479264532274\n",
      "Gradient Descent(605/999): loss=0.3474783471883512\n",
      "Gradient Descent(606/999): loss=0.347461941718369\n",
      "Gradient Descent(607/999): loss=0.3474455761202861\n",
      "Gradient Descent(608/999): loss=0.3474292502794672\n",
      "Gradient Descent(609/999): loss=0.3474129640817305\n",
      "Gradient Descent(610/999): loss=0.3473967174133436\n",
      "Gradient Descent(611/999): loss=0.34738051016102045\n",
      "Gradient Descent(612/999): loss=0.3473643422119193\n",
      "Gradient Descent(613/999): loss=0.34734821345363875\n",
      "Gradient Descent(614/999): loss=0.3473321237742162\n",
      "Gradient Descent(615/999): loss=0.34731607306212287\n",
      "Gradient Descent(616/999): loss=0.34730006120626333\n",
      "Gradient Descent(617/999): loss=0.34728408809597144\n",
      "Gradient Descent(618/999): loss=0.3472681536210075\n",
      "Gradient Descent(619/999): loss=0.3472522576715562\n",
      "Gradient Descent(620/999): loss=0.3472364001382236\n",
      "Gradient Descent(621/999): loss=0.34722058091203417\n",
      "Gradient Descent(622/999): loss=0.3472047998844285\n",
      "Gradient Descent(623/999): loss=0.3471890569472611\n",
      "Gradient Descent(624/999): loss=0.34717335199279664\n",
      "Gradient Descent(625/999): loss=0.3471576849137088\n",
      "Gradient Descent(626/999): loss=0.34714205560307604\n",
      "Gradient Descent(627/999): loss=0.34712646395438146\n",
      "Gradient Descent(628/999): loss=0.34711090986150817\n",
      "Gradient Descent(629/999): loss=0.3470953932187375\n",
      "Gradient Descent(630/999): loss=0.34707991392074705\n",
      "Gradient Descent(631/999): loss=0.3470644718626082\n",
      "Gradient Descent(632/999): loss=0.347049066939783\n",
      "Gradient Descent(633/999): loss=0.34703369904812276\n",
      "Gradient Descent(634/999): loss=0.34701836808386494\n",
      "Gradient Descent(635/999): loss=0.34700307394363156\n",
      "Gradient Descent(636/999): loss=0.34698781652442623\n",
      "Gradient Descent(637/999): loss=0.34697259572363265\n",
      "Gradient Descent(638/999): loss=0.34695741143901143\n",
      "Gradient Descent(639/999): loss=0.3469422635686991\n",
      "Gradient Descent(640/999): loss=0.34692715201120466\n",
      "Gradient Descent(641/999): loss=0.34691207666540824\n",
      "Gradient Descent(642/999): loss=0.34689703743055883\n",
      "Gradient Descent(643/999): loss=0.3468820342062716\n",
      "Gradient Descent(644/999): loss=0.3468670668925267\n",
      "Gradient Descent(645/999): loss=0.34685213538966647\n",
      "Gradient Descent(646/999): loss=0.346837239598394\n",
      "Gradient Descent(647/999): loss=0.3468223794197702\n",
      "Gradient Descent(648/999): loss=0.3468075547552123\n",
      "Gradient Descent(649/999): loss=0.3467927655064922\n",
      "Gradient Descent(650/999): loss=0.34677801157573396\n",
      "Gradient Descent(651/999): loss=0.34676329286541197\n",
      "Gradient Descent(652/999): loss=0.3467486092783492\n",
      "Gradient Descent(653/999): loss=0.34673396071771484\n",
      "Gradient Descent(654/999): loss=0.34671934708702273\n",
      "Gradient Descent(655/999): loss=0.3467047682901295\n",
      "Gradient Descent(656/999): loss=0.34669022423123247\n",
      "Gradient Descent(657/999): loss=0.34667571481486786\n",
      "Gradient Descent(658/999): loss=0.3466612399459093\n",
      "Gradient Descent(659/999): loss=0.3466467995295653\n",
      "Gradient Descent(660/999): loss=0.34663239347137814\n",
      "Gradient Descent(661/999): loss=0.34661802167722155\n",
      "Gradient Descent(662/999): loss=0.34660368405329944\n",
      "Gradient Descent(663/999): loss=0.3465893805061437\n",
      "Gradient Descent(664/999): loss=0.34657511094261245\n",
      "Gradient Descent(665/999): loss=0.3465608752698888\n",
      "Gradient Descent(666/999): loss=0.3465466733954789\n",
      "Gradient Descent(667/999): loss=0.34653250522720974\n",
      "Gradient Descent(668/999): loss=0.3465183706732284\n",
      "Gradient Descent(669/999): loss=0.3465042696419995\n",
      "Gradient Descent(670/999): loss=0.3464902020423041\n",
      "Gradient Descent(671/999): loss=0.3464761677832377\n",
      "Gradient Descent(672/999): loss=0.3464621667742094\n",
      "Gradient Descent(673/999): loss=0.34644819892493867\n",
      "Gradient Descent(674/999): loss=0.3464342641454559\n",
      "Gradient Descent(675/999): loss=0.34642036234609885\n",
      "Gradient Descent(676/999): loss=0.3464064934375125\n",
      "Gradient Descent(677/999): loss=0.3463926573306465\n",
      "Gradient Descent(678/999): loss=0.34637885393675427\n",
      "Gradient Descent(679/999): loss=0.3463650831673914\n",
      "Gradient Descent(680/999): loss=0.34635134493441366\n",
      "Gradient Descent(681/999): loss=0.34633763914997606\n",
      "Gradient Descent(682/999): loss=0.3463239657265309\n",
      "Gradient Descent(683/999): loss=0.34631032457682664\n",
      "Gradient Descent(684/999): loss=0.34629671561390657\n",
      "Gradient Descent(685/999): loss=0.3462831387511065\n",
      "Gradient Descent(686/999): loss=0.34626959390205453\n",
      "Gradient Descent(687/999): loss=0.34625608098066823\n",
      "Gradient Descent(688/999): loss=0.34624259990115475\n",
      "Gradient Descent(689/999): loss=0.34622915057800857\n",
      "Gradient Descent(690/999): loss=0.34621573292600943\n",
      "Gradient Descent(691/999): loss=0.3462023468602226\n",
      "Gradient Descent(692/999): loss=0.3461889922959961\n",
      "Gradient Descent(693/999): loss=0.3461756691489597\n",
      "Gradient Descent(694/999): loss=0.3461623773350242\n",
      "Gradient Descent(695/999): loss=0.34614911677037913\n",
      "Gradient Descent(696/999): loss=0.3461358873714921\n",
      "Gradient Descent(697/999): loss=0.3461226890551072\n",
      "Gradient Descent(698/999): loss=0.3461095217382439\n",
      "Gradient Descent(699/999): loss=0.346096385338195\n",
      "Gradient Descent(700/999): loss=0.34608327977252684\n",
      "Gradient Descent(701/999): loss=0.34607020495907614\n",
      "Gradient Descent(702/999): loss=0.34605716081595056\n",
      "Gradient Descent(703/999): loss=0.34604414726152566\n",
      "Gradient Descent(704/999): loss=0.3460311642144455\n",
      "Gradient Descent(705/999): loss=0.3460182115936198\n",
      "Gradient Descent(706/999): loss=0.3460052893182238\n",
      "Gradient Descent(707/999): loss=0.3459923973076959\n",
      "Gradient Descent(708/999): loss=0.34597953548173765\n",
      "Gradient Descent(709/999): loss=0.34596670376031236\n",
      "Gradient Descent(710/999): loss=0.345953902063643\n",
      "Gradient Descent(711/999): loss=0.3459411303122115\n",
      "Gradient Descent(712/999): loss=0.34592838842675816\n",
      "Gradient Descent(713/999): loss=0.3459156763282796\n",
      "Gradient Descent(714/999): loss=0.3459029939380281\n",
      "Gradient Descent(715/999): loss=0.3458903411775103\n",
      "Gradient Descent(716/999): loss=0.3458777179684861\n",
      "Gradient Descent(717/999): loss=0.34586512423296717\n",
      "Gradient Descent(718/999): loss=0.34585255989321684\n",
      "Gradient Descent(719/999): loss=0.3458400248717475\n",
      "Gradient Descent(720/999): loss=0.3458275190913206\n",
      "Gradient Descent(721/999): loss=0.34581504247494543\n",
      "Gradient Descent(722/999): loss=0.3458025949458773\n",
      "Gradient Descent(723/999): loss=0.34579017642761745\n",
      "Gradient Descent(724/999): loss=0.3457777868439109\n",
      "Gradient Descent(725/999): loss=0.34576542611874644\n",
      "Gradient Descent(726/999): loss=0.34575309417635486\n",
      "Gradient Descent(727/999): loss=0.3457407909412081\n",
      "Gradient Descent(728/999): loss=0.34572851633801815\n",
      "Gradient Descent(729/999): loss=0.34571627029173596\n",
      "Gradient Descent(730/999): loss=0.3457040527275507\n",
      "Gradient Descent(731/999): loss=0.3456918635708883\n",
      "Gradient Descent(732/999): loss=0.3456797027474106\n",
      "Gradient Descent(733/999): loss=0.3456675701830147\n",
      "Gradient Descent(734/999): loss=0.34565546580383094\n",
      "Gradient Descent(735/999): loss=0.34564338953622353\n",
      "Gradient Descent(736/999): loss=0.3456313413067877\n",
      "Gradient Descent(737/999): loss=0.34561932104234994\n",
      "Gradient Descent(738/999): loss=0.3456073286699668\n",
      "Gradient Descent(739/999): loss=0.34559536411692343\n",
      "Gradient Descent(740/999): loss=0.3455834273107336\n",
      "Gradient Descent(741/999): loss=0.3455715181791371\n",
      "Gradient Descent(742/999): loss=0.34555963665010075\n",
      "Gradient Descent(743/999): loss=0.34554778265181607\n",
      "Gradient Descent(744/999): loss=0.3455359561126982\n",
      "Gradient Descent(745/999): loss=0.3455241569613867\n",
      "Gradient Descent(746/999): loss=0.34551238512674215\n",
      "Gradient Descent(747/999): loss=0.3455006405378476\n",
      "Gradient Descent(748/999): loss=0.34548892312400553\n",
      "Gradient Descent(749/999): loss=0.3454772328147384\n",
      "Gradient Descent(750/999): loss=0.34546556953978746\n",
      "Gradient Descent(751/999): loss=0.3454539332291114\n",
      "Gradient Descent(752/999): loss=0.34544232381288553\n",
      "Gradient Descent(753/999): loss=0.3454307412215015\n",
      "Gradient Descent(754/999): loss=0.3454191853855659\n",
      "Gradient Descent(755/999): loss=0.34540765623589875\n",
      "Gradient Descent(756/999): loss=0.34539615370353416\n",
      "Gradient Descent(757/999): loss=0.34538467771971826\n",
      "Gradient Descent(758/999): loss=0.34537322821590893\n",
      "Gradient Descent(759/999): loss=0.34536180512377423\n",
      "Gradient Descent(760/999): loss=0.3453504083751924\n",
      "Gradient Descent(761/999): loss=0.3453390379022508\n",
      "Gradient Descent(762/999): loss=0.34532769363724397\n",
      "Gradient Descent(763/999): loss=0.3453163755126747\n",
      "Gradient Descent(764/999): loss=0.34530508346125216\n",
      "Gradient Descent(765/999): loss=0.34529381741589016\n",
      "Gradient Descent(766/999): loss=0.34528257730970846\n",
      "Gradient Descent(767/999): loss=0.3452713630760298\n",
      "Gradient Descent(768/999): loss=0.3452601746483806\n",
      "Gradient Descent(769/999): loss=0.34524901196048957\n",
      "Gradient Descent(770/999): loss=0.3452378749462868\n",
      "Gradient Descent(771/999): loss=0.3452267635399029\n",
      "Gradient Descent(772/999): loss=0.3452156776756685\n",
      "Gradient Descent(773/999): loss=0.34520461728811397\n",
      "Gradient Descent(774/999): loss=0.3451935823119672\n",
      "Gradient Descent(775/999): loss=0.34518257268215347\n",
      "Gradient Descent(776/999): loss=0.34517158833379613\n",
      "Gradient Descent(777/999): loss=0.3451606292022132\n",
      "Gradient Descent(778/999): loss=0.3451496952229184\n",
      "Gradient Descent(779/999): loss=0.34513878633162\n",
      "Gradient Descent(780/999): loss=0.3451279024642201\n",
      "Gradient Descent(781/999): loss=0.3451170435568132\n",
      "Gradient Descent(782/999): loss=0.3451062095456864\n",
      "Gradient Descent(783/999): loss=0.3450954003673186\n",
      "Gradient Descent(784/999): loss=0.3450846159583785\n",
      "Gradient Descent(785/999): loss=0.3450738562557255\n",
      "Gradient Descent(786/999): loss=0.3450631211964082\n",
      "Gradient Descent(787/999): loss=0.3450524107176631\n",
      "Gradient Descent(788/999): loss=0.3450417247569152\n",
      "Gradient Descent(789/999): loss=0.3450310632517759\n",
      "Gradient Descent(790/999): loss=0.3450204261400436\n",
      "Gradient Descent(791/999): loss=0.3450098133597018\n",
      "Gradient Descent(792/999): loss=0.3449992248489192\n",
      "Gradient Descent(793/999): loss=0.3449886605460483\n",
      "Gradient Descent(794/999): loss=0.34497812038962583\n",
      "Gradient Descent(795/999): loss=0.3449676043183706\n",
      "Gradient Descent(796/999): loss=0.3449571122711838\n",
      "Gradient Descent(797/999): loss=0.34494664418714815\n",
      "Gradient Descent(798/999): loss=0.3449362000055268\n",
      "Gradient Descent(799/999): loss=0.3449257796657633\n",
      "Gradient Descent(800/999): loss=0.34491538310748027\n",
      "Gradient Descent(801/999): loss=0.344905010270479\n",
      "Gradient Descent(802/999): loss=0.34489466109473915\n",
      "Gradient Descent(803/999): loss=0.3448843355204174\n",
      "Gradient Descent(804/999): loss=0.34487403348784706\n",
      "Gradient Descent(805/999): loss=0.3448637549375378\n",
      "Gradient Descent(806/999): loss=0.3448534998101741\n",
      "Gradient Descent(807/999): loss=0.3448432680466158\n",
      "Gradient Descent(808/999): loss=0.3448330595878963\n",
      "Gradient Descent(809/999): loss=0.3448228743752225\n",
      "Gradient Descent(810/999): loss=0.34481271234997424\n",
      "Gradient Descent(811/999): loss=0.3448025734537031\n",
      "Gradient Descent(812/999): loss=0.3447924576281325\n",
      "Gradient Descent(813/999): loss=0.3447823648151566\n",
      "Gradient Descent(814/999): loss=0.3447722949568397\n",
      "Gradient Descent(815/999): loss=0.34476224799541566\n",
      "Gradient Descent(816/999): loss=0.34475222387328724\n",
      "Gradient Descent(817/999): loss=0.3447422225330258\n",
      "Gradient Descent(818/999): loss=0.34473224391737034\n",
      "Gradient Descent(819/999): loss=0.3447222879692268\n",
      "Gradient Descent(820/999): loss=0.3447123546316674\n",
      "Gradient Descent(821/999): loss=0.3447024438479308\n",
      "Gradient Descent(822/999): loss=0.3446925555614205\n",
      "Gradient Descent(823/999): loss=0.34468268971570487\n",
      "Gradient Descent(824/999): loss=0.3446728462545162\n",
      "Gradient Descent(825/999): loss=0.3446630251217505\n",
      "Gradient Descent(826/999): loss=0.3446532262614663\n",
      "Gradient Descent(827/999): loss=0.3446434496178845\n",
      "Gradient Descent(828/999): loss=0.34463369513538805\n",
      "Gradient Descent(829/999): loss=0.34462396275852075\n",
      "Gradient Descent(830/999): loss=0.34461425243198684\n",
      "Gradient Descent(831/999): loss=0.34460456410065055\n",
      "Gradient Descent(832/999): loss=0.34459489770953555\n",
      "Gradient Descent(833/999): loss=0.3445852532038244\n",
      "Gradient Descent(834/999): loss=0.3445756305288577\n",
      "Gradient Descent(835/999): loss=0.34456602963013383\n",
      "Gradient Descent(836/999): loss=0.34455645045330835\n",
      "Gradient Descent(837/999): loss=0.34454689294419316\n",
      "Gradient Descent(838/999): loss=0.3445373570487561\n",
      "Gradient Descent(839/999): loss=0.3445278427131207\n",
      "Gradient Descent(840/999): loss=0.3445183498835651\n",
      "Gradient Descent(841/999): loss=0.34450887850652206\n",
      "Gradient Descent(842/999): loss=0.3444994285285777\n",
      "Gradient Descent(843/999): loss=0.34448999989647133\n",
      "Gradient Descent(844/999): loss=0.34448059255709557\n",
      "Gradient Descent(845/999): loss=0.3444712064574947\n",
      "Gradient Descent(846/999): loss=0.34446184154486464\n",
      "Gradient Descent(847/999): loss=0.3444524977665521\n",
      "Gradient Descent(848/999): loss=0.34444317507005495\n",
      "Gradient Descent(849/999): loss=0.3444338734030207\n",
      "Gradient Descent(850/999): loss=0.34442459271324594\n",
      "Gradient Descent(851/999): loss=0.34441533294867693\n",
      "Gradient Descent(852/999): loss=0.3444060940574083\n",
      "Gradient Descent(853/999): loss=0.34439687598768176\n",
      "Gradient Descent(854/999): loss=0.34438767868788706\n",
      "Gradient Descent(855/999): loss=0.34437850210656085\n",
      "Gradient Descent(856/999): loss=0.3443693461923859\n",
      "Gradient Descent(857/999): loss=0.34436021089419083\n",
      "Gradient Descent(858/999): loss=0.34435109616094983\n",
      "Gradient Descent(859/999): loss=0.3443420019417817\n",
      "Gradient Descent(860/999): loss=0.3443329281859495\n",
      "Gradient Descent(861/999): loss=0.34432387484286037\n",
      "Gradient Descent(862/999): loss=0.3443148418620645\n",
      "Gradient Descent(863/999): loss=0.34430582919325503\n",
      "Gradient Descent(864/999): loss=0.3442968367862675\n",
      "Gradient Descent(865/999): loss=0.3442878645910793\n",
      "Gradient Descent(866/999): loss=0.34427891255780885\n",
      "Gradient Descent(867/999): loss=0.34426998063671593\n",
      "Gradient Descent(868/999): loss=0.3442610687782002\n",
      "Gradient Descent(869/999): loss=0.34425217693280175\n",
      "Gradient Descent(870/999): loss=0.34424330505119954\n",
      "Gradient Descent(871/999): loss=0.3442344530842119\n",
      "Gradient Descent(872/999): loss=0.3442256209827953\n",
      "Gradient Descent(873/999): loss=0.3442168086980443\n",
      "Gradient Descent(874/999): loss=0.3442080161811911\n",
      "Gradient Descent(875/999): loss=0.3441992433836047\n",
      "Gradient Descent(876/999): loss=0.344190490256791\n",
      "Gradient Descent(877/999): loss=0.34418175675239177\n",
      "Gradient Descent(878/999): loss=0.3441730428221842\n",
      "Gradient Descent(879/999): loss=0.34416434841808136\n",
      "Gradient Descent(880/999): loss=0.3441556734921303\n",
      "Gradient Descent(881/999): loss=0.3441470179965127\n",
      "Gradient Descent(882/999): loss=0.3441383818835442\n",
      "Gradient Descent(883/999): loss=0.3441297651056734\n",
      "Gradient Descent(884/999): loss=0.34412116761548195\n",
      "Gradient Descent(885/999): loss=0.344112589365684\n",
      "Gradient Descent(886/999): loss=0.34410403030912595\n",
      "Gradient Descent(887/999): loss=0.3440954903987854\n",
      "Gradient Descent(888/999): loss=0.3440869695877712\n",
      "Gradient Descent(889/999): loss=0.34407846782932267\n",
      "Gradient Descent(890/999): loss=0.3440699850768098\n",
      "Gradient Descent(891/999): loss=0.34406152128373196\n",
      "Gradient Descent(892/999): loss=0.3440530764037182\n",
      "Gradient Descent(893/999): loss=0.34404465039052645\n",
      "Gradient Descent(894/999): loss=0.3440362431980427\n",
      "Gradient Descent(895/999): loss=0.34402785478028186\n",
      "Gradient Descent(896/999): loss=0.3440194850913855\n",
      "Gradient Descent(897/999): loss=0.3440111340856233\n",
      "Gradient Descent(898/999): loss=0.344002801717391\n",
      "Gradient Descent(899/999): loss=0.34399448794121107\n",
      "Gradient Descent(900/999): loss=0.34398619271173214\n",
      "Gradient Descent(901/999): loss=0.34397791598372773\n",
      "Gradient Descent(902/999): loss=0.3439696577120969\n",
      "Gradient Descent(903/999): loss=0.3439614178518636\n",
      "Gradient Descent(904/999): loss=0.34395319635817573\n",
      "Gradient Descent(905/999): loss=0.34394499318630484\n",
      "Gradient Descent(906/999): loss=0.34393680829164647\n",
      "Gradient Descent(907/999): loss=0.343928641629719\n",
      "Gradient Descent(908/999): loss=0.3439204931561635\n",
      "Gradient Descent(909/999): loss=0.3439123628267429\n",
      "Gradient Descent(910/999): loss=0.3439042505973425\n",
      "Gradient Descent(911/999): loss=0.34389615642396865\n",
      "Gradient Descent(912/999): loss=0.34388808026274903\n",
      "Gradient Descent(913/999): loss=0.3438800220699318\n",
      "Gradient Descent(914/999): loss=0.34387198180188505\n",
      "Gradient Descent(915/999): loss=0.3438639594150976\n",
      "Gradient Descent(916/999): loss=0.3438559548661767\n",
      "Gradient Descent(917/999): loss=0.34384796811184914\n",
      "Gradient Descent(918/999): loss=0.3438399991089606\n",
      "Gradient Descent(919/999): loss=0.3438320478144748\n",
      "Gradient Descent(920/999): loss=0.34382411418547304\n",
      "Gradient Descent(921/999): loss=0.3438161981791549\n",
      "Gradient Descent(922/999): loss=0.343808299752836\n",
      "Gradient Descent(923/999): loss=0.3438004188639501\n",
      "Gradient Descent(924/999): loss=0.34379255547004606\n",
      "Gradient Descent(925/999): loss=0.3437847095287892\n",
      "Gradient Descent(926/999): loss=0.343776880997961\n",
      "Gradient Descent(927/999): loss=0.3437690698354571\n",
      "Gradient Descent(928/999): loss=0.3437612759992888\n",
      "Gradient Descent(929/999): loss=0.34375349944758155\n",
      "Gradient Descent(930/999): loss=0.3437457401385753\n",
      "Gradient Descent(931/999): loss=0.3437379980306232\n",
      "Gradient Descent(932/999): loss=0.3437302730821922\n",
      "Gradient Descent(933/999): loss=0.34372256525186196\n",
      "Gradient Descent(934/999): loss=0.34371487449832516\n",
      "Gradient Descent(935/999): loss=0.34370720078038647\n",
      "Gradient Descent(936/999): loss=0.3436995440569626\n",
      "Gradient Descent(937/999): loss=0.34369190428708174\n",
      "Gradient Descent(938/999): loss=0.3436842814298835\n",
      "Gradient Descent(939/999): loss=0.343676675444618\n",
      "Gradient Descent(940/999): loss=0.34366908629064613\n",
      "Gradient Descent(941/999): loss=0.3436615139274388\n",
      "Gradient Descent(942/999): loss=0.34365395831457696\n",
      "Gradient Descent(943/999): loss=0.34364641941175045\n",
      "Gradient Descent(944/999): loss=0.3436388971787587\n",
      "Gradient Descent(945/999): loss=0.3436313915755097\n",
      "Gradient Descent(946/999): loss=0.3436239025620199\n",
      "Gradient Descent(947/999): loss=0.34361643009841364\n",
      "Gradient Descent(948/999): loss=0.343608974144923\n",
      "Gradient Descent(949/999): loss=0.34360153466188753\n",
      "Gradient Descent(950/999): loss=0.3435941116097536\n",
      "Gradient Descent(951/999): loss=0.34358670494907445\n",
      "Gradient Descent(952/999): loss=0.34357931464050984\n",
      "Gradient Descent(953/999): loss=0.3435719406448249\n",
      "Gradient Descent(954/999): loss=0.3435645829228909\n",
      "Gradient Descent(955/999): loss=0.34355724143568445\n",
      "Gradient Descent(956/999): loss=0.3435499161442868\n",
      "Gradient Descent(957/999): loss=0.34354260700988426\n",
      "Gradient Descent(958/999): loss=0.3435353139937674\n",
      "Gradient Descent(959/999): loss=0.34352803705733037\n",
      "Gradient Descent(960/999): loss=0.3435207761620716\n",
      "Gradient Descent(961/999): loss=0.34351353126959233\n",
      "Gradient Descent(962/999): loss=0.3435063023415973\n",
      "Gradient Descent(963/999): loss=0.3434990893398935\n",
      "Gradient Descent(964/999): loss=0.34349189222639065\n",
      "Gradient Descent(965/999): loss=0.3434847109631002\n",
      "Gradient Descent(966/999): loss=0.34347754551213566\n",
      "Gradient Descent(967/999): loss=0.3434703958357115\n",
      "Gradient Descent(968/999): loss=0.3434632618961438\n",
      "Gradient Descent(969/999): loss=0.3434561436558493\n",
      "Gradient Descent(970/999): loss=0.3434490410773447\n",
      "Gradient Descent(971/999): loss=0.3434419541232473\n",
      "Gradient Descent(972/999): loss=0.3434348827562744\n",
      "Gradient Descent(973/999): loss=0.3434278269392422\n",
      "Gradient Descent(974/999): loss=0.3434207866350666\n",
      "Gradient Descent(975/999): loss=0.34341376180676225\n",
      "Gradient Descent(976/999): loss=0.34340675241744256\n",
      "Gradient Descent(977/999): loss=0.3433997584303188\n",
      "Gradient Descent(978/999): loss=0.34339277980870064\n",
      "Gradient Descent(979/999): loss=0.3433858165159951\n",
      "Gradient Descent(980/999): loss=0.34337886851570676\n",
      "Gradient Descent(981/999): loss=0.3433719357714373\n",
      "Gradient Descent(982/999): loss=0.343365018246885\n",
      "Gradient Descent(983/999): loss=0.34335811590584464\n",
      "Gradient Descent(984/999): loss=0.3433512287122073\n",
      "Gradient Descent(985/999): loss=0.34334435662996\n",
      "Gradient Descent(986/999): loss=0.3433374996231846\n",
      "Gradient Descent(987/999): loss=0.34333065765605947\n",
      "Gradient Descent(988/999): loss=0.343323830692857\n",
      "Gradient Descent(989/999): loss=0.3433170186979443\n",
      "Gradient Descent(990/999): loss=0.34331022163578345\n",
      "Gradient Descent(991/999): loss=0.34330343947093006\n",
      "Gradient Descent(992/999): loss=0.3432966721680339\n",
      "Gradient Descent(993/999): loss=0.34328991969183814\n",
      "Gradient Descent(994/999): loss=0.3432831820071792\n",
      "Gradient Descent(995/999): loss=0.3432764590789867\n",
      "Gradient Descent(996/999): loss=0.34326975087228206\n",
      "Gradient Descent(997/999): loss=0.34326305735218005\n",
      "Gradient Descent(998/999): loss=0.34325637848388735\n",
      "Gradient Descent(999/999): loss=0.3432497142327018\n",
      "parameters w:  [-0.31465042  0.04197621 -0.23534325 -0.17314583  0.02012779 -0.01187588\n",
      "  0.26245187 -0.01749717  0.20186582 -0.02009936 -0.00310099 -0.12085995\n",
      "  0.1184256  -0.01337476  0.18706606 -0.00096492 -0.00162282  0.18152135\n",
      " -0.00082432  0.00249176  0.08814838  0.00115061 -0.07005346 -0.10721263\n",
      "  0.02615664  0.02936407  0.02937945 -0.01606007 -0.01384844 -0.01391513\n",
      " -0.08729817]\n"
     ]
    }
   ],
   "source": [
    "from gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "# start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tX, w_initial, max_iters, gamma)\n",
    "# end_time = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/999): loss=0.5\n",
      "Stochastic Gradient Descent(1/999): loss=0.4947578722433728\n",
      "Stochastic Gradient Descent(2/999): loss=0.48860593976191496\n",
      "Stochastic Gradient Descent(3/999): loss=0.4631878921798514\n",
      "Stochastic Gradient Descent(4/999): loss=0.47113587508155164\n",
      "Stochastic Gradient Descent(5/999): loss=0.4427453058975212\n",
      "Stochastic Gradient Descent(6/999): loss=0.4948860997688802\n",
      "Stochastic Gradient Descent(7/999): loss=0.5010534133384662\n",
      "Stochastic Gradient Descent(8/999): loss=0.4720562308933522\n",
      "Stochastic Gradient Descent(9/999): loss=0.4803584627109302\n",
      "Stochastic Gradient Descent(10/999): loss=0.4405678413776737\n",
      "Stochastic Gradient Descent(11/999): loss=0.45656926175731277\n",
      "Stochastic Gradient Descent(12/999): loss=0.4284439718503893\n",
      "Stochastic Gradient Descent(13/999): loss=0.42364173887804396\n",
      "Stochastic Gradient Descent(14/999): loss=0.4554961907709581\n",
      "Stochastic Gradient Descent(15/999): loss=0.4502846366348319\n",
      "Stochastic Gradient Descent(16/999): loss=0.41475794584713604\n",
      "Stochastic Gradient Descent(17/999): loss=0.43908630396245873\n",
      "Stochastic Gradient Descent(18/999): loss=0.45085268989118277\n",
      "Stochastic Gradient Descent(19/999): loss=0.45529194746678614\n",
      "Stochastic Gradient Descent(20/999): loss=0.43266246472577047\n",
      "Stochastic Gradient Descent(21/999): loss=0.4053020929308925\n",
      "Stochastic Gradient Descent(22/999): loss=0.4194449909656611\n",
      "Stochastic Gradient Descent(23/999): loss=0.4617615440119847\n",
      "Stochastic Gradient Descent(24/999): loss=0.4203493969029228\n",
      "Stochastic Gradient Descent(25/999): loss=0.4251343320945562\n",
      "Stochastic Gradient Descent(26/999): loss=0.42887224864721823\n",
      "Stochastic Gradient Descent(27/999): loss=0.398409446353962\n",
      "Stochastic Gradient Descent(28/999): loss=0.45368298430791254\n",
      "Stochastic Gradient Descent(29/999): loss=0.42368444725833526\n",
      "Stochastic Gradient Descent(30/999): loss=0.41025277053119213\n",
      "Stochastic Gradient Descent(31/999): loss=0.43942659861555033\n",
      "Stochastic Gradient Descent(32/999): loss=0.4444296857731823\n",
      "Stochastic Gradient Descent(33/999): loss=0.42172659941730184\n",
      "Stochastic Gradient Descent(34/999): loss=0.37897669350813934\n",
      "Stochastic Gradient Descent(35/999): loss=0.4569832673582075\n",
      "Stochastic Gradient Descent(36/999): loss=0.4290866174151705\n",
      "Stochastic Gradient Descent(37/999): loss=0.4164180905458857\n",
      "Stochastic Gradient Descent(38/999): loss=0.34238316292127735\n",
      "Stochastic Gradient Descent(39/999): loss=0.38860856813954103\n",
      "Stochastic Gradient Descent(40/999): loss=0.40090345906397473\n",
      "Stochastic Gradient Descent(41/999): loss=0.39474785204058466\n",
      "Stochastic Gradient Descent(42/999): loss=0.3914841627227162\n",
      "Stochastic Gradient Descent(43/999): loss=0.4111475933162197\n",
      "Stochastic Gradient Descent(44/999): loss=0.41876956547269417\n",
      "Stochastic Gradient Descent(45/999): loss=0.47354330163336156\n",
      "Stochastic Gradient Descent(46/999): loss=0.3547951498880475\n",
      "Stochastic Gradient Descent(47/999): loss=0.40391763545379966\n",
      "Stochastic Gradient Descent(48/999): loss=0.4342745475333285\n",
      "Stochastic Gradient Descent(49/999): loss=0.40820530045022024\n",
      "Stochastic Gradient Descent(50/999): loss=0.47766429999280424\n",
      "Stochastic Gradient Descent(51/999): loss=0.38542006535776074\n",
      "Stochastic Gradient Descent(52/999): loss=0.38068261715755447\n",
      "Stochastic Gradient Descent(53/999): loss=0.4216865435446416\n",
      "Stochastic Gradient Descent(54/999): loss=0.43044918365539053\n",
      "Stochastic Gradient Descent(55/999): loss=0.38105416908602696\n",
      "Stochastic Gradient Descent(56/999): loss=0.3835560964924667\n",
      "Stochastic Gradient Descent(57/999): loss=0.4901070138400718\n",
      "Stochastic Gradient Descent(58/999): loss=0.32099560448745623\n",
      "Stochastic Gradient Descent(59/999): loss=0.35880214339321426\n",
      "Stochastic Gradient Descent(60/999): loss=0.34252489183935453\n",
      "Stochastic Gradient Descent(61/999): loss=0.4483233003454702\n",
      "Stochastic Gradient Descent(62/999): loss=0.39256551469219353\n",
      "Stochastic Gradient Descent(63/999): loss=0.41421307126579476\n",
      "Stochastic Gradient Descent(64/999): loss=0.39077164550661353\n",
      "Stochastic Gradient Descent(65/999): loss=0.438596952698691\n",
      "Stochastic Gradient Descent(66/999): loss=0.4065099057978244\n",
      "Stochastic Gradient Descent(67/999): loss=0.3422600370101743\n",
      "Stochastic Gradient Descent(68/999): loss=0.42024556807038527\n",
      "Stochastic Gradient Descent(69/999): loss=0.35468310262629715\n",
      "Stochastic Gradient Descent(70/999): loss=0.38214137272916804\n",
      "Stochastic Gradient Descent(71/999): loss=0.32742264655526737\n",
      "Stochastic Gradient Descent(72/999): loss=0.4788368527841618\n",
      "Stochastic Gradient Descent(73/999): loss=0.4155650100227654\n",
      "Stochastic Gradient Descent(74/999): loss=0.3476855192707887\n",
      "Stochastic Gradient Descent(75/999): loss=0.3580415248149266\n",
      "Stochastic Gradient Descent(76/999): loss=0.4542881121188013\n",
      "Stochastic Gradient Descent(77/999): loss=0.3593471384443474\n",
      "Stochastic Gradient Descent(78/999): loss=0.3252442665921474\n",
      "Stochastic Gradient Descent(79/999): loss=0.40684900814402325\n",
      "Stochastic Gradient Descent(80/999): loss=0.42256786664280827\n",
      "Stochastic Gradient Descent(81/999): loss=0.33080361110814915\n",
      "Stochastic Gradient Descent(82/999): loss=0.4045597835438647\n",
      "Stochastic Gradient Descent(83/999): loss=0.38909930664913345\n",
      "Stochastic Gradient Descent(84/999): loss=0.3441081871872747\n",
      "Stochastic Gradient Descent(85/999): loss=0.4112800228401307\n",
      "Stochastic Gradient Descent(86/999): loss=0.3530277105003891\n",
      "Stochastic Gradient Descent(87/999): loss=0.34352522946315517\n",
      "Stochastic Gradient Descent(88/999): loss=0.4176003371540986\n",
      "Stochastic Gradient Descent(89/999): loss=0.40938666845140337\n",
      "Stochastic Gradient Descent(90/999): loss=0.3651655768017784\n",
      "Stochastic Gradient Descent(91/999): loss=0.43995045750372996\n",
      "Stochastic Gradient Descent(92/999): loss=0.3138006964600907\n",
      "Stochastic Gradient Descent(93/999): loss=0.33698462688615516\n",
      "Stochastic Gradient Descent(94/999): loss=0.3527603026187249\n",
      "Stochastic Gradient Descent(95/999): loss=0.32244663206673296\n",
      "Stochastic Gradient Descent(96/999): loss=0.3919397494507297\n",
      "Stochastic Gradient Descent(97/999): loss=0.35457624060834797\n",
      "Stochastic Gradient Descent(98/999): loss=0.34963579158924185\n",
      "Stochastic Gradient Descent(99/999): loss=0.372815715089751\n",
      "Stochastic Gradient Descent(100/999): loss=0.3727428175570589\n",
      "Stochastic Gradient Descent(101/999): loss=0.3786462876094989\n",
      "Stochastic Gradient Descent(102/999): loss=0.3771880230343956\n",
      "Stochastic Gradient Descent(103/999): loss=0.42540230084064823\n",
      "Stochastic Gradient Descent(104/999): loss=0.3517034182034046\n",
      "Stochastic Gradient Descent(105/999): loss=0.3766084372418645\n",
      "Stochastic Gradient Descent(106/999): loss=0.37519958454380203\n",
      "Stochastic Gradient Descent(107/999): loss=0.297051691661431\n",
      "Stochastic Gradient Descent(108/999): loss=0.41416738709209683\n",
      "Stochastic Gradient Descent(109/999): loss=0.38941410802742965\n",
      "Stochastic Gradient Descent(110/999): loss=0.3732291023698723\n",
      "Stochastic Gradient Descent(111/999): loss=0.3200934478569995\n",
      "Stochastic Gradient Descent(112/999): loss=0.40472938698789984\n",
      "Stochastic Gradient Descent(113/999): loss=0.3282097159809958\n",
      "Stochastic Gradient Descent(114/999): loss=0.2698575987046099\n",
      "Stochastic Gradient Descent(115/999): loss=0.4143269415395105\n",
      "Stochastic Gradient Descent(116/999): loss=0.36385181605727346\n",
      "Stochastic Gradient Descent(117/999): loss=0.4503896194583578\n",
      "Stochastic Gradient Descent(118/999): loss=0.3161418369246799\n",
      "Stochastic Gradient Descent(119/999): loss=0.44300556278993286\n",
      "Stochastic Gradient Descent(120/999): loss=0.35383199375197555\n",
      "Stochastic Gradient Descent(121/999): loss=0.273100914795315\n",
      "Stochastic Gradient Descent(122/999): loss=0.41838480030640696\n",
      "Stochastic Gradient Descent(123/999): loss=0.365411000902224\n",
      "Stochastic Gradient Descent(124/999): loss=0.3622456865824325\n",
      "Stochastic Gradient Descent(125/999): loss=0.31212351505763825\n",
      "Stochastic Gradient Descent(126/999): loss=0.2780702812900084\n",
      "Stochastic Gradient Descent(127/999): loss=0.3094494503254382\n",
      "Stochastic Gradient Descent(128/999): loss=0.3090371728482968\n",
      "Stochastic Gradient Descent(129/999): loss=0.3313039918816553\n",
      "Stochastic Gradient Descent(130/999): loss=0.2825917992656076\n",
      "Stochastic Gradient Descent(131/999): loss=0.3826995296387984\n",
      "Stochastic Gradient Descent(132/999): loss=0.4086138855074857\n",
      "Stochastic Gradient Descent(133/999): loss=0.4216165665980103\n",
      "Stochastic Gradient Descent(134/999): loss=0.3487509546863072\n",
      "Stochastic Gradient Descent(135/999): loss=0.43156326570433934\n",
      "Stochastic Gradient Descent(136/999): loss=0.35744931805213054\n",
      "Stochastic Gradient Descent(137/999): loss=0.4032339803058615\n",
      "Stochastic Gradient Descent(138/999): loss=0.38005833902089675\n",
      "Stochastic Gradient Descent(139/999): loss=0.4655517265560061\n",
      "Stochastic Gradient Descent(140/999): loss=0.35594692720891025\n",
      "Stochastic Gradient Descent(141/999): loss=0.36589385230032007\n",
      "Stochastic Gradient Descent(142/999): loss=0.32764845634292106\n",
      "Stochastic Gradient Descent(143/999): loss=0.3323436878899806\n",
      "Stochastic Gradient Descent(144/999): loss=0.3820126689547631\n",
      "Stochastic Gradient Descent(145/999): loss=0.36217137634158514\n",
      "Stochastic Gradient Descent(146/999): loss=0.37684205618325306\n",
      "Stochastic Gradient Descent(147/999): loss=0.31713297370367316\n",
      "Stochastic Gradient Descent(148/999): loss=0.44641753030215325\n",
      "Stochastic Gradient Descent(149/999): loss=0.32090530219901403\n",
      "Stochastic Gradient Descent(150/999): loss=0.43367999964972237\n",
      "Stochastic Gradient Descent(151/999): loss=0.46496854622938494\n",
      "Stochastic Gradient Descent(152/999): loss=0.3355851849904113\n",
      "Stochastic Gradient Descent(153/999): loss=0.3024951481158077\n",
      "Stochastic Gradient Descent(154/999): loss=0.3401850088831616\n",
      "Stochastic Gradient Descent(155/999): loss=0.36450501490908166\n",
      "Stochastic Gradient Descent(156/999): loss=0.36461408969034315\n",
      "Stochastic Gradient Descent(157/999): loss=0.388076183275663\n",
      "Stochastic Gradient Descent(158/999): loss=0.4016693781517693\n",
      "Stochastic Gradient Descent(159/999): loss=0.32829755230930097\n",
      "Stochastic Gradient Descent(160/999): loss=0.333428731874035\n",
      "Stochastic Gradient Descent(161/999): loss=0.31965662963077096\n",
      "Stochastic Gradient Descent(162/999): loss=0.42989493997215233\n",
      "Stochastic Gradient Descent(163/999): loss=0.3714413141702744\n",
      "Stochastic Gradient Descent(164/999): loss=0.33630940629801137\n",
      "Stochastic Gradient Descent(165/999): loss=0.4082173322320927\n",
      "Stochastic Gradient Descent(166/999): loss=0.3207986325150614\n",
      "Stochastic Gradient Descent(167/999): loss=0.38225336684870953\n",
      "Stochastic Gradient Descent(168/999): loss=0.4445483148774249\n",
      "Stochastic Gradient Descent(169/999): loss=0.4058029131474694\n",
      "Stochastic Gradient Descent(170/999): loss=0.35137056704607134\n",
      "Stochastic Gradient Descent(171/999): loss=0.4017655162509759\n",
      "Stochastic Gradient Descent(172/999): loss=0.3721642497235165\n",
      "Stochastic Gradient Descent(173/999): loss=0.3141587453728588\n",
      "Stochastic Gradient Descent(174/999): loss=0.3570998631234578\n",
      "Stochastic Gradient Descent(175/999): loss=0.39238927029247117\n",
      "Stochastic Gradient Descent(176/999): loss=0.37326221779320157\n",
      "Stochastic Gradient Descent(177/999): loss=0.3414635412767679\n",
      "Stochastic Gradient Descent(178/999): loss=0.3639778086248738\n",
      "Stochastic Gradient Descent(179/999): loss=0.3727408622920319\n",
      "Stochastic Gradient Descent(180/999): loss=0.34439862432901364\n",
      "Stochastic Gradient Descent(181/999): loss=0.2851919482080422\n",
      "Stochastic Gradient Descent(182/999): loss=0.28804758564837285\n",
      "Stochastic Gradient Descent(183/999): loss=0.40216768370577677\n",
      "Stochastic Gradient Descent(184/999): loss=0.35663291228713084\n",
      "Stochastic Gradient Descent(185/999): loss=0.3481323976601153\n",
      "Stochastic Gradient Descent(186/999): loss=0.2955746437428031\n",
      "Stochastic Gradient Descent(187/999): loss=0.37187726515468844\n",
      "Stochastic Gradient Descent(188/999): loss=0.4015343089329358\n",
      "Stochastic Gradient Descent(189/999): loss=0.4338164011647295\n",
      "Stochastic Gradient Descent(190/999): loss=0.35749509231753046\n",
      "Stochastic Gradient Descent(191/999): loss=0.3022688654063834\n",
      "Stochastic Gradient Descent(192/999): loss=0.3941685967855766\n",
      "Stochastic Gradient Descent(193/999): loss=0.30454683824893825\n",
      "Stochastic Gradient Descent(194/999): loss=0.40715949473768737\n",
      "Stochastic Gradient Descent(195/999): loss=0.42618453457252714\n",
      "Stochastic Gradient Descent(196/999): loss=0.4410663672673018\n",
      "Stochastic Gradient Descent(197/999): loss=0.40689505336707854\n",
      "Stochastic Gradient Descent(198/999): loss=0.3549830392893021\n",
      "Stochastic Gradient Descent(199/999): loss=0.3153653548881828\n",
      "Stochastic Gradient Descent(200/999): loss=0.49106209459843714\n",
      "Stochastic Gradient Descent(201/999): loss=0.36138435405178915\n",
      "Stochastic Gradient Descent(202/999): loss=0.3048300060608835\n",
      "Stochastic Gradient Descent(203/999): loss=0.32180086126894863\n",
      "Stochastic Gradient Descent(204/999): loss=0.3646109761101697\n",
      "Stochastic Gradient Descent(205/999): loss=0.307193672934194\n",
      "Stochastic Gradient Descent(206/999): loss=0.3598862683240532\n",
      "Stochastic Gradient Descent(207/999): loss=0.3802327611830348\n",
      "Stochastic Gradient Descent(208/999): loss=0.4481133199836024\n",
      "Stochastic Gradient Descent(209/999): loss=0.31892872406176714\n",
      "Stochastic Gradient Descent(210/999): loss=0.3150541680026498\n",
      "Stochastic Gradient Descent(211/999): loss=0.44072897532165917\n",
      "Stochastic Gradient Descent(212/999): loss=0.314794978187375\n",
      "Stochastic Gradient Descent(213/999): loss=0.3925130550372301\n",
      "Stochastic Gradient Descent(214/999): loss=0.425942308159934\n",
      "Stochastic Gradient Descent(215/999): loss=0.34418599713248704\n",
      "Stochastic Gradient Descent(216/999): loss=0.3753917302688369\n",
      "Stochastic Gradient Descent(217/999): loss=0.38824961648280215\n",
      "Stochastic Gradient Descent(218/999): loss=0.42008549761611347\n",
      "Stochastic Gradient Descent(219/999): loss=0.35035272764846503\n",
      "Stochastic Gradient Descent(220/999): loss=0.37519282477747284\n",
      "Stochastic Gradient Descent(221/999): loss=0.549113331007155\n",
      "Stochastic Gradient Descent(222/999): loss=0.3173371843889701\n",
      "Stochastic Gradient Descent(223/999): loss=0.3066964698208421\n",
      "Stochastic Gradient Descent(224/999): loss=0.3753841787549091\n",
      "Stochastic Gradient Descent(225/999): loss=0.4153634256341167\n",
      "Stochastic Gradient Descent(226/999): loss=0.2705710124248844\n",
      "Stochastic Gradient Descent(227/999): loss=0.41434360816043303\n",
      "Stochastic Gradient Descent(228/999): loss=0.42171792686599907\n",
      "Stochastic Gradient Descent(229/999): loss=0.340000898403146\n",
      "Stochastic Gradient Descent(230/999): loss=0.3112526582815849\n",
      "Stochastic Gradient Descent(231/999): loss=0.3310380033212163\n",
      "Stochastic Gradient Descent(232/999): loss=0.3226299406042459\n",
      "Stochastic Gradient Descent(233/999): loss=0.44410495684164764\n",
      "Stochastic Gradient Descent(234/999): loss=0.3252150356780061\n",
      "Stochastic Gradient Descent(235/999): loss=0.3797791240262902\n",
      "Stochastic Gradient Descent(236/999): loss=0.37734567996240825\n",
      "Stochastic Gradient Descent(237/999): loss=0.2912257071136443\n",
      "Stochastic Gradient Descent(238/999): loss=0.4229094797792034\n",
      "Stochastic Gradient Descent(239/999): loss=0.4043141989322475\n",
      "Stochastic Gradient Descent(240/999): loss=0.3185525201527792\n",
      "Stochastic Gradient Descent(241/999): loss=0.38227979998912676\n",
      "Stochastic Gradient Descent(242/999): loss=0.31651552610305994\n",
      "Stochastic Gradient Descent(243/999): loss=0.32341195895197977\n",
      "Stochastic Gradient Descent(244/999): loss=0.3417389458572317\n",
      "Stochastic Gradient Descent(245/999): loss=0.33193460284335524\n",
      "Stochastic Gradient Descent(246/999): loss=0.381823894459869\n",
      "Stochastic Gradient Descent(247/999): loss=0.3843036260843878\n",
      "Stochastic Gradient Descent(248/999): loss=0.38277076698007095\n",
      "Stochastic Gradient Descent(249/999): loss=0.2880178668748156\n",
      "Stochastic Gradient Descent(250/999): loss=0.43241583290195607\n",
      "Stochastic Gradient Descent(251/999): loss=0.3176030412264506\n",
      "Stochastic Gradient Descent(252/999): loss=0.47893147459943625\n",
      "Stochastic Gradient Descent(253/999): loss=0.338416338360127\n",
      "Stochastic Gradient Descent(254/999): loss=0.33949170879143226\n",
      "Stochastic Gradient Descent(255/999): loss=0.3967854680450694\n",
      "Stochastic Gradient Descent(256/999): loss=0.31117420463343715\n",
      "Stochastic Gradient Descent(257/999): loss=0.345860927012708\n",
      "Stochastic Gradient Descent(258/999): loss=0.40362173027965587\n",
      "Stochastic Gradient Descent(259/999): loss=0.307144925081255\n",
      "Stochastic Gradient Descent(260/999): loss=0.39476789576174176\n",
      "Stochastic Gradient Descent(261/999): loss=0.35576824058193446\n",
      "Stochastic Gradient Descent(262/999): loss=0.4321327942823623\n",
      "Stochastic Gradient Descent(263/999): loss=0.3312384856613668\n",
      "Stochastic Gradient Descent(264/999): loss=0.32299849339682046\n",
      "Stochastic Gradient Descent(265/999): loss=0.37032406699550807\n",
      "Stochastic Gradient Descent(266/999): loss=0.40635425187579693\n",
      "Stochastic Gradient Descent(267/999): loss=0.34531194071837235\n",
      "Stochastic Gradient Descent(268/999): loss=0.3097382552490843\n",
      "Stochastic Gradient Descent(269/999): loss=0.43506543572528644\n",
      "Stochastic Gradient Descent(270/999): loss=0.3525996506078205\n",
      "Stochastic Gradient Descent(271/999): loss=0.40936658379186075\n",
      "Stochastic Gradient Descent(272/999): loss=0.35719040014066117\n",
      "Stochastic Gradient Descent(273/999): loss=0.4229939937060348\n",
      "Stochastic Gradient Descent(274/999): loss=0.37577705299970504\n",
      "Stochastic Gradient Descent(275/999): loss=0.3836659378164791\n",
      "Stochastic Gradient Descent(276/999): loss=0.32270083594131505\n",
      "Stochastic Gradient Descent(277/999): loss=0.4168133244018306\n",
      "Stochastic Gradient Descent(278/999): loss=0.3394009181402916\n",
      "Stochastic Gradient Descent(279/999): loss=0.29172519174810607\n",
      "Stochastic Gradient Descent(280/999): loss=0.37382758401516564\n",
      "Stochastic Gradient Descent(281/999): loss=0.38443236684237087\n",
      "Stochastic Gradient Descent(282/999): loss=0.3896052717591136\n",
      "Stochastic Gradient Descent(283/999): loss=0.39704230486610387\n",
      "Stochastic Gradient Descent(284/999): loss=0.28939645505738776\n",
      "Stochastic Gradient Descent(285/999): loss=0.3622139771392675\n",
      "Stochastic Gradient Descent(286/999): loss=0.3769497311659866\n",
      "Stochastic Gradient Descent(287/999): loss=0.3042587882453586\n",
      "Stochastic Gradient Descent(288/999): loss=0.2700318997923243\n",
      "Stochastic Gradient Descent(289/999): loss=0.30814616991876803\n",
      "Stochastic Gradient Descent(290/999): loss=0.447018154157572\n",
      "Stochastic Gradient Descent(291/999): loss=0.3582538347988528\n",
      "Stochastic Gradient Descent(292/999): loss=0.2996591777955174\n",
      "Stochastic Gradient Descent(293/999): loss=0.44935717866979724\n",
      "Stochastic Gradient Descent(294/999): loss=0.3386874438832249\n",
      "Stochastic Gradient Descent(295/999): loss=0.4392512747852498\n",
      "Stochastic Gradient Descent(296/999): loss=0.3774115360870867\n",
      "Stochastic Gradient Descent(297/999): loss=0.33438083934193324\n",
      "Stochastic Gradient Descent(298/999): loss=0.3161993491782206\n",
      "Stochastic Gradient Descent(299/999): loss=0.3837208968188684\n",
      "Stochastic Gradient Descent(300/999): loss=0.3462281035191604\n",
      "Stochastic Gradient Descent(301/999): loss=0.33528498562969006\n",
      "Stochastic Gradient Descent(302/999): loss=0.346477637942942\n",
      "Stochastic Gradient Descent(303/999): loss=0.3572819305607915\n",
      "Stochastic Gradient Descent(304/999): loss=0.3766486673570354\n",
      "Stochastic Gradient Descent(305/999): loss=0.32851733877468703\n",
      "Stochastic Gradient Descent(306/999): loss=0.3729629473266195\n",
      "Stochastic Gradient Descent(307/999): loss=0.450872773995794\n",
      "Stochastic Gradient Descent(308/999): loss=0.38881455789632996\n",
      "Stochastic Gradient Descent(309/999): loss=0.361070936333959\n",
      "Stochastic Gradient Descent(310/999): loss=0.45749910751688844\n",
      "Stochastic Gradient Descent(311/999): loss=0.3574456023967276\n",
      "Stochastic Gradient Descent(312/999): loss=0.34142352741086446\n",
      "Stochastic Gradient Descent(313/999): loss=0.30099683979512293\n",
      "Stochastic Gradient Descent(314/999): loss=0.2760019672579172\n",
      "Stochastic Gradient Descent(315/999): loss=0.3582414312937337\n",
      "Stochastic Gradient Descent(316/999): loss=0.36375636559541696\n",
      "Stochastic Gradient Descent(317/999): loss=0.28171014169616554\n",
      "Stochastic Gradient Descent(318/999): loss=0.3840480239011\n",
      "Stochastic Gradient Descent(319/999): loss=0.34836445800165416\n",
      "Stochastic Gradient Descent(320/999): loss=0.4125343263460887\n",
      "Stochastic Gradient Descent(321/999): loss=0.3721509243727879\n",
      "Stochastic Gradient Descent(322/999): loss=0.400915552730981\n",
      "Stochastic Gradient Descent(323/999): loss=0.36126602063626284\n",
      "Stochastic Gradient Descent(324/999): loss=0.29632450117365106\n",
      "Stochastic Gradient Descent(325/999): loss=0.35895809614380525\n",
      "Stochastic Gradient Descent(326/999): loss=0.3299300594422817\n",
      "Stochastic Gradient Descent(327/999): loss=0.31620094986789093\n",
      "Stochastic Gradient Descent(328/999): loss=0.49851552869147736\n",
      "Stochastic Gradient Descent(329/999): loss=0.28961819338201134\n",
      "Stochastic Gradient Descent(330/999): loss=0.41586896380784344\n",
      "Stochastic Gradient Descent(331/999): loss=0.3034687711670509\n",
      "Stochastic Gradient Descent(332/999): loss=0.38999958629895093\n",
      "Stochastic Gradient Descent(333/999): loss=0.3443794219947409\n",
      "Stochastic Gradient Descent(334/999): loss=0.35790807282909604\n",
      "Stochastic Gradient Descent(335/999): loss=0.39727295918491373\n",
      "Stochastic Gradient Descent(336/999): loss=0.4424310395360622\n",
      "Stochastic Gradient Descent(337/999): loss=0.39396313821405515\n",
      "Stochastic Gradient Descent(338/999): loss=0.33246793032709737\n",
      "Stochastic Gradient Descent(339/999): loss=0.3550115178908378\n",
      "Stochastic Gradient Descent(340/999): loss=0.44040076104378056\n",
      "Stochastic Gradient Descent(341/999): loss=0.31671311632269455\n",
      "Stochastic Gradient Descent(342/999): loss=0.37299464704537627\n",
      "Stochastic Gradient Descent(343/999): loss=0.28871425877208057\n",
      "Stochastic Gradient Descent(344/999): loss=0.42097686110684046\n",
      "Stochastic Gradient Descent(345/999): loss=0.3281154734121124\n",
      "Stochastic Gradient Descent(346/999): loss=0.39213357862981285\n",
      "Stochastic Gradient Descent(347/999): loss=0.32131092087305146\n",
      "Stochastic Gradient Descent(348/999): loss=0.292602184778802\n",
      "Stochastic Gradient Descent(349/999): loss=0.2747185962836366\n",
      "Stochastic Gradient Descent(350/999): loss=0.270816966008497\n",
      "Stochastic Gradient Descent(351/999): loss=0.40249456996985494\n",
      "Stochastic Gradient Descent(352/999): loss=0.35911637759422627\n",
      "Stochastic Gradient Descent(353/999): loss=0.35533028725415394\n",
      "Stochastic Gradient Descent(354/999): loss=0.3436723674366216\n",
      "Stochastic Gradient Descent(355/999): loss=0.3314776157066084\n",
      "Stochastic Gradient Descent(356/999): loss=0.40718147813796707\n",
      "Stochastic Gradient Descent(357/999): loss=0.31050413142454075\n",
      "Stochastic Gradient Descent(358/999): loss=0.3284332735863264\n",
      "Stochastic Gradient Descent(359/999): loss=0.2591457474170999\n",
      "Stochastic Gradient Descent(360/999): loss=0.396722883805224\n",
      "Stochastic Gradient Descent(361/999): loss=0.41671271580760205\n",
      "Stochastic Gradient Descent(362/999): loss=0.2974466986197304\n",
      "Stochastic Gradient Descent(363/999): loss=0.3571106570641669\n",
      "Stochastic Gradient Descent(364/999): loss=0.3119763462871876\n",
      "Stochastic Gradient Descent(365/999): loss=0.4132740641819025\n",
      "Stochastic Gradient Descent(366/999): loss=0.3590528293216723\n",
      "Stochastic Gradient Descent(367/999): loss=0.3540191239517003\n",
      "Stochastic Gradient Descent(368/999): loss=0.3106894420162318\n",
      "Stochastic Gradient Descent(369/999): loss=0.3280039204937013\n",
      "Stochastic Gradient Descent(370/999): loss=0.30792211915239187\n",
      "Stochastic Gradient Descent(371/999): loss=0.3652886216717414\n",
      "Stochastic Gradient Descent(372/999): loss=0.36077293121041476\n",
      "Stochastic Gradient Descent(373/999): loss=0.3009801248062286\n",
      "Stochastic Gradient Descent(374/999): loss=0.41613216118248625\n",
      "Stochastic Gradient Descent(375/999): loss=0.285293985065481\n",
      "Stochastic Gradient Descent(376/999): loss=0.26405799441034494\n",
      "Stochastic Gradient Descent(377/999): loss=0.34245544717219434\n",
      "Stochastic Gradient Descent(378/999): loss=0.2833179801906378\n",
      "Stochastic Gradient Descent(379/999): loss=0.31233378618401625\n",
      "Stochastic Gradient Descent(380/999): loss=0.37997053808753867\n",
      "Stochastic Gradient Descent(381/999): loss=0.2907584884381501\n",
      "Stochastic Gradient Descent(382/999): loss=0.3169917702884555\n",
      "Stochastic Gradient Descent(383/999): loss=0.32095989008745257\n",
      "Stochastic Gradient Descent(384/999): loss=0.30308637891737705\n",
      "Stochastic Gradient Descent(385/999): loss=0.40803426867189585\n",
      "Stochastic Gradient Descent(386/999): loss=0.3399803520759651\n",
      "Stochastic Gradient Descent(387/999): loss=0.32829815028927095\n",
      "Stochastic Gradient Descent(388/999): loss=0.37204917105131735\n",
      "Stochastic Gradient Descent(389/999): loss=0.31034700473340526\n",
      "Stochastic Gradient Descent(390/999): loss=0.28665452080694004\n",
      "Stochastic Gradient Descent(391/999): loss=0.20572565894140774\n",
      "Stochastic Gradient Descent(392/999): loss=0.41918064316672643\n",
      "Stochastic Gradient Descent(393/999): loss=0.3021788983029674\n",
      "Stochastic Gradient Descent(394/999): loss=0.3875970266323081\n",
      "Stochastic Gradient Descent(395/999): loss=0.46242832906473663\n",
      "Stochastic Gradient Descent(396/999): loss=0.4057877661461597\n",
      "Stochastic Gradient Descent(397/999): loss=0.28720709129047245\n",
      "Stochastic Gradient Descent(398/999): loss=0.3492454862305274\n",
      "Stochastic Gradient Descent(399/999): loss=0.3735267999407306\n",
      "Stochastic Gradient Descent(400/999): loss=0.4468891175590345\n",
      "Stochastic Gradient Descent(401/999): loss=0.38385842805472065\n",
      "Stochastic Gradient Descent(402/999): loss=0.3588433382062106\n",
      "Stochastic Gradient Descent(403/999): loss=0.35137792707015264\n",
      "Stochastic Gradient Descent(404/999): loss=0.4047472733235342\n",
      "Stochastic Gradient Descent(405/999): loss=0.38764694478200723\n",
      "Stochastic Gradient Descent(406/999): loss=0.3183440980148859\n",
      "Stochastic Gradient Descent(407/999): loss=0.37501211222078257\n",
      "Stochastic Gradient Descent(408/999): loss=0.3691426774735083\n",
      "Stochastic Gradient Descent(409/999): loss=0.34266324128393044\n",
      "Stochastic Gradient Descent(410/999): loss=0.40503127479146456\n",
      "Stochastic Gradient Descent(411/999): loss=0.37330095042769584\n",
      "Stochastic Gradient Descent(412/999): loss=0.37603098247134964\n",
      "Stochastic Gradient Descent(413/999): loss=0.37399653876859307\n",
      "Stochastic Gradient Descent(414/999): loss=0.3562011339861254\n",
      "Stochastic Gradient Descent(415/999): loss=0.34778471550086365\n",
      "Stochastic Gradient Descent(416/999): loss=0.46757223051295027\n",
      "Stochastic Gradient Descent(417/999): loss=0.3473405146980062\n",
      "Stochastic Gradient Descent(418/999): loss=0.3743458030580586\n",
      "Stochastic Gradient Descent(419/999): loss=0.46935943393105023\n",
      "Stochastic Gradient Descent(420/999): loss=0.3373841783352038\n",
      "Stochastic Gradient Descent(421/999): loss=0.2922974741218012\n",
      "Stochastic Gradient Descent(422/999): loss=0.41837229059599257\n",
      "Stochastic Gradient Descent(423/999): loss=0.2947357586086028\n",
      "Stochastic Gradient Descent(424/999): loss=0.25522358842570986\n",
      "Stochastic Gradient Descent(425/999): loss=0.22360870436842256\n",
      "Stochastic Gradient Descent(426/999): loss=0.3775181695234875\n",
      "Stochastic Gradient Descent(427/999): loss=0.23229631152174843\n",
      "Stochastic Gradient Descent(428/999): loss=0.32070189191305376\n",
      "Stochastic Gradient Descent(429/999): loss=0.32629111575932285\n",
      "Stochastic Gradient Descent(430/999): loss=0.33875971624366025\n",
      "Stochastic Gradient Descent(431/999): loss=0.34959107503227066\n",
      "Stochastic Gradient Descent(432/999): loss=0.2996948441881912\n",
      "Stochastic Gradient Descent(433/999): loss=0.39248256345718835\n",
      "Stochastic Gradient Descent(434/999): loss=0.3656384242350405\n",
      "Stochastic Gradient Descent(435/999): loss=0.33654770235296994\n",
      "Stochastic Gradient Descent(436/999): loss=0.28018136759308304\n",
      "Stochastic Gradient Descent(437/999): loss=0.3958118427123027\n",
      "Stochastic Gradient Descent(438/999): loss=0.37661421944254486\n",
      "Stochastic Gradient Descent(439/999): loss=0.35502067134455695\n",
      "Stochastic Gradient Descent(440/999): loss=0.31011221146987267\n",
      "Stochastic Gradient Descent(441/999): loss=0.39287158088403856\n",
      "Stochastic Gradient Descent(442/999): loss=0.3053815792968111\n",
      "Stochastic Gradient Descent(443/999): loss=0.3314069187964783\n",
      "Stochastic Gradient Descent(444/999): loss=0.4109916494090116\n",
      "Stochastic Gradient Descent(445/999): loss=0.3369284125902918\n",
      "Stochastic Gradient Descent(446/999): loss=0.32124292633015095\n",
      "Stochastic Gradient Descent(447/999): loss=0.24697331347227652\n",
      "Stochastic Gradient Descent(448/999): loss=0.3122355204442928\n",
      "Stochastic Gradient Descent(449/999): loss=0.3498242544444302\n",
      "Stochastic Gradient Descent(450/999): loss=0.3728306021563881\n",
      "Stochastic Gradient Descent(451/999): loss=0.3538728740836629\n",
      "Stochastic Gradient Descent(452/999): loss=0.31742615620191766\n",
      "Stochastic Gradient Descent(453/999): loss=0.24500028911917743\n",
      "Stochastic Gradient Descent(454/999): loss=0.28758144464034735\n",
      "Stochastic Gradient Descent(455/999): loss=0.4076651383734468\n",
      "Stochastic Gradient Descent(456/999): loss=0.3570184105002234\n",
      "Stochastic Gradient Descent(457/999): loss=0.3548611283182674\n",
      "Stochastic Gradient Descent(458/999): loss=0.3049739649543086\n",
      "Stochastic Gradient Descent(459/999): loss=0.32734134662015557\n",
      "Stochastic Gradient Descent(460/999): loss=0.42012291071334285\n",
      "Stochastic Gradient Descent(461/999): loss=0.40258179949735534\n",
      "Stochastic Gradient Descent(462/999): loss=0.2837013403098453\n",
      "Stochastic Gradient Descent(463/999): loss=0.23737873020591002\n",
      "Stochastic Gradient Descent(464/999): loss=0.31265658721381584\n",
      "Stochastic Gradient Descent(465/999): loss=0.3353719613929791\n",
      "Stochastic Gradient Descent(466/999): loss=0.31547282032408047\n",
      "Stochastic Gradient Descent(467/999): loss=0.3915454144822123\n",
      "Stochastic Gradient Descent(468/999): loss=0.39853108038202245\n",
      "Stochastic Gradient Descent(469/999): loss=0.41370151339578926\n",
      "Stochastic Gradient Descent(470/999): loss=0.37513062856941504\n",
      "Stochastic Gradient Descent(471/999): loss=0.3146169454278539\n",
      "Stochastic Gradient Descent(472/999): loss=0.3802476881383814\n",
      "Stochastic Gradient Descent(473/999): loss=0.3249838400985168\n",
      "Stochastic Gradient Descent(474/999): loss=0.3097070509158937\n",
      "Stochastic Gradient Descent(475/999): loss=0.30931633789293245\n",
      "Stochastic Gradient Descent(476/999): loss=0.35570128691058506\n",
      "Stochastic Gradient Descent(477/999): loss=0.3268743263156779\n",
      "Stochastic Gradient Descent(478/999): loss=0.371422834246212\n",
      "Stochastic Gradient Descent(479/999): loss=0.37179259059939\n",
      "Stochastic Gradient Descent(480/999): loss=0.34831258444350516\n",
      "Stochastic Gradient Descent(481/999): loss=0.36208419762012634\n",
      "Stochastic Gradient Descent(482/999): loss=0.45658803701078504\n",
      "Stochastic Gradient Descent(483/999): loss=0.3335822666121904\n",
      "Stochastic Gradient Descent(484/999): loss=0.3126687146104442\n",
      "Stochastic Gradient Descent(485/999): loss=0.3151838539862325\n",
      "Stochastic Gradient Descent(486/999): loss=0.3119961728360923\n",
      "Stochastic Gradient Descent(487/999): loss=0.3690518440459672\n",
      "Stochastic Gradient Descent(488/999): loss=0.321397300057411\n",
      "Stochastic Gradient Descent(489/999): loss=0.35539085559677863\n",
      "Stochastic Gradient Descent(490/999): loss=0.3357763573476317\n",
      "Stochastic Gradient Descent(491/999): loss=0.3694644982048058\n",
      "Stochastic Gradient Descent(492/999): loss=0.4685770310395014\n",
      "Stochastic Gradient Descent(493/999): loss=0.3155992221306543\n",
      "Stochastic Gradient Descent(494/999): loss=0.311519607473705\n",
      "Stochastic Gradient Descent(495/999): loss=0.3443961102005715\n",
      "Stochastic Gradient Descent(496/999): loss=0.3155459539119488\n",
      "Stochastic Gradient Descent(497/999): loss=0.3709068823441782\n",
      "Stochastic Gradient Descent(498/999): loss=0.384636209524988\n",
      "Stochastic Gradient Descent(499/999): loss=0.30629874548793895\n",
      "Stochastic Gradient Descent(500/999): loss=0.36365614881657926\n",
      "Stochastic Gradient Descent(501/999): loss=0.39318521039164117\n",
      "Stochastic Gradient Descent(502/999): loss=0.3186458520129371\n",
      "Stochastic Gradient Descent(503/999): loss=0.3324816185609838\n",
      "Stochastic Gradient Descent(504/999): loss=0.30789629628075565\n",
      "Stochastic Gradient Descent(505/999): loss=0.3135431418308764\n",
      "Stochastic Gradient Descent(506/999): loss=0.3636747882203788\n",
      "Stochastic Gradient Descent(507/999): loss=0.38078215073363836\n",
      "Stochastic Gradient Descent(508/999): loss=0.2591306134320179\n",
      "Stochastic Gradient Descent(509/999): loss=0.40668395109717487\n",
      "Stochastic Gradient Descent(510/999): loss=0.2828405537141182\n",
      "Stochastic Gradient Descent(511/999): loss=0.34559017030055444\n",
      "Stochastic Gradient Descent(512/999): loss=0.3279366066939009\n",
      "Stochastic Gradient Descent(513/999): loss=0.3017245045961766\n",
      "Stochastic Gradient Descent(514/999): loss=0.27565760897092284\n",
      "Stochastic Gradient Descent(515/999): loss=0.31917013000283745\n",
      "Stochastic Gradient Descent(516/999): loss=0.32634295601103863\n",
      "Stochastic Gradient Descent(517/999): loss=0.2973337631052055\n",
      "Stochastic Gradient Descent(518/999): loss=0.2674271429518661\n",
      "Stochastic Gradient Descent(519/999): loss=0.29956873188506367\n",
      "Stochastic Gradient Descent(520/999): loss=0.3980430840746449\n",
      "Stochastic Gradient Descent(521/999): loss=0.33708878697811884\n",
      "Stochastic Gradient Descent(522/999): loss=0.28945576223412334\n",
      "Stochastic Gradient Descent(523/999): loss=0.2853109931269322\n",
      "Stochastic Gradient Descent(524/999): loss=0.4084884835743031\n",
      "Stochastic Gradient Descent(525/999): loss=0.34432198256349056\n",
      "Stochastic Gradient Descent(526/999): loss=0.3977732922866656\n",
      "Stochastic Gradient Descent(527/999): loss=0.30771052386884284\n",
      "Stochastic Gradient Descent(528/999): loss=0.39526712952666687\n",
      "Stochastic Gradient Descent(529/999): loss=0.3186060238424177\n",
      "Stochastic Gradient Descent(530/999): loss=0.30641484488355325\n",
      "Stochastic Gradient Descent(531/999): loss=0.37333103628611314\n",
      "Stochastic Gradient Descent(532/999): loss=0.37264544852751824\n",
      "Stochastic Gradient Descent(533/999): loss=0.3268481327589193\n",
      "Stochastic Gradient Descent(534/999): loss=0.34834728211046145\n",
      "Stochastic Gradient Descent(535/999): loss=0.2757006933231938\n",
      "Stochastic Gradient Descent(536/999): loss=0.34606081319058396\n",
      "Stochastic Gradient Descent(537/999): loss=0.45022448248675867\n",
      "Stochastic Gradient Descent(538/999): loss=0.41488445230678417\n",
      "Stochastic Gradient Descent(539/999): loss=0.3163067478301101\n",
      "Stochastic Gradient Descent(540/999): loss=0.44498196135324114\n",
      "Stochastic Gradient Descent(541/999): loss=0.37915158900476265\n",
      "Stochastic Gradient Descent(542/999): loss=0.2708771393506173\n",
      "Stochastic Gradient Descent(543/999): loss=0.3466197381993403\n",
      "Stochastic Gradient Descent(544/999): loss=0.4666409794552365\n",
      "Stochastic Gradient Descent(545/999): loss=0.36810857777484074\n",
      "Stochastic Gradient Descent(546/999): loss=0.33114263305561226\n",
      "Stochastic Gradient Descent(547/999): loss=0.33838816417004913\n",
      "Stochastic Gradient Descent(548/999): loss=0.2993811643155484\n",
      "Stochastic Gradient Descent(549/999): loss=0.36810380249411556\n",
      "Stochastic Gradient Descent(550/999): loss=0.3148592485895562\n",
      "Stochastic Gradient Descent(551/999): loss=0.3017675446100489\n",
      "Stochastic Gradient Descent(552/999): loss=0.39221218851551076\n",
      "Stochastic Gradient Descent(553/999): loss=0.428338315485299\n",
      "Stochastic Gradient Descent(554/999): loss=0.4121656182321774\n",
      "Stochastic Gradient Descent(555/999): loss=0.31934283622952947\n",
      "Stochastic Gradient Descent(556/999): loss=0.3397639016218246\n",
      "Stochastic Gradient Descent(557/999): loss=0.30426548765100625\n",
      "Stochastic Gradient Descent(558/999): loss=0.32135257207456547\n",
      "Stochastic Gradient Descent(559/999): loss=0.320618811284833\n",
      "Stochastic Gradient Descent(560/999): loss=0.3483149684506474\n",
      "Stochastic Gradient Descent(561/999): loss=0.3799267668281262\n",
      "Stochastic Gradient Descent(562/999): loss=0.3537416962030222\n",
      "Stochastic Gradient Descent(563/999): loss=0.343112768936501\n",
      "Stochastic Gradient Descent(564/999): loss=0.27854904454228036\n",
      "Stochastic Gradient Descent(565/999): loss=0.36554971760949057\n",
      "Stochastic Gradient Descent(566/999): loss=0.3175788194038509\n",
      "Stochastic Gradient Descent(567/999): loss=0.34641671821455494\n",
      "Stochastic Gradient Descent(568/999): loss=0.31937462337994993\n",
      "Stochastic Gradient Descent(569/999): loss=0.26622373537793276\n",
      "Stochastic Gradient Descent(570/999): loss=0.35262672052654104\n",
      "Stochastic Gradient Descent(571/999): loss=0.3089328544200785\n",
      "Stochastic Gradient Descent(572/999): loss=0.42481544990366105\n",
      "Stochastic Gradient Descent(573/999): loss=0.34533678476474394\n",
      "Stochastic Gradient Descent(574/999): loss=0.4003424433958162\n",
      "Stochastic Gradient Descent(575/999): loss=0.3432412516491214\n",
      "Stochastic Gradient Descent(576/999): loss=0.3863705174452612\n",
      "Stochastic Gradient Descent(577/999): loss=0.3532214045885187\n",
      "Stochastic Gradient Descent(578/999): loss=0.32344139610527484\n",
      "Stochastic Gradient Descent(579/999): loss=0.3041594860921973\n",
      "Stochastic Gradient Descent(580/999): loss=0.40586683792316086\n",
      "Stochastic Gradient Descent(581/999): loss=0.3799443205616502\n",
      "Stochastic Gradient Descent(582/999): loss=0.30533717330445365\n",
      "Stochastic Gradient Descent(583/999): loss=0.29034829012004354\n",
      "Stochastic Gradient Descent(584/999): loss=0.3825585263502804\n",
      "Stochastic Gradient Descent(585/999): loss=0.44217753109426444\n",
      "Stochastic Gradient Descent(586/999): loss=0.3867348679443452\n",
      "Stochastic Gradient Descent(587/999): loss=0.37022790903030406\n",
      "Stochastic Gradient Descent(588/999): loss=0.3057363450093284\n",
      "Stochastic Gradient Descent(589/999): loss=0.3893162820431082\n",
      "Stochastic Gradient Descent(590/999): loss=0.3101470110902472\n",
      "Stochastic Gradient Descent(591/999): loss=0.3854293614461001\n",
      "Stochastic Gradient Descent(592/999): loss=0.30996581984407423\n",
      "Stochastic Gradient Descent(593/999): loss=0.44933969016911957\n",
      "Stochastic Gradient Descent(594/999): loss=0.36574165842984313\n",
      "Stochastic Gradient Descent(595/999): loss=0.3942016392718432\n",
      "Stochastic Gradient Descent(596/999): loss=0.3969806785761298\n",
      "Stochastic Gradient Descent(597/999): loss=0.38069254347030806\n",
      "Stochastic Gradient Descent(598/999): loss=0.3345776710041141\n",
      "Stochastic Gradient Descent(599/999): loss=0.32226810130616296\n",
      "Stochastic Gradient Descent(600/999): loss=0.3697541834074295\n",
      "Stochastic Gradient Descent(601/999): loss=0.37443805226662474\n",
      "Stochastic Gradient Descent(602/999): loss=0.3116765778350249\n",
      "Stochastic Gradient Descent(603/999): loss=0.3093140482945567\n",
      "Stochastic Gradient Descent(604/999): loss=0.40094831825022853\n",
      "Stochastic Gradient Descent(605/999): loss=0.4480279891146905\n",
      "Stochastic Gradient Descent(606/999): loss=0.3008863301064542\n",
      "Stochastic Gradient Descent(607/999): loss=0.39230398880393\n",
      "Stochastic Gradient Descent(608/999): loss=0.3569534110727247\n",
      "Stochastic Gradient Descent(609/999): loss=0.3397188763903523\n",
      "Stochastic Gradient Descent(610/999): loss=0.3609796076581038\n",
      "Stochastic Gradient Descent(611/999): loss=0.2485737601076237\n",
      "Stochastic Gradient Descent(612/999): loss=0.2893744550453227\n",
      "Stochastic Gradient Descent(613/999): loss=0.4310324014264871\n",
      "Stochastic Gradient Descent(614/999): loss=0.4322189371335782\n",
      "Stochastic Gradient Descent(615/999): loss=0.314774053964813\n",
      "Stochastic Gradient Descent(616/999): loss=0.4063824562792715\n",
      "Stochastic Gradient Descent(617/999): loss=0.43345616089336775\n",
      "Stochastic Gradient Descent(618/999): loss=0.34533907743471914\n",
      "Stochastic Gradient Descent(619/999): loss=0.27730845727009235\n",
      "Stochastic Gradient Descent(620/999): loss=0.30130542709155994\n",
      "Stochastic Gradient Descent(621/999): loss=0.3675846590085957\n",
      "Stochastic Gradient Descent(622/999): loss=0.36999513081372976\n",
      "Stochastic Gradient Descent(623/999): loss=0.24588539662634956\n",
      "Stochastic Gradient Descent(624/999): loss=0.37209743102430465\n",
      "Stochastic Gradient Descent(625/999): loss=0.2975709507422834\n",
      "Stochastic Gradient Descent(626/999): loss=0.3485214208612151\n",
      "Stochastic Gradient Descent(627/999): loss=0.3388073730978804\n",
      "Stochastic Gradient Descent(628/999): loss=0.28655031702193734\n",
      "Stochastic Gradient Descent(629/999): loss=0.35755564646339016\n",
      "Stochastic Gradient Descent(630/999): loss=0.33829472695661345\n",
      "Stochastic Gradient Descent(631/999): loss=0.33642540629723655\n",
      "Stochastic Gradient Descent(632/999): loss=0.3046113434522426\n",
      "Stochastic Gradient Descent(633/999): loss=0.41392254257103817\n",
      "Stochastic Gradient Descent(634/999): loss=0.3483935736671959\n",
      "Stochastic Gradient Descent(635/999): loss=0.26845187251083025\n",
      "Stochastic Gradient Descent(636/999): loss=0.35474509796932524\n",
      "Stochastic Gradient Descent(637/999): loss=0.33994198494642947\n",
      "Stochastic Gradient Descent(638/999): loss=0.45582810931253925\n",
      "Stochastic Gradient Descent(639/999): loss=0.3917479950741465\n",
      "Stochastic Gradient Descent(640/999): loss=0.36441406163366563\n",
      "Stochastic Gradient Descent(641/999): loss=0.3602161968594477\n",
      "Stochastic Gradient Descent(642/999): loss=0.2984027477471401\n",
      "Stochastic Gradient Descent(643/999): loss=0.3907002246178692\n",
      "Stochastic Gradient Descent(644/999): loss=0.3455519349996421\n",
      "Stochastic Gradient Descent(645/999): loss=0.2873701290446354\n",
      "Stochastic Gradient Descent(646/999): loss=0.30807371959184576\n",
      "Stochastic Gradient Descent(647/999): loss=0.2861124294690553\n",
      "Stochastic Gradient Descent(648/999): loss=0.33786655820945705\n",
      "Stochastic Gradient Descent(649/999): loss=0.32124726979143653\n",
      "Stochastic Gradient Descent(650/999): loss=0.3400866907232455\n",
      "Stochastic Gradient Descent(651/999): loss=0.3474744972198339\n",
      "Stochastic Gradient Descent(652/999): loss=0.32818963963785175\n",
      "Stochastic Gradient Descent(653/999): loss=0.3780014472751535\n",
      "Stochastic Gradient Descent(654/999): loss=0.38302094144246956\n",
      "Stochastic Gradient Descent(655/999): loss=0.3380301772411851\n",
      "Stochastic Gradient Descent(656/999): loss=0.3479244084148246\n",
      "Stochastic Gradient Descent(657/999): loss=0.344587117968983\n",
      "Stochastic Gradient Descent(658/999): loss=0.3830541925654558\n",
      "Stochastic Gradient Descent(659/999): loss=0.3465010185144291\n",
      "Stochastic Gradient Descent(660/999): loss=0.3616290196432038\n",
      "Stochastic Gradient Descent(661/999): loss=0.3515924239493591\n",
      "Stochastic Gradient Descent(662/999): loss=0.36099383090838066\n",
      "Stochastic Gradient Descent(663/999): loss=0.2948222181252128\n",
      "Stochastic Gradient Descent(664/999): loss=0.34170909306073743\n",
      "Stochastic Gradient Descent(665/999): loss=0.5229095392904256\n",
      "Stochastic Gradient Descent(666/999): loss=0.32604772461848486\n",
      "Stochastic Gradient Descent(667/999): loss=0.37709623529854774\n",
      "Stochastic Gradient Descent(668/999): loss=0.3722058589173799\n",
      "Stochastic Gradient Descent(669/999): loss=0.317536487667889\n",
      "Stochastic Gradient Descent(670/999): loss=0.41425247177018826\n",
      "Stochastic Gradient Descent(671/999): loss=0.2587194113884072\n",
      "Stochastic Gradient Descent(672/999): loss=0.32149545253366796\n",
      "Stochastic Gradient Descent(673/999): loss=0.38362054848672783\n",
      "Stochastic Gradient Descent(674/999): loss=0.3908099654680003\n",
      "Stochastic Gradient Descent(675/999): loss=0.42333516421201833\n",
      "Stochastic Gradient Descent(676/999): loss=0.39656581477475816\n",
      "Stochastic Gradient Descent(677/999): loss=0.3090520969213838\n",
      "Stochastic Gradient Descent(678/999): loss=0.41345895784539594\n",
      "Stochastic Gradient Descent(679/999): loss=0.4018110368115174\n",
      "Stochastic Gradient Descent(680/999): loss=0.2747413469292182\n",
      "Stochastic Gradient Descent(681/999): loss=0.338353863028227\n",
      "Stochastic Gradient Descent(682/999): loss=0.31074324959264615\n",
      "Stochastic Gradient Descent(683/999): loss=0.3675686993850833\n",
      "Stochastic Gradient Descent(684/999): loss=0.3623121381057734\n",
      "Stochastic Gradient Descent(685/999): loss=0.2665623851296743\n",
      "Stochastic Gradient Descent(686/999): loss=0.33219677650705093\n",
      "Stochastic Gradient Descent(687/999): loss=0.3404260946277356\n",
      "Stochastic Gradient Descent(688/999): loss=0.32543831938816503\n",
      "Stochastic Gradient Descent(689/999): loss=0.33591461324750127\n",
      "Stochastic Gradient Descent(690/999): loss=0.3876863173515577\n",
      "Stochastic Gradient Descent(691/999): loss=0.2921402339317248\n",
      "Stochastic Gradient Descent(692/999): loss=0.40781745179643303\n",
      "Stochastic Gradient Descent(693/999): loss=0.24232343607774331\n",
      "Stochastic Gradient Descent(694/999): loss=0.3057831640617723\n",
      "Stochastic Gradient Descent(695/999): loss=0.34353794161567924\n",
      "Stochastic Gradient Descent(696/999): loss=0.38944167786147416\n",
      "Stochastic Gradient Descent(697/999): loss=0.3614390269437708\n",
      "Stochastic Gradient Descent(698/999): loss=0.2751387672224814\n",
      "Stochastic Gradient Descent(699/999): loss=0.33129689927454153\n",
      "Stochastic Gradient Descent(700/999): loss=0.41013512576747035\n",
      "Stochastic Gradient Descent(701/999): loss=0.3993513735902196\n",
      "Stochastic Gradient Descent(702/999): loss=0.35605256315540734\n",
      "Stochastic Gradient Descent(703/999): loss=0.3589969950848312\n",
      "Stochastic Gradient Descent(704/999): loss=0.33265571788432935\n",
      "Stochastic Gradient Descent(705/999): loss=0.37751732730909504\n",
      "Stochastic Gradient Descent(706/999): loss=0.37794616290872235\n",
      "Stochastic Gradient Descent(707/999): loss=0.3901458118564027\n",
      "Stochastic Gradient Descent(708/999): loss=0.24555065278345506\n",
      "Stochastic Gradient Descent(709/999): loss=0.3361155462116615\n",
      "Stochastic Gradient Descent(710/999): loss=0.34799564246446907\n",
      "Stochastic Gradient Descent(711/999): loss=0.3095667588828566\n",
      "Stochastic Gradient Descent(712/999): loss=0.4392130630749187\n",
      "Stochastic Gradient Descent(713/999): loss=0.38747044308412915\n",
      "Stochastic Gradient Descent(714/999): loss=0.3223476956054444\n",
      "Stochastic Gradient Descent(715/999): loss=0.42865946695536045\n",
      "Stochastic Gradient Descent(716/999): loss=0.35203294361564325\n",
      "Stochastic Gradient Descent(717/999): loss=0.39330082386016263\n",
      "Stochastic Gradient Descent(718/999): loss=0.3911446713261275\n",
      "Stochastic Gradient Descent(719/999): loss=0.31086119301234116\n",
      "Stochastic Gradient Descent(720/999): loss=0.3148834286563337\n",
      "Stochastic Gradient Descent(721/999): loss=0.45204674058492145\n",
      "Stochastic Gradient Descent(722/999): loss=0.3737058626645063\n",
      "Stochastic Gradient Descent(723/999): loss=0.3680371620863423\n",
      "Stochastic Gradient Descent(724/999): loss=0.3537980406962196\n",
      "Stochastic Gradient Descent(725/999): loss=0.29234579635850083\n",
      "Stochastic Gradient Descent(726/999): loss=0.3840422505765638\n",
      "Stochastic Gradient Descent(727/999): loss=0.38130788494623374\n",
      "Stochastic Gradient Descent(728/999): loss=0.33659023750639067\n",
      "Stochastic Gradient Descent(729/999): loss=0.38673544466891374\n",
      "Stochastic Gradient Descent(730/999): loss=0.33010862731305446\n",
      "Stochastic Gradient Descent(731/999): loss=0.35607829493697757\n",
      "Stochastic Gradient Descent(732/999): loss=0.38598763562250354\n",
      "Stochastic Gradient Descent(733/999): loss=0.3932433338435745\n",
      "Stochastic Gradient Descent(734/999): loss=0.34039033115406075\n",
      "Stochastic Gradient Descent(735/999): loss=0.4094426330381762\n",
      "Stochastic Gradient Descent(736/999): loss=0.3869460843391871\n",
      "Stochastic Gradient Descent(737/999): loss=0.2863013497629781\n",
      "Stochastic Gradient Descent(738/999): loss=0.26411879620513573\n",
      "Stochastic Gradient Descent(739/999): loss=0.3405275168514533\n",
      "Stochastic Gradient Descent(740/999): loss=0.3074704957916671\n",
      "Stochastic Gradient Descent(741/999): loss=0.3159990487797231\n",
      "Stochastic Gradient Descent(742/999): loss=0.2690505925789586\n",
      "Stochastic Gradient Descent(743/999): loss=0.3405975129718531\n",
      "Stochastic Gradient Descent(744/999): loss=0.3334412475835534\n",
      "Stochastic Gradient Descent(745/999): loss=0.32687052288594415\n",
      "Stochastic Gradient Descent(746/999): loss=0.31908099025821335\n",
      "Stochastic Gradient Descent(747/999): loss=0.34788388516827495\n",
      "Stochastic Gradient Descent(748/999): loss=0.29916446080338516\n",
      "Stochastic Gradient Descent(749/999): loss=0.34903334546559234\n",
      "Stochastic Gradient Descent(750/999): loss=0.36227783717929646\n",
      "Stochastic Gradient Descent(751/999): loss=0.3159021186796942\n",
      "Stochastic Gradient Descent(752/999): loss=0.37185561843688875\n",
      "Stochastic Gradient Descent(753/999): loss=0.31788468547341586\n",
      "Stochastic Gradient Descent(754/999): loss=0.31018892363594935\n",
      "Stochastic Gradient Descent(755/999): loss=0.34970885418140907\n",
      "Stochastic Gradient Descent(756/999): loss=0.3620972916015689\n",
      "Stochastic Gradient Descent(757/999): loss=0.3604548147471089\n",
      "Stochastic Gradient Descent(758/999): loss=0.34016251462717984\n",
      "Stochastic Gradient Descent(759/999): loss=0.294882925611973\n",
      "Stochastic Gradient Descent(760/999): loss=0.26268286710236965\n",
      "Stochastic Gradient Descent(761/999): loss=0.27337968272265156\n",
      "Stochastic Gradient Descent(762/999): loss=0.3609724336848851\n",
      "Stochastic Gradient Descent(763/999): loss=0.3016419510700224\n",
      "Stochastic Gradient Descent(764/999): loss=0.3852302599647178\n",
      "Stochastic Gradient Descent(765/999): loss=0.29645231082644946\n",
      "Stochastic Gradient Descent(766/999): loss=0.35731610088819243\n",
      "Stochastic Gradient Descent(767/999): loss=0.3345002412689196\n",
      "Stochastic Gradient Descent(768/999): loss=0.38912189663990787\n",
      "Stochastic Gradient Descent(769/999): loss=0.347253717094971\n",
      "Stochastic Gradient Descent(770/999): loss=0.3125797763084191\n",
      "Stochastic Gradient Descent(771/999): loss=0.4092483620954027\n",
      "Stochastic Gradient Descent(772/999): loss=0.27096914489117924\n",
      "Stochastic Gradient Descent(773/999): loss=0.44137279843289606\n",
      "Stochastic Gradient Descent(774/999): loss=0.2393030251297165\n",
      "Stochastic Gradient Descent(775/999): loss=0.36396785696945083\n",
      "Stochastic Gradient Descent(776/999): loss=0.30890972090699603\n",
      "Stochastic Gradient Descent(777/999): loss=0.3473535951506448\n",
      "Stochastic Gradient Descent(778/999): loss=0.3258300441188913\n",
      "Stochastic Gradient Descent(779/999): loss=0.4289079562093323\n",
      "Stochastic Gradient Descent(780/999): loss=0.3824365527813069\n",
      "Stochastic Gradient Descent(781/999): loss=0.3220835738187131\n",
      "Stochastic Gradient Descent(782/999): loss=0.29167113178012194\n",
      "Stochastic Gradient Descent(783/999): loss=0.38840031778430967\n",
      "Stochastic Gradient Descent(784/999): loss=0.3197265464622917\n",
      "Stochastic Gradient Descent(785/999): loss=0.3787673463725081\n",
      "Stochastic Gradient Descent(786/999): loss=0.3491230086284696\n",
      "Stochastic Gradient Descent(787/999): loss=0.29108088572188284\n",
      "Stochastic Gradient Descent(788/999): loss=0.3591951749061153\n",
      "Stochastic Gradient Descent(789/999): loss=0.3066638601756121\n",
      "Stochastic Gradient Descent(790/999): loss=0.4087352803644842\n",
      "Stochastic Gradient Descent(791/999): loss=0.258904770976192\n",
      "Stochastic Gradient Descent(792/999): loss=0.3920311973729035\n",
      "Stochastic Gradient Descent(793/999): loss=0.4105395501631328\n",
      "Stochastic Gradient Descent(794/999): loss=0.3645391488741116\n",
      "Stochastic Gradient Descent(795/999): loss=0.36888219584485915\n",
      "Stochastic Gradient Descent(796/999): loss=0.318930844600612\n",
      "Stochastic Gradient Descent(797/999): loss=0.2911523205278762\n",
      "Stochastic Gradient Descent(798/999): loss=0.2780516848318923\n",
      "Stochastic Gradient Descent(799/999): loss=0.3911560815944894\n",
      "Stochastic Gradient Descent(800/999): loss=0.32390037294450524\n",
      "Stochastic Gradient Descent(801/999): loss=0.3103447656379245\n",
      "Stochastic Gradient Descent(802/999): loss=0.2911519070334756\n",
      "Stochastic Gradient Descent(803/999): loss=0.40722706468379904\n",
      "Stochastic Gradient Descent(804/999): loss=0.3333216374575592\n",
      "Stochastic Gradient Descent(805/999): loss=0.4106592605588372\n",
      "Stochastic Gradient Descent(806/999): loss=0.2991734886489358\n",
      "Stochastic Gradient Descent(807/999): loss=0.33340290892189245\n",
      "Stochastic Gradient Descent(808/999): loss=0.34140369581285623\n",
      "Stochastic Gradient Descent(809/999): loss=0.4235456196101395\n",
      "Stochastic Gradient Descent(810/999): loss=0.3553935246240523\n",
      "Stochastic Gradient Descent(811/999): loss=0.3844136322018457\n",
      "Stochastic Gradient Descent(812/999): loss=0.30214886034728755\n",
      "Stochastic Gradient Descent(813/999): loss=0.4089486116444121\n",
      "Stochastic Gradient Descent(814/999): loss=0.35054401823541126\n",
      "Stochastic Gradient Descent(815/999): loss=0.2473883647112052\n",
      "Stochastic Gradient Descent(816/999): loss=0.3981606565045055\n",
      "Stochastic Gradient Descent(817/999): loss=0.38428504340293423\n",
      "Stochastic Gradient Descent(818/999): loss=0.33361372153457636\n",
      "Stochastic Gradient Descent(819/999): loss=0.40011637944204487\n",
      "Stochastic Gradient Descent(820/999): loss=0.3606729388424024\n",
      "Stochastic Gradient Descent(821/999): loss=0.3244701870964103\n",
      "Stochastic Gradient Descent(822/999): loss=0.3289504721257464\n",
      "Stochastic Gradient Descent(823/999): loss=0.26569648677598645\n",
      "Stochastic Gradient Descent(824/999): loss=0.3670097077079324\n",
      "Stochastic Gradient Descent(825/999): loss=0.3288276960934051\n",
      "Stochastic Gradient Descent(826/999): loss=0.3736020121333698\n",
      "Stochastic Gradient Descent(827/999): loss=0.36871350605993874\n",
      "Stochastic Gradient Descent(828/999): loss=0.35556245287916255\n",
      "Stochastic Gradient Descent(829/999): loss=0.44269249322437126\n",
      "Stochastic Gradient Descent(830/999): loss=0.36584775026951233\n",
      "Stochastic Gradient Descent(831/999): loss=0.3451209772879817\n",
      "Stochastic Gradient Descent(832/999): loss=0.32192175576152365\n",
      "Stochastic Gradient Descent(833/999): loss=0.3819952502175945\n",
      "Stochastic Gradient Descent(834/999): loss=0.25982571284190553\n",
      "Stochastic Gradient Descent(835/999): loss=0.34004319862501475\n",
      "Stochastic Gradient Descent(836/999): loss=0.34074346619346485\n",
      "Stochastic Gradient Descent(837/999): loss=0.43382003298704525\n",
      "Stochastic Gradient Descent(838/999): loss=0.3229497602447841\n",
      "Stochastic Gradient Descent(839/999): loss=0.3139024347334744\n",
      "Stochastic Gradient Descent(840/999): loss=0.35830881165541983\n",
      "Stochastic Gradient Descent(841/999): loss=0.3026936408258038\n",
      "Stochastic Gradient Descent(842/999): loss=0.382528440271875\n",
      "Stochastic Gradient Descent(843/999): loss=0.424073019787968\n",
      "Stochastic Gradient Descent(844/999): loss=0.3454683076563147\n",
      "Stochastic Gradient Descent(845/999): loss=0.38097277938035434\n",
      "Stochastic Gradient Descent(846/999): loss=0.40709161130276383\n",
      "Stochastic Gradient Descent(847/999): loss=0.2966447794908401\n",
      "Stochastic Gradient Descent(848/999): loss=0.27791506015682527\n",
      "Stochastic Gradient Descent(849/999): loss=0.2775004080613969\n",
      "Stochastic Gradient Descent(850/999): loss=0.3513362681983536\n",
      "Stochastic Gradient Descent(851/999): loss=0.3652633392841566\n",
      "Stochastic Gradient Descent(852/999): loss=0.27539971352516934\n",
      "Stochastic Gradient Descent(853/999): loss=0.3110591413466341\n",
      "Stochastic Gradient Descent(854/999): loss=0.34943485281276215\n",
      "Stochastic Gradient Descent(855/999): loss=0.3151091187193955\n",
      "Stochastic Gradient Descent(856/999): loss=0.3686140793133588\n",
      "Stochastic Gradient Descent(857/999): loss=0.3479276384212025\n",
      "Stochastic Gradient Descent(858/999): loss=0.3130382958746594\n",
      "Stochastic Gradient Descent(859/999): loss=0.25045726219075926\n",
      "Stochastic Gradient Descent(860/999): loss=0.25262326314131345\n",
      "Stochastic Gradient Descent(861/999): loss=0.44129324007995835\n",
      "Stochastic Gradient Descent(862/999): loss=0.4355046183059409\n",
      "Stochastic Gradient Descent(863/999): loss=0.3224143497817117\n",
      "Stochastic Gradient Descent(864/999): loss=0.2671949131312298\n",
      "Stochastic Gradient Descent(865/999): loss=0.31005746866382095\n",
      "Stochastic Gradient Descent(866/999): loss=0.2825588415622375\n",
      "Stochastic Gradient Descent(867/999): loss=0.3006175369181412\n",
      "Stochastic Gradient Descent(868/999): loss=0.33950157813607423\n",
      "Stochastic Gradient Descent(869/999): loss=0.30826215582530614\n",
      "Stochastic Gradient Descent(870/999): loss=0.3155956635056487\n",
      "Stochastic Gradient Descent(871/999): loss=0.2868350621057051\n",
      "Stochastic Gradient Descent(872/999): loss=0.26720864799778193\n",
      "Stochastic Gradient Descent(873/999): loss=0.36294605501072524\n",
      "Stochastic Gradient Descent(874/999): loss=0.4215919951342787\n",
      "Stochastic Gradient Descent(875/999): loss=0.3437334084558842\n",
      "Stochastic Gradient Descent(876/999): loss=0.3430423895247952\n",
      "Stochastic Gradient Descent(877/999): loss=0.34674846149218214\n",
      "Stochastic Gradient Descent(878/999): loss=0.38000982440999387\n",
      "Stochastic Gradient Descent(879/999): loss=0.31236794446500804\n",
      "Stochastic Gradient Descent(880/999): loss=0.3430903081507852\n",
      "Stochastic Gradient Descent(881/999): loss=0.34110729726114014\n",
      "Stochastic Gradient Descent(882/999): loss=0.376452620398613\n",
      "Stochastic Gradient Descent(883/999): loss=0.2602372201941222\n",
      "Stochastic Gradient Descent(884/999): loss=0.40822149723952433\n",
      "Stochastic Gradient Descent(885/999): loss=0.40636061398634327\n",
      "Stochastic Gradient Descent(886/999): loss=0.33425736054856847\n",
      "Stochastic Gradient Descent(887/999): loss=0.30495059423131876\n",
      "Stochastic Gradient Descent(888/999): loss=0.3197005650711532\n",
      "Stochastic Gradient Descent(889/999): loss=0.29705023768212085\n",
      "Stochastic Gradient Descent(890/999): loss=0.27875018751916253\n",
      "Stochastic Gradient Descent(891/999): loss=0.3494822163671445\n",
      "Stochastic Gradient Descent(892/999): loss=0.28920628416889865\n",
      "Stochastic Gradient Descent(893/999): loss=0.38707378494209815\n",
      "Stochastic Gradient Descent(894/999): loss=0.35822283663557103\n",
      "Stochastic Gradient Descent(895/999): loss=0.3076788751751626\n",
      "Stochastic Gradient Descent(896/999): loss=0.3624431881164679\n",
      "Stochastic Gradient Descent(897/999): loss=0.3914112327967463\n",
      "Stochastic Gradient Descent(898/999): loss=0.4070417072547064\n",
      "Stochastic Gradient Descent(899/999): loss=0.35273953160004007\n",
      "Stochastic Gradient Descent(900/999): loss=0.2839040241338562\n",
      "Stochastic Gradient Descent(901/999): loss=0.29093772486171293\n",
      "Stochastic Gradient Descent(902/999): loss=0.3787430735275713\n",
      "Stochastic Gradient Descent(903/999): loss=0.31999357160495856\n",
      "Stochastic Gradient Descent(904/999): loss=0.27026768280243857\n",
      "Stochastic Gradient Descent(905/999): loss=0.31890399684406895\n",
      "Stochastic Gradient Descent(906/999): loss=0.34890441438188075\n",
      "Stochastic Gradient Descent(907/999): loss=0.30564816854585936\n",
      "Stochastic Gradient Descent(908/999): loss=0.3592521247049131\n",
      "Stochastic Gradient Descent(909/999): loss=0.29591796327077885\n",
      "Stochastic Gradient Descent(910/999): loss=0.4466243196960856\n",
      "Stochastic Gradient Descent(911/999): loss=0.3134044589283948\n",
      "Stochastic Gradient Descent(912/999): loss=0.34018256447919265\n",
      "Stochastic Gradient Descent(913/999): loss=0.34905425333018825\n",
      "Stochastic Gradient Descent(914/999): loss=0.33396795259320994\n",
      "Stochastic Gradient Descent(915/999): loss=0.3756747289482324\n",
      "Stochastic Gradient Descent(916/999): loss=0.3910005220505603\n",
      "Stochastic Gradient Descent(917/999): loss=0.44597944766883457\n",
      "Stochastic Gradient Descent(918/999): loss=0.3237824953379384\n",
      "Stochastic Gradient Descent(919/999): loss=0.34722183796766726\n",
      "Stochastic Gradient Descent(920/999): loss=0.36660413753925225\n",
      "Stochastic Gradient Descent(921/999): loss=0.30239250608409696\n",
      "Stochastic Gradient Descent(922/999): loss=0.38730212869227365\n",
      "Stochastic Gradient Descent(923/999): loss=0.3840926688411625\n",
      "Stochastic Gradient Descent(924/999): loss=0.32844497573797227\n",
      "Stochastic Gradient Descent(925/999): loss=0.37769565721927784\n",
      "Stochastic Gradient Descent(926/999): loss=0.37148412003086617\n",
      "Stochastic Gradient Descent(927/999): loss=0.3116339505885698\n",
      "Stochastic Gradient Descent(928/999): loss=0.3154904028444575\n",
      "Stochastic Gradient Descent(929/999): loss=0.3376486513896111\n",
      "Stochastic Gradient Descent(930/999): loss=0.33682165702352934\n",
      "Stochastic Gradient Descent(931/999): loss=0.317042002551903\n",
      "Stochastic Gradient Descent(932/999): loss=0.31845619336176817\n",
      "Stochastic Gradient Descent(933/999): loss=0.3600089661171272\n",
      "Stochastic Gradient Descent(934/999): loss=0.3495134581644164\n",
      "Stochastic Gradient Descent(935/999): loss=0.3350455946127209\n",
      "Stochastic Gradient Descent(936/999): loss=0.38854333318684264\n",
      "Stochastic Gradient Descent(937/999): loss=0.3555757646166075\n",
      "Stochastic Gradient Descent(938/999): loss=0.29722275714014057\n",
      "Stochastic Gradient Descent(939/999): loss=0.3809947994950494\n",
      "Stochastic Gradient Descent(940/999): loss=0.27721190575187205\n",
      "Stochastic Gradient Descent(941/999): loss=0.35261708899696964\n",
      "Stochastic Gradient Descent(942/999): loss=0.29667198294844593\n",
      "Stochastic Gradient Descent(943/999): loss=0.2831011811762983\n",
      "Stochastic Gradient Descent(944/999): loss=0.2112573874100014\n",
      "Stochastic Gradient Descent(945/999): loss=0.2779229084407993\n",
      "Stochastic Gradient Descent(946/999): loss=0.36944120731714103\n",
      "Stochastic Gradient Descent(947/999): loss=0.33804388372362576\n",
      "Stochastic Gradient Descent(948/999): loss=0.301715945538274\n",
      "Stochastic Gradient Descent(949/999): loss=0.3599346041353569\n",
      "Stochastic Gradient Descent(950/999): loss=0.3725833317742857\n",
      "Stochastic Gradient Descent(951/999): loss=0.24068759758207625\n",
      "Stochastic Gradient Descent(952/999): loss=0.3273646598800032\n",
      "Stochastic Gradient Descent(953/999): loss=0.30533139143099863\n",
      "Stochastic Gradient Descent(954/999): loss=0.4041721967541062\n",
      "Stochastic Gradient Descent(955/999): loss=0.3165298705432174\n",
      "Stochastic Gradient Descent(956/999): loss=0.2854377342280142\n",
      "Stochastic Gradient Descent(957/999): loss=0.41963054079536705\n",
      "Stochastic Gradient Descent(958/999): loss=0.3561102785428871\n",
      "Stochastic Gradient Descent(959/999): loss=0.3527491777232495\n",
      "Stochastic Gradient Descent(960/999): loss=0.3693278470537837\n",
      "Stochastic Gradient Descent(961/999): loss=0.3647686023825235\n",
      "Stochastic Gradient Descent(962/999): loss=0.4137473462011061\n",
      "Stochastic Gradient Descent(963/999): loss=0.3598213340066971\n",
      "Stochastic Gradient Descent(964/999): loss=0.35561285667902176\n",
      "Stochastic Gradient Descent(965/999): loss=0.45501399072969895\n",
      "Stochastic Gradient Descent(966/999): loss=0.3253265601360173\n",
      "Stochastic Gradient Descent(967/999): loss=0.2881418498739439\n",
      "Stochastic Gradient Descent(968/999): loss=0.37014566008951405\n",
      "Stochastic Gradient Descent(969/999): loss=0.368081607539819\n",
      "Stochastic Gradient Descent(970/999): loss=0.29357461998560036\n",
      "Stochastic Gradient Descent(971/999): loss=0.385550678953168\n",
      "Stochastic Gradient Descent(972/999): loss=0.3455293236966282\n",
      "Stochastic Gradient Descent(973/999): loss=0.29683769941540894\n",
      "Stochastic Gradient Descent(974/999): loss=0.33803211328103283\n",
      "Stochastic Gradient Descent(975/999): loss=0.2838092295520659\n",
      "Stochastic Gradient Descent(976/999): loss=0.34624354219181547\n",
      "Stochastic Gradient Descent(977/999): loss=0.2654405227203519\n",
      "Stochastic Gradient Descent(978/999): loss=0.403256197208846\n",
      "Stochastic Gradient Descent(979/999): loss=0.3558001981299521\n",
      "Stochastic Gradient Descent(980/999): loss=0.28250884591048614\n",
      "Stochastic Gradient Descent(981/999): loss=0.3467857286530659\n",
      "Stochastic Gradient Descent(982/999): loss=0.3132969108055221\n",
      "Stochastic Gradient Descent(983/999): loss=0.34891715382039984\n",
      "Stochastic Gradient Descent(984/999): loss=0.29192256419016777\n",
      "Stochastic Gradient Descent(985/999): loss=0.3221746086526098\n",
      "Stochastic Gradient Descent(986/999): loss=0.30012624227015117\n",
      "Stochastic Gradient Descent(987/999): loss=0.30496825835371033\n",
      "Stochastic Gradient Descent(988/999): loss=0.33246429415041184\n",
      "Stochastic Gradient Descent(989/999): loss=0.31687951743756093\n",
      "Stochastic Gradient Descent(990/999): loss=0.38695149047281374\n",
      "Stochastic Gradient Descent(991/999): loss=0.36244154184628335\n",
      "Stochastic Gradient Descent(992/999): loss=0.3544086731613428\n",
      "Stochastic Gradient Descent(993/999): loss=0.3901532684257875\n",
      "Stochastic Gradient Descent(994/999): loss=0.47751162784115275\n",
      "Stochastic Gradient Descent(995/999): loss=0.3698014294741385\n",
      "Stochastic Gradient Descent(996/999): loss=0.2882568234196912\n",
      "Stochastic Gradient Descent(997/999): loss=0.3189028220803843\n",
      "Stochastic Gradient Descent(998/999): loss=0.3414581963140958\n",
      "Stochastic Gradient Descent(999/999): loss=0.43291765910465835\n",
      "[ -3.08588932e-01   5.42244049e-02  -2.37568371e-01  -1.79856692e-01\n",
      "   3.14006743e-02  -1.16912172e-02   2.66790387e-01  -1.73937679e-02\n",
      "   2.08331695e-01  -1.66643064e-02  -4.89791770e-03  -1.06047084e-01\n",
      "   1.25850084e-01  -1.32490850e-02   1.84939558e-01   8.89545881e-04\n",
      "  -1.83015950e-04   1.81686867e-01   4.37162110e-03  -9.58943016e-03\n",
      "   8.60035372e-02  -1.23943414e-02  -8.34478569e-02  -1.07626498e-01\n",
      "   3.06871801e-02   3.19984727e-02   3.18489435e-02  -1.75585015e-02\n",
      "  -1.36339261e-02  -1.38006900e-02  -8.89704648e-02]\n"
     ]
    }
   ],
   "source": [
    "from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "batch_size = 50\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "# start_time = datetime.datetime.now()\n",
    "stoch_gradient_losses, stoch_gradient_ws = stochastic_gradient_descent(y, tX, w_initial, batch_size, max_iters, gamma)\n",
    "# end_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.339445598499\n",
      "parameters w:  [ -3.14664000e-01   2.93790611e-02  -2.52531197e-01  -2.54790177e-01\n",
      "  -3.03690297e-02  -1.40091635e+00   2.95716877e-01  -1.07873855e+01\n",
      "   2.67880203e-01  -2.44944270e-03  -3.28831618e+02  -1.82647939e-01\n",
      "   1.14039609e-01   2.05024908e+01   6.38861252e+01  -3.18964649e-04\n",
      "  -1.80884217e-03   6.29952650e+01  -4.48641572e-04   1.54379298e-03\n",
      "   1.21462700e-01   3.95268716e-04  -6.33223476e-02  -2.06747093e-01\n",
      "  -1.16655766e-01   9.86256504e-02   1.67907699e-01  -3.35146241e-02\n",
      "  -2.98358674e+00  -5.36388102e+00   2.78482381e+02]\n"
     ]
    }
   ],
   "source": [
    "from least_squares import *\n",
    "\n",
    "# start_ls_time = datetime.datetime.now()\n",
    "ls_wopt, ls_loss = least_squares(y,tX)\n",
    "# end_ls_time = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.340763305383\n",
      "rmse:  0.825546249925\n",
      "parameters:  [-0.30849412  0.03572402 -0.24077855 -0.21610016 -0.01054923 -0.01919922\n",
      "  0.34814992 -0.03018983  0.23504977 -0.01013674 -0.00087247 -0.15674991\n",
      "  0.11455458 -0.02161131  0.18503225 -0.00076269 -0.00130455  0.23850622\n",
      " -0.00086979  0.00249252  0.10383837  0.00113401 -0.06198121 -0.14991608\n",
      "  0.03645409  0.04367213  0.0436772  -0.02328488 -0.02309077 -0.0234825\n",
      " -0.09703028]\n"
     ]
    }
   ],
   "source": [
    "from ridge_regression import ridge_regression\n",
    "\n",
    "# lambdas = np.logspace(-3, 1, 10)      \n",
    "# _x = build_poly(x, degree)\n",
    "# x_train, x_test, y_train, y_test = split_data(tX, y, ratio, seed)\n",
    "    \n",
    "#     for lamb in lambdas:\n",
    "\n",
    "w_ridge = ridge_regression(y, tX, 0.01)\n",
    "\n",
    "# rmse_tr = np.sqrt(2*compute_loss(y, tX, w_ridge))\n",
    "# rmse_te = np.sqrt(2*compute_loss(y, tX, w_ridge))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = \"../Data/test.csv\" # TODO: download test data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
