{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCML Project-1 ~ Team #60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from costs import compute_loss\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape:  (250000,)\n",
      "original tX shape:  (250000, 30)\n",
      "ids shape:  (250000,)\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "\n",
    "DATA_TRAIN_PATH = \"../Data/train.csv\" # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "#print the shape of the offset x matrix.\n",
    "print('y shape: ',y.shape)\n",
    "print('original tX shape: ',tX.shape)\n",
    "print('ids shape: ',ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Outliers - Extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature:  0  ->  38114.0\n",
      "feature:  1  ->  0.0\n",
      "feature:  2  ->  0.0\n",
      "feature:  3  ->  0.0\n",
      "feature:  4  ->  177457.0\n",
      "feature:  5  ->  177457.0\n",
      "feature:  6  ->  177457.0\n",
      "feature:  7  ->  0.0\n",
      "feature:  8  ->  0.0\n",
      "feature:  9  ->  0.0\n",
      "feature:  10  ->  0.0\n",
      "feature:  11  ->  0.0\n",
      "feature:  12  ->  177457.0\n",
      "feature:  13  ->  0.0\n",
      "feature:  14  ->  0.0\n",
      "feature:  15  ->  0.0\n",
      "feature:  16  ->  0.0\n",
      "feature:  17  ->  0.0\n",
      "feature:  18  ->  0.0\n",
      "feature:  19  ->  0.0\n",
      "feature:  20  ->  0.0\n",
      "feature:  21  ->  0.0\n",
      "feature:  22  ->  0.0\n",
      "feature:  23  ->  99913.0\n",
      "feature:  24  ->  99913.0\n",
      "feature:  25  ->  99913.0\n",
      "feature:  26  ->  177457.0\n",
      "feature:  27  ->  177457.0\n",
      "feature:  28  ->  177457.0\n",
      "feature:  29  ->  0.0\n"
     ]
    }
   ],
   "source": [
    "outliers = count_outliers(tX,-999)\n",
    "for feature in range(tX.shape[1]):\n",
    "    print('feature: ',feature,' -> ',outliers[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000 30\n",
      "(250000, 31)\n",
      "standardized tX shape:  (250000, 31)\n",
      "tX mean shape:  (30,)\n",
      "tX std shape:  (30,)\n"
     ]
    }
   ],
   "source": [
    "#standardization\n",
    "# tX, mean_x, std_x = standardize(tX, mean_x=None, std_x=None)\n",
    "tX, mean_x, std_x = standardize_outliers(tX)\n",
    "print('standardized tX shape: ',tX.shape)\n",
    "print('tX mean shape: ',mean_x.shape)\n",
    "print('tX std shape: ',std_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of output y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Histogram of output y\n",
    "plt.hist(y, bins=10, align='mid')\n",
    "plt.title(\"Histogram of output y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of y as a function of all its features (one by one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Analyse y as a function of all the other features (one by one)\n",
    "plot_features_by_y(y,tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gradient_descent import least_squares_GD\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "# start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = least_squares_GD(y, tX, w_initial, gamma, max_iters)\n",
    "# end_time = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stochastic_gradient_descent import least_squares_SGD\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "batch_size = 50\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "stoch_gradient_losses, stoch_gradient_ws = least_squares_SGD(y, tX, w_initial, batch_size, gamma, max_iters)\n",
    "\n",
    "min_stoch_i, min_stoch_loss = get_min_param_index(stoch_gradient_losses)\n",
    "print('min index: ',min_stoch_i)\n",
    "print('min loss: ',min_stoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss= 0.340409452162\n",
      "parameters w:  [ -3.14664000e-01   1.04652995e-02  -2.54719228e-01  -2.63502968e-01\n",
      "  -1.10181135e-03   4.05482642e-02   1.67176106e-01   8.97553275e-03\n",
      "   2.82008766e-01  -2.81502578e-02  -3.29279652e+02  -1.88141151e-01\n",
      "   1.18065030e-01   1.42232505e-01   6.39663558e+01  -7.79460474e-04\n",
      "  -8.30656823e-04   6.30821834e+01  -8.61168797e-04   2.51791347e-03\n",
      "   1.03659311e-01   9.33786062e-04  -4.70019028e-02   4.17575954e-02\n",
      "  -6.14055754e-02   8.39840624e-04   2.43612108e-04  -6.79446901e-02\n",
      "   2.89296903e-03  -3.23605722e-03   2.78944548e+02]\n"
     ]
    }
   ],
   "source": [
    "from least_squares import least_squares\n",
    "\n",
    "# start_ls_time = datetime.datetime.now()\n",
    "ls_wopt, ls_loss = least_squares(y,tX)\n",
    "# end_ls_time = datetime.datetime.now()\n",
    "print('loss=',ls_loss)\n",
    "print('parameters w: ',ls_wopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ridge_regression import ridge_regression\n",
    "\n",
    "w_ridge = ridge_regression(y, tX, 0.01)\n",
    "err = compute_loss(y, tX, w_ridge)\n",
    "\n",
    "print('loss: ',err)\n",
    "print('parameters w: ',w_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ridge_regression import cross_validation_ridge_regression\n",
    "\n",
    "cross_validation_ridge_regression(y,tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=0.693147180559945\n",
      "Current iteration=1000, the loss=0.4535788439094665\n",
      "Current iteration=2000, the loss=0.2554613354813791\n",
      "Current iteration=3000, the loss=0.08502833099738107\n"
     ]
    }
   ],
   "source": [
    "from helpers import de_standardize\n",
    "from logistic_regression import logistic_regression_gradient_descent\n",
    "from plots import visualization\n",
    "\n",
    "lr_loss, lr_w = logistic_regression_gradient_descent(y, tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_GD(y, tX, ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_SGD(y, tX, ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_LS(y, tX, ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_RR(y, tX, ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_LR(y, tX, ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_RLR(y, tX, ratio=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "weights = gradient_ws[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "weights = stoch_gradient_ws[min_stoch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Least Squares\n",
    "weights = ls_wopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "weights = w_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "weights = lr_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weights ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subtract features\n",
    "weights = np.ones(31)\n",
    "# weights[[0,4,5,6,23,24,25,26,27,28]] = 0\n",
    "weights[[0,12,13,15,16,18,19,20,21,23,25,26,28,29]] = 0\n",
    "print(weights)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = \"../Data/test.csv\" # TODO: download test data and supply path here \n",
    "y_test, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568238 30\n",
      "(568238, 31)\n"
     ]
    }
   ],
   "source": [
    "tX_test, _, _ = standardize_outliers(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from logistic_regression import sigmoid\n",
    "def predict_logistic_labels(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    \n",
    "    y_pred = sigmoid(np.dot(data, weights))\n",
    "#     y_pred[np.where(y_pred <= 0)] = -1\n",
    "#     y_pred[np.where(y_pred > 0)] = 1\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = predict_logistic_labels(weights,tX_test)\n",
    "p[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: \t\t 1.0\n"
     ]
    }
   ],
   "source": [
    "# logistic\n",
    "OUTPUT_PATH = '../Data/results.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_logistic_labels(weights, tX_test)\n",
    "y_pred = y_pred.reshape(1,y_pred.shape[0]).flatten()\n",
    "print('final score: \\t\\t',(y_pred == y_test).sum()/y_pred.size)\n",
    "# create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. ...,  1.  1. -1.]\n",
      "final score: \t\t 0.276665763289\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '../Data/results.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "print('final score: \\t\\t',(y_pred == y_test).sum()/y_pred.size)\n",
    "# create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: \t\t\t 568238\n",
      "correct predictions: \t 568238\n",
      "wrong predictions: \t 0\n",
      "final score: \t\t 1.0\n"
     ]
    }
   ],
   "source": [
    "count_true = 0\n",
    "for row in range(0,len(y_pred)):\n",
    "    if y_pred[row] == y_test[row]:\n",
    "        count_true += 1\n",
    "#         print(row,' True')\n",
    "\n",
    "total_pred = y_test.shape[0]\n",
    "print('total: \\t\\t\\t',total_pred)\n",
    "print('correct predictions: \\t', count_true)\n",
    "print('wrong predictions: \\t', total_pred-count_true)\n",
    "print('final score: \\t\\t', count_true/y_pred.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
