{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCML Project-1 ~ Team #60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from costs import compute_loss\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "\n",
    "DATA_TRAIN_PATH = \"../Data/train.csv\" # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "#print the shape of the offset x matrix.\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#standardization\n",
    "tX, mean_x, std_x = standardize(tX, mean_x=None, std_x=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.5\n",
      "Gradient Descent(1/999): loss=0.4930403211757793\n",
      "Gradient Descent(2/999): loss=0.4870098501022072\n",
      "Gradient Descent(3/999): loss=0.4817168268664753\n",
      "Gradient Descent(4/999): loss=0.4770128047663221\n",
      "Gradient Descent(5/999): loss=0.4727827104481938\n",
      "Gradient Descent(6/999): loss=0.4689371916527971\n",
      "Gradient Descent(7/999): loss=0.4654067258061777\n",
      "Gradient Descent(8/999): loss=0.4621370840030196\n",
      "Gradient Descent(9/999): loss=0.45908583830059513\n",
      "Gradient Descent(10/999): loss=0.45621967211044767\n",
      "Gradient Descent(11/999): loss=0.4535123087925472\n",
      "Gradient Descent(12/999): loss=0.45094291613495796\n",
      "Gradient Descent(13/999): loss=0.4484948771748208\n",
      "Gradient Descent(14/999): loss=0.4461548430418665\n",
      "Gradient Descent(15/999): loss=0.4439120029218537\n",
      "Gradient Descent(16/999): loss=0.4417575211821269\n",
      "Gradient Descent(17/999): loss=0.4396841032046329\n",
      "Gradient Descent(18/999): loss=0.43768566032584444\n",
      "Gradient Descent(19/999): loss=0.43575705109814233\n",
      "Gradient Descent(20/999): loss=0.4338938813329179\n",
      "Gradient Descent(21/999): loss=0.4320923494233507\n",
      "Gradient Descent(22/999): loss=0.4303491265527386\n",
      "Gradient Descent(23/999): loss=0.4286612637864871\n",
      "Gradient Descent(24/999): loss=0.4270261198872494\n",
      "Gradient Descent(25/999): loss=0.42544130511008876\n",
      "Gradient Descent(26/999): loss=0.42390463732556116\n",
      "Gradient Descent(27/999): loss=0.4224141076584347\n",
      "Gradient Descent(28/999): loss=0.4209678534762368\n",
      "Gradient Descent(29/999): loss=0.4195641370594624\n",
      "Gradient Descent(30/999): loss=0.4182013286683655\n",
      "Gradient Descent(31/999): loss=0.41687789301616646\n",
      "Gradient Descent(32/999): loss=0.4155923783855599\n",
      "Gradient Descent(33/999): loss=0.4143434078001904\n",
      "Gradient Descent(34/999): loss=0.41312967179736826\n",
      "Gradient Descent(35/999): loss=0.4119499224519139\n",
      "Gradient Descent(36/999): loss=0.4108029683808321\n",
      "Gradient Descent(37/999): loss=0.4096876705199781\n",
      "Gradient Descent(38/999): loss=0.4086029385112189\n",
      "Gradient Descent(39/999): loss=0.4075477275750668\n",
      "Gradient Descent(40/999): loss=0.40652103577187293\n",
      "Gradient Descent(41/999): loss=0.4055219015763233\n",
      "Gradient Descent(42/999): loss=0.4045494017066949\n",
      "Gradient Descent(43/999): loss=0.4036026491632055\n",
      "Gradient Descent(44/999): loss=0.4026807914397397\n",
      "Gradient Descent(45/999): loss=0.40178300888091195\n",
      "Gradient Descent(46/999): loss=0.4009085131623653\n",
      "Gradient Descent(47/999): loss=0.40005654587679157\n",
      "Gradient Descent(48/999): loss=0.3992263772117204\n",
      "Gradient Descent(49/999): loss=0.3984173047078789\n",
      "Gradient Descent(50/999): loss=0.3976286520890715\n",
      "Gradient Descent(51/999): loss=0.3968597681561937\n",
      "Gradient Descent(52/999): loss=0.3961100257393025\n",
      "Gradient Descent(53/999): loss=0.3953788207026833\n",
      "Gradient Descent(54/999): loss=0.3946655709986549\n",
      "Gradient Descent(55/999): loss=0.3939697157664929\n",
      "Gradient Descent(56/999): loss=0.3932907144733497\n",
      "Gradient Descent(57/999): loss=0.3926280460944537\n",
      "Gradient Descent(58/999): loss=0.39198120833019573\n",
      "Gradient Descent(59/999): loss=0.39134971685796766\n",
      "Gradient Descent(60/999): loss=0.3907331046168399\n",
      "Gradient Descent(61/999): loss=0.3901309211233352\n",
      "Gradient Descent(62/999): loss=0.38954273181671084\n",
      "Gradient Descent(63/999): loss=0.38896811743228027\n",
      "Gradient Descent(64/999): loss=0.38840667340141793\n",
      "Gradient Descent(65/999): loss=0.3878580092769802\n",
      "Gradient Descent(66/999): loss=0.3873217481829583\n",
      "Gradient Descent(67/999): loss=0.3867975262872486\n",
      "Gradient Descent(68/999): loss=0.3862849922964908\n",
      "Gradient Descent(69/999): loss=0.38578380697198617\n",
      "Gradient Descent(70/999): loss=0.3852936426657501\n",
      "Gradient Descent(71/999): loss=0.38481418287581554\n",
      "Gradient Descent(72/999): loss=0.38434512181993413\n",
      "Gradient Descent(73/999): loss=0.3838861640268767\n",
      "Gradient Descent(74/999): loss=0.3834370239445601\n",
      "Gradient Descent(75/999): loss=0.3829974255642747\n",
      "Gradient Descent(76/999): loss=0.38256710206031536\n",
      "Gradient Descent(77/999): loss=0.382145795444348\n",
      "Gradient Descent(78/999): loss=0.3817332562338833\n",
      "Gradient Descent(79/999): loss=0.38132924313424604\n",
      "Gradient Descent(80/999): loss=0.38093352273346764\n",
      "Gradient Descent(81/999): loss=0.38054586920954475\n",
      "Gradient Descent(82/999): loss=0.38016606404953845\n",
      "Gradient Descent(83/999): loss=0.37979389578000927\n",
      "Gradient Descent(84/999): loss=0.379429159708306\n",
      "Gradient Descent(85/999): loss=0.3790716576742464\n",
      "Gradient Descent(86/999): loss=0.3787211978117502\n",
      "Gradient Descent(87/999): loss=0.37837759432000334\n",
      "Gradient Descent(88/999): loss=0.37804066724374963\n",
      "Gradient Descent(89/999): loss=0.37771024226232447\n",
      "Gradient Descent(90/999): loss=0.3773861504870649\n",
      "Gradient Descent(91/999): loss=0.37706822826673775\n",
      "Gradient Descent(92/999): loss=0.37675631700065787\n",
      "Gradient Descent(93/999): loss=0.3764502629591642\n",
      "Gradient Descent(94/999): loss=0.376149917111152\n",
      "Gradient Descent(95/999): loss=0.37585513495836603\n",
      "Gradient Descent(96/999): loss=0.37556577637616756\n",
      "Gradient Descent(97/999): loss=0.3752817054605107\n",
      "Gradient Descent(98/999): loss=0.37500279038086753\n",
      "Gradient Descent(99/999): loss=0.3747289032388546\n",
      "Gradient Descent(100/999): loss=0.3744599199323246\n",
      "Gradient Descent(101/999): loss=0.3741957200246976\n",
      "Gradient Descent(102/999): loss=0.3739361866193146\n",
      "Gradient Descent(103/999): loss=0.37368120623860707\n",
      "Gradient Descent(104/999): loss=0.373430668707883\n",
      "Gradient Descent(105/999): loss=0.37318446704354036\n",
      "Gradient Descent(106/999): loss=0.3729424973455276\n",
      "Gradient Descent(107/999): loss=0.3727046586938736\n",
      "Gradient Descent(108/999): loss=0.3724708530491261\n",
      "Gradient Descent(109/999): loss=0.3722409851565345\n",
      "Gradient Descent(110/999): loss=0.3720149624538261\n",
      "Gradient Descent(111/999): loss=0.3717926949824323\n",
      "Gradient Descent(112/999): loss=0.3715740953020202\n",
      "Gradient Descent(113/999): loss=0.37135907840820054\n",
      "Gradient Descent(114/999): loss=0.3711475616532802\n",
      "Gradient Descent(115/999): loss=0.37093946466993916\n",
      "Gradient Descent(116/999): loss=0.370734709297712\n",
      "Gradient Descent(117/999): loss=0.37053321951216506\n",
      "Gradient Descent(118/999): loss=0.3703349213566558\n",
      "Gradient Descent(119/999): loss=0.37013974287657847\n",
      "Gradient Descent(120/999): loss=0.3699476140559895\n",
      "Gradient Descent(121/999): loss=0.3697584667565246\n",
      "Gradient Descent(122/999): loss=0.3695722346585125\n",
      "Gradient Descent(123/999): loss=0.36938885320419956\n",
      "Gradient Descent(124/999): loss=0.36920825954300374\n",
      "Gradient Descent(125/999): loss=0.3690303924787152\n",
      "Gradient Descent(126/999): loss=0.368855192418569\n",
      "Gradient Descent(127/999): loss=0.368682601324117\n",
      "Gradient Descent(128/999): loss=0.3685125626638262\n",
      "Gradient Descent(129/999): loss=0.36834502136734015\n",
      "Gradient Descent(130/999): loss=0.3681799237813348\n",
      "Gradient Descent(131/999): loss=0.3680172176269095\n",
      "Gradient Descent(132/999): loss=0.3678568519584542\n",
      "Gradient Descent(133/999): loss=0.3676987771239343\n",
      "Gradient Descent(134/999): loss=0.36754294472653914\n",
      "Gradient Descent(135/999): loss=0.3673893075876433\n",
      "Gradient Descent(136/999): loss=0.36723781971102837\n",
      "Gradient Descent(137/999): loss=0.3670884362483197\n",
      "Gradient Descent(138/999): loss=0.3669411134655903\n",
      "Gradient Descent(139/999): loss=0.36679580871108647\n",
      "Gradient Descent(140/999): loss=0.36665248038403814\n",
      "Gradient Descent(141/999): loss=0.36651108790450554\n",
      "Gradient Descent(142/999): loss=0.36637159168423006\n",
      "Gradient Descent(143/999): loss=0.36623395309844803\n",
      "Gradient Descent(144/999): loss=0.3660981344586321\n",
      "Gradient Descent(145/999): loss=0.3659640989861277\n",
      "Gradient Descent(146/999): loss=0.36583181078664845\n",
      "Gradient Descent(147/999): loss=0.36570123482560074\n",
      "Gradient Descent(148/999): loss=0.3655723369042077\n",
      "Gradient Descent(149/999): loss=0.36544508363640094\n",
      "Gradient Descent(150/999): loss=0.3653194424264537\n",
      "Gradient Descent(151/999): loss=0.36519538144732827\n",
      "Gradient Descent(152/999): loss=0.3650728696197119\n",
      "Gradient Descent(153/999): loss=0.36495187659171485\n",
      "Gradient Descent(154/999): loss=0.3648323727192085\n",
      "Gradient Descent(155/999): loss=0.36471432904678064\n",
      "Gradient Descent(156/999): loss=0.3645977172892844\n",
      "Gradient Descent(157/999): loss=0.36448250981395985\n",
      "Gradient Descent(158/999): loss=0.3643686796231107\n",
      "Gradient Descent(159/999): loss=0.3642562003373118\n",
      "Gradient Descent(160/999): loss=0.3641450461791334\n",
      "Gradient Descent(161/999): loss=0.3640351919573606\n",
      "Gradient Descent(162/999): loss=0.3639266130516928\n",
      "Gradient Descent(163/999): loss=0.36381928539790576\n",
      "Gradient Descent(164/999): loss=0.3637131854734592\n",
      "Gradient Descent(165/999): loss=0.36360829028353725\n",
      "Gradient Descent(166/999): loss=0.363504577347503\n",
      "Gradient Descent(167/999): loss=0.36340202468575744\n",
      "Gradient Descent(168/999): loss=0.3633006108069846\n",
      "Gradient Descent(169/999): loss=0.36320031469577313\n",
      "Gradient Descent(170/999): loss=0.36310111580059984\n",
      "Gradient Descent(171/999): loss=0.36300299402216324\n",
      "Gradient Descent(172/999): loss=0.3629059297020566\n",
      "Gradient Descent(173/999): loss=0.36280990361176635\n",
      "Gradient Descent(174/999): loss=0.36271489694198844\n",
      "Gradient Descent(175/999): loss=0.36262089129224945\n",
      "Gradient Descent(176/999): loss=0.3625278686608239\n",
      "Gradient Descent(177/999): loss=0.3624358114349372\n",
      "Gradient Descent(178/999): loss=0.36234470238124566\n",
      "Gradient Descent(179/999): loss=0.3622545246365834\n",
      "Gradient Descent(180/999): loss=0.36216526169896857\n",
      "Gradient Descent(181/999): loss=0.36207689741886084\n",
      "Gradient Descent(182/999): loss=0.3619894159906598\n",
      "Gradient Descent(183/999): loss=0.3619028019444396\n",
      "Gradient Descent(184/999): loss=0.36181704013790955\n",
      "Gradient Descent(185/999): loss=0.3617321157485961\n",
      "Gradient Descent(186/999): loss=0.36164801426623705\n",
      "Gradient Descent(187/999): loss=0.361564721485382\n",
      "Gradient Descent(188/999): loss=0.36148222349819314\n",
      "Gradient Descent(189/999): loss=0.36140050668744006\n",
      "Gradient Descent(190/999): loss=0.36131955771968066\n",
      "Gradient Descent(191/999): loss=0.36123936353862673\n",
      "Gradient Descent(192/999): loss=0.3611599113586822\n",
      "Gradient Descent(193/999): loss=0.3610811886586557\n",
      "Gradient Descent(194/999): loss=0.36100318317563623\n",
      "Gradient Descent(195/999): loss=0.360925882899031\n",
      "Gradient Descent(196/999): loss=0.36084927606475836\n",
      "Gradient Descent(197/999): loss=0.36077335114959186\n",
      "Gradient Descent(198/999): loss=0.3606980968656512\n",
      "Gradient Descent(199/999): loss=0.3606235021550349\n",
      "Gradient Descent(200/999): loss=0.36054955618459134\n",
      "Gradient Descent(201/999): loss=0.3604762483408235\n",
      "Gradient Descent(202/999): loss=0.36040356822492364\n",
      "Gradient Descent(203/999): loss=0.3603315056479341\n",
      "Gradient Descent(204/999): loss=0.360260050626031\n",
      "Gradient Descent(205/999): loss=0.36018919337592575\n",
      "Gradient Descent(206/999): loss=0.36011892431038384\n",
      "Gradient Descent(207/999): loss=0.360049234033853\n",
      "Gradient Descent(208/999): loss=0.3599801133382045\n",
      "Gradient Descent(209/999): loss=0.35991155319857493\n",
      "Gradient Descent(210/999): loss=0.3598435447693159\n",
      "Gradient Descent(211/999): loss=0.3597760793800403\n",
      "Gradient Descent(212/999): loss=0.3597091485317665\n",
      "Gradient Descent(213/999): loss=0.3596427438931587\n",
      "Gradient Descent(214/999): loss=0.3595768572968554\n",
      "Gradient Descent(215/999): loss=0.359511480735891\n",
      "Gradient Descent(216/999): loss=0.3594466063602014\n",
      "Gradient Descent(217/999): loss=0.3593822264732148\n",
      "Gradient Descent(218/999): loss=0.35931833352852505\n",
      "Gradient Descent(219/999): loss=0.35925492012664373\n",
      "Gradient Descent(220/999): loss=0.3591919790118313\n",
      "Gradient Descent(221/999): loss=0.35912950306900165\n",
      "Gradient Descent(222/999): loss=0.359067485320703\n",
      "Gradient Descent(223/999): loss=0.35900591892416667\n",
      "Gradient Descent(224/999): loss=0.35894479716842903\n",
      "Gradient Descent(225/999): loss=0.3588841134715191\n",
      "Gradient Descent(226/999): loss=0.3588238613777123\n",
      "Gradient Descent(227/999): loss=0.3587640345548489\n",
      "Gradient Descent(228/999): loss=0.3587046267917148\n",
      "Gradient Descent(229/999): loss=0.35864563199548294\n",
      "Gradient Descent(230/999): loss=0.3585870441892144\n",
      "Gradient Descent(231/999): loss=0.358528857509417\n",
      "Gradient Descent(232/999): loss=0.3584710662036595\n",
      "Gradient Descent(233/999): loss=0.35841366462824226\n",
      "Gradient Descent(234/999): loss=0.3583566472459196\n",
      "Gradient Descent(235/999): loss=0.3583000086236748\n",
      "Gradient Descent(236/999): loss=0.35824374343054627\n",
      "Gradient Descent(237/999): loss=0.3581878464355023\n",
      "Gradient Descent(238/999): loss=0.3581323125053643\n",
      "Gradient Descent(239/999): loss=0.35807713660277707\n",
      "Gradient Descent(240/999): loss=0.35802231378422483\n",
      "Gradient Descent(241/999): loss=0.35796783919809194\n",
      "Gradient Descent(242/999): loss=0.35791370808276646\n",
      "Gradient Descent(243/999): loss=0.3578599157647867\n",
      "Gradient Descent(244/999): loss=0.3578064576570296\n",
      "Gradient Descent(245/999): loss=0.3577533292569382\n",
      "Gradient Descent(246/999): loss=0.35770052614478975\n",
      "Gradient Descent(247/999): loss=0.35764804398200095\n",
      "Gradient Descent(248/999): loss=0.35759587850947183\n",
      "Gradient Descent(249/999): loss=0.3575440255459655\n",
      "Gradient Descent(250/999): loss=0.35749248098652353\n",
      "Gradient Descent(251/999): loss=0.35744124080091616\n",
      "Gradient Descent(252/999): loss=0.3573903010321269\n",
      "Gradient Descent(253/999): loss=0.35733965779486965\n",
      "Gradient Descent(254/999): loss=0.3572893072741385\n",
      "Gradient Descent(255/999): loss=0.35723924572378896\n",
      "Gradient Descent(256/999): loss=0.35718946946514973\n",
      "Gradient Descent(257/999): loss=0.35713997488566607\n",
      "Gradient Descent(258/999): loss=0.3570907584375692\n",
      "Gradient Descent(259/999): loss=0.3570418166365786\n",
      "Gradient Descent(260/999): loss=0.3569931460606292\n",
      "Gradient Descent(261/999): loss=0.35694474334862736\n",
      "Gradient Descent(262/999): loss=0.35689660519923194\n",
      "Gradient Descent(263/999): loss=0.3568487283696644\n",
      "Gradient Descent(264/999): loss=0.35680110967454143\n",
      "Gradient Descent(265/999): loss=0.35675374598473397\n",
      "Gradient Descent(266/999): loss=0.35670663422625015\n",
      "Gradient Descent(267/999): loss=0.3566597713791422\n",
      "Gradient Descent(268/999): loss=0.35661315447643593\n",
      "Gradient Descent(269/999): loss=0.3565667806030839\n",
      "Gradient Descent(270/999): loss=0.3565206468949402\n",
      "Gradient Descent(271/999): loss=0.35647475053775607\n",
      "Gradient Descent(272/999): loss=0.35642908876619833\n",
      "Gradient Descent(273/999): loss=0.3563836588628875\n",
      "Gradient Descent(274/999): loss=0.3563384581574552\n",
      "Gradient Descent(275/999): loss=0.35629348402562394\n",
      "Gradient Descent(276/999): loss=0.3562487338883035\n",
      "Gradient Descent(277/999): loss=0.3562042052107074\n",
      "Gradient Descent(278/999): loss=0.35615989550148786\n",
      "Gradient Descent(279/999): loss=0.35611580231188905\n",
      "Gradient Descent(280/999): loss=0.35607192323491677\n",
      "Gradient Descent(281/999): loss=0.35602825590452614\n",
      "Gradient Descent(282/999): loss=0.35598479799482746\n",
      "Gradient Descent(283/999): loss=0.3559415472193049\n",
      "Gradient Descent(284/999): loss=0.35589850133005607\n",
      "Gradient Descent(285/999): loss=0.35585565811704317\n",
      "Gradient Descent(286/999): loss=0.35581301540736215\n",
      "Gradient Descent(287/999): loss=0.35577057106452603\n",
      "Gradient Descent(288/999): loss=0.3557283229877625\n",
      "Gradient Descent(289/999): loss=0.35568626911132706\n",
      "Gradient Descent(290/999): loss=0.3556444074038291\n",
      "Gradient Descent(291/999): loss=0.3556027358675728\n",
      "Gradient Descent(292/999): loss=0.35556125253791093\n",
      "Gradient Descent(293/999): loss=0.3555199554826118\n",
      "Gradient Descent(294/999): loss=0.3554788428012393\n",
      "Gradient Descent(295/999): loss=0.35543791262454616\n",
      "Gradient Descent(296/999): loss=0.35539716311387776\n",
      "Gradient Descent(297/999): loss=0.35535659246059076\n",
      "Gradient Descent(298/999): loss=0.3553161988854811\n",
      "Gradient Descent(299/999): loss=0.35527598063822463\n",
      "Gradient Descent(300/999): loss=0.3552359359968292\n",
      "Gradient Descent(301/999): loss=0.35519606326709785\n",
      "Gradient Descent(302/999): loss=0.35515636078210233\n",
      "Gradient Descent(303/999): loss=0.3551168269016672\n",
      "Gradient Descent(304/999): loss=0.35507746001186546\n",
      "Gradient Descent(305/999): loss=0.35503825852452336\n",
      "Gradient Descent(306/999): loss=0.35499922087673463\n",
      "Gradient Descent(307/999): loss=0.35496034553038647\n",
      "Gradient Descent(308/999): loss=0.3549216309716924\n",
      "Gradient Descent(309/999): loss=0.35488307571073713\n",
      "Gradient Descent(310/999): loss=0.3548446782810285\n",
      "Gradient Descent(311/999): loss=0.3548064372390596\n",
      "Gradient Descent(312/999): loss=0.3547683511638784\n",
      "Gradient Descent(313/999): loss=0.3547304186566685\n",
      "Gradient Descent(314/999): loss=0.35469263834033454\n",
      "Gradient Descent(315/999): loss=0.3546550088590989\n",
      "Gradient Descent(316/999): loss=0.3546175288781055\n",
      "Gradient Descent(317/999): loss=0.35458019708303123\n",
      "Gradient Descent(318/999): loss=0.3545430121797042\n",
      "Gradient Descent(319/999): loss=0.3545059728937328\n",
      "Gradient Descent(320/999): loss=0.3544690779701374\n",
      "Gradient Descent(321/999): loss=0.3544323261729936\n",
      "Gradient Descent(322/999): loss=0.3543957162850801\n",
      "Gradient Descent(323/999): loss=0.3543592471075344\n",
      "Gradient Descent(324/999): loss=0.35432291745951505\n",
      "Gradient Descent(325/999): loss=0.3542867261778701\n",
      "Gradient Descent(326/999): loss=0.3542506721168139\n",
      "Gradient Descent(327/999): loss=0.35421475414760745\n",
      "Gradient Descent(328/999): loss=0.35417897115824787\n",
      "Gradient Descent(329/999): loss=0.35414332205316146\n",
      "Gradient Descent(330/999): loss=0.3541078057529047\n",
      "Gradient Descent(331/999): loss=0.35407242119387083\n",
      "Gradient Descent(332/999): loss=0.3540371673280007\n",
      "Gradient Descent(333/999): loss=0.3540020431225006\n",
      "Gradient Descent(334/999): loss=0.3539670475595658\n",
      "Gradient Descent(335/999): loss=0.35393217963610896\n",
      "Gradient Descent(336/999): loss=0.35389743836349347\n",
      "Gradient Descent(337/999): loss=0.3538628227672725\n",
      "Gradient Descent(338/999): loss=0.3538283318869333\n",
      "Gradient Descent(339/999): loss=0.35379396477564634\n",
      "Gradient Descent(340/999): loss=0.3537597205000189\n",
      "Gradient Descent(341/999): loss=0.35372559813985394\n",
      "Gradient Descent(342/999): loss=0.35369159678791334\n",
      "Gradient Descent(343/999): loss=0.35365771554968634\n",
      "Gradient Descent(344/999): loss=0.35362395354316145\n",
      "Gradient Descent(345/999): loss=0.35359030989860346\n",
      "Gradient Descent(346/999): loss=0.353556783758335\n",
      "Gradient Descent(347/999): loss=0.35352337427652175\n",
      "Gradient Descent(348/999): loss=0.3534900806189613\n",
      "Gradient Descent(349/999): loss=0.3534569019628787\n",
      "Gradient Descent(350/999): loss=0.3534238374967217\n",
      "Gradient Descent(351/999): loss=0.35339088641996413\n",
      "Gradient Descent(352/999): loss=0.35335804794291004\n",
      "Gradient Descent(353/999): loss=0.35332532128650357\n",
      "Gradient Descent(354/999): loss=0.35329270568214116\n",
      "Gradient Descent(355/999): loss=0.3532602003714881\n",
      "Gradient Descent(356/999): loss=0.35322780460629866\n",
      "Gradient Descent(357/999): loss=0.35319551764823864\n",
      "Gradient Descent(358/999): loss=0.3531633387687132\n",
      "Gradient Descent(359/999): loss=0.3531312672486956\n",
      "Gradient Descent(360/999): loss=0.3530993023785613\n",
      "Gradient Descent(361/999): loss=0.3530674434579245\n",
      "Gradient Descent(362/999): loss=0.3530356897954769\n",
      "Gradient Descent(363/999): loss=0.35300404070883123\n",
      "Gradient Descent(364/999): loss=0.352972495524366\n",
      "Gradient Descent(365/999): loss=0.35294105357707517\n",
      "Gradient Descent(366/999): loss=0.3529097142104183\n",
      "Gradient Descent(367/999): loss=0.3528784767761758\n",
      "Gradient Descent(368/999): loss=0.35284734063430556\n",
      "Gradient Descent(369/999): loss=0.3528163051528026\n",
      "Gradient Descent(370/999): loss=0.3527853697075618\n",
      "Gradient Descent(371/999): loss=0.35275453368224263\n",
      "Gradient Descent(372/999): loss=0.3527237964681363\n",
      "Gradient Descent(373/999): loss=0.35269315746403723\n",
      "Gradient Descent(374/999): loss=0.35266261607611377\n",
      "Gradient Descent(375/999): loss=0.3526321717177843\n",
      "Gradient Descent(376/999): loss=0.3526018238095945\n",
      "Gradient Descent(377/999): loss=0.3525715717790957\n",
      "Gradient Descent(378/999): loss=0.3525414150607282\n",
      "Gradient Descent(379/999): loss=0.3525113530957052\n",
      "Gradient Descent(380/999): loss=0.3524813853318976\n",
      "Gradient Descent(381/999): loss=0.3524515112237242\n",
      "Gradient Descent(382/999): loss=0.35242173023204093\n",
      "Gradient Descent(383/999): loss=0.35239204182403383\n",
      "Gradient Descent(384/999): loss=0.3523624454731139\n",
      "Gradient Descent(385/999): loss=0.35233294065881293\n",
      "Gradient Descent(386/999): loss=0.352303526866682\n",
      "Gradient Descent(387/999): loss=0.3522742035881928\n",
      "Gradient Descent(388/999): loss=0.35224497032063845\n",
      "Gradient Descent(389/999): loss=0.3522158265670387\n",
      "Gradient Descent(390/999): loss=0.35218677183604463\n",
      "Gradient Descent(391/999): loss=0.3521578056418474\n",
      "Gradient Descent(392/999): loss=0.3521289275040861\n",
      "Gradient Descent(393/999): loss=0.3521001369477601\n",
      "Gradient Descent(394/999): loss=0.3520714335031412\n",
      "Gradient Descent(395/999): loss=0.35204281670568666\n",
      "Gradient Descent(396/999): loss=0.3520142860959571\n",
      "Gradient Descent(397/999): loss=0.35198584121953214\n",
      "Gradient Descent(398/999): loss=0.35195748162693036\n",
      "Gradient Descent(399/999): loss=0.35192920687352874\n",
      "Gradient Descent(400/999): loss=0.3519010165194856\n",
      "Gradient Descent(401/999): loss=0.35187291012966265\n",
      "Gradient Descent(402/999): loss=0.3518448872735513\n",
      "Gradient Descent(403/999): loss=0.3518169475251968\n",
      "Gradient Descent(404/999): loss=0.3517890904631274\n",
      "Gradient Descent(405/999): loss=0.3517613156702817\n",
      "Gradient Descent(406/999): loss=0.35173362273394027\n",
      "Gradient Descent(407/999): loss=0.3517060112456552\n",
      "Gradient Descent(408/999): loss=0.3516784808011846\n",
      "Gradient Descent(409/999): loss=0.35165103100042494\n",
      "Gradient Descent(410/999): loss=0.3516236614473468\n",
      "Gradient Descent(411/999): loss=0.35159637174993086\n",
      "Gradient Descent(412/999): loss=0.3515691615201055\n",
      "Gradient Descent(413/999): loss=0.35154203037368537\n",
      "Gradient Descent(414/999): loss=0.3515149779303101\n",
      "Gradient Descent(415/999): loss=0.35148800381338724\n",
      "Gradient Descent(416/999): loss=0.3514611076500311\n",
      "Gradient Descent(417/999): loss=0.35143428907100843\n",
      "Gradient Descent(418/999): loss=0.35140754771068017\n",
      "Gradient Descent(419/999): loss=0.35138088320694755\n",
      "Gradient Descent(420/999): loss=0.35135429520119793\n",
      "Gradient Descent(421/999): loss=0.35132778333825126\n",
      "Gradient Descent(422/999): loss=0.351301347266308\n",
      "Gradient Descent(423/999): loss=0.35127498663689793\n",
      "Gradient Descent(424/999): loss=0.35124870110483036\n",
      "Gradient Descent(425/999): loss=0.3512224903281439\n",
      "Gradient Descent(426/999): loss=0.3511963539680584\n",
      "Gradient Descent(427/999): loss=0.3511702916889271\n",
      "Gradient Descent(428/999): loss=0.3511443031581904\n",
      "Gradient Descent(429/999): loss=0.3511183880463287\n",
      "Gradient Descent(430/999): loss=0.351092546026819\n",
      "Gradient Descent(431/999): loss=0.3510667767760885\n",
      "Gradient Descent(432/999): loss=0.35104107997347306\n",
      "Gradient Descent(433/999): loss=0.3510154553011727\n",
      "Gradient Descent(434/999): loss=0.3509899024442112\n",
      "Gradient Descent(435/999): loss=0.35096442109039294\n",
      "Gradient Descent(436/999): loss=0.3509390109302644\n",
      "Gradient Descent(437/999): loss=0.3509136716570729\n",
      "Gradient Descent(438/999): loss=0.35088840296672785\n",
      "Gradient Descent(439/999): loss=0.3508632045577625\n",
      "Gradient Descent(440/999): loss=0.3508380761312959\n",
      "Gradient Descent(441/999): loss=0.35081301739099635\n",
      "Gradient Descent(442/999): loss=0.3507880280430443\n",
      "Gradient Descent(443/999): loss=0.35076310779609676\n",
      "Gradient Descent(444/999): loss=0.35073825636125233\n",
      "Gradient Descent(445/999): loss=0.35071347345201676\n",
      "Gradient Descent(446/999): loss=0.3506887587842679\n",
      "Gradient Descent(447/999): loss=0.3506641120762241\n",
      "Gradient Descent(448/999): loss=0.3506395330484102\n",
      "Gradient Descent(449/999): loss=0.3506150214236251\n",
      "Gradient Descent(450/999): loss=0.3505905769269115\n",
      "Gradient Descent(451/999): loss=0.35056619928552296\n",
      "Gradient Descent(452/999): loss=0.35054188822889515\n",
      "Gradient Descent(453/999): loss=0.3505176434886143\n",
      "Gradient Descent(454/999): loss=0.35049346479838855\n",
      "Gradient Descent(455/999): loss=0.35046935189401873\n",
      "Gradient Descent(456/999): loss=0.3504453045133699\n",
      "Gradient Descent(457/999): loss=0.3504213223963432\n",
      "Gradient Descent(458/999): loss=0.35039740528484825\n",
      "Gradient Descent(459/999): loss=0.35037355292277633\n",
      "Gradient Descent(460/999): loss=0.35034976505597315\n",
      "Gradient Descent(461/999): loss=0.35032604143221413\n",
      "Gradient Descent(462/999): loss=0.3503023818011773\n",
      "Gradient Descent(463/999): loss=0.3502787859144179\n",
      "Gradient Descent(464/999): loss=0.35025525352534487\n",
      "Gradient Descent(465/999): loss=0.3502317843891957\n",
      "Gradient Descent(466/999): loss=0.3502083782630117\n",
      "Gradient Descent(467/999): loss=0.3501850349056159\n",
      "Gradient Descent(468/999): loss=0.35016175407758837\n",
      "Gradient Descent(469/999): loss=0.3501385355412451\n",
      "Gradient Descent(470/999): loss=0.3501153790606133\n",
      "Gradient Descent(471/999): loss=0.35009228440141155\n",
      "Gradient Descent(472/999): loss=0.3500692513310271\n",
      "Gradient Descent(473/999): loss=0.35004627961849444\n",
      "Gradient Descent(474/999): loss=0.3500233690344746\n",
      "Gradient Descent(475/999): loss=0.3500005193512345\n",
      "Gradient Descent(476/999): loss=0.3499777303426269\n",
      "Gradient Descent(477/999): loss=0.34995500178407013\n",
      "Gradient Descent(478/999): loss=0.3499323334525286\n",
      "Gradient Descent(479/999): loss=0.3499097251264935\n",
      "Gradient Descent(480/999): loss=0.34988717658596435\n",
      "Gradient Descent(481/999): loss=0.349864687612429\n",
      "Gradient Descent(482/999): loss=0.34984225798884744\n",
      "Gradient Descent(483/999): loss=0.34981988749963155\n",
      "Gradient Descent(484/999): loss=0.34979757593062877\n",
      "Gradient Descent(485/999): loss=0.3497753230691042\n",
      "Gradient Descent(486/999): loss=0.34975312870372344\n",
      "Gradient Descent(487/999): loss=0.34973099262453583\n",
      "Gradient Descent(488/999): loss=0.34970891462295767\n",
      "Gradient Descent(489/999): loss=0.349686894491756\n",
      "Gradient Descent(490/999): loss=0.34966493202503274\n",
      "Gradient Descent(491/999): loss=0.34964302701820843\n",
      "Gradient Descent(492/999): loss=0.3496211792680067\n",
      "Gradient Descent(493/999): loss=0.34959938857243983\n",
      "Gradient Descent(494/999): loss=0.34957765473079233\n",
      "Gradient Descent(495/999): loss=0.3495559775436071\n",
      "Gradient Descent(496/999): loss=0.34953435681267087\n",
      "Gradient Descent(497/999): loss=0.34951279234099925\n",
      "Gradient Descent(498/999): loss=0.34949128393282286\n",
      "Gradient Descent(499/999): loss=0.34946983139357374\n",
      "Gradient Descent(500/999): loss=0.3494484345298713\n",
      "Gradient Descent(501/999): loss=0.34942709314950904\n",
      "Gradient Descent(502/999): loss=0.34940580706144153\n",
      "Gradient Descent(503/999): loss=0.349384576075771\n",
      "Gradient Descent(504/999): loss=0.3493634000037345\n",
      "Gradient Descent(505/999): loss=0.34934227865769135\n",
      "Gradient Descent(506/999): loss=0.3493212118511114\n",
      "Gradient Descent(507/999): loss=0.34930019939856155\n",
      "Gradient Descent(508/999): loss=0.34927924111569464\n",
      "Gradient Descent(509/999): loss=0.3492583368192376\n",
      "Gradient Descent(510/999): loss=0.3492374863269786\n",
      "Gradient Descent(511/999): loss=0.3492166894577575\n",
      "Gradient Descent(512/999): loss=0.3491959460314529\n",
      "Gradient Descent(513/999): loss=0.3491752558689716\n",
      "Gradient Descent(514/999): loss=0.34915461879223814\n",
      "Gradient Descent(515/999): loss=0.349134034624183\n",
      "Gradient Descent(516/999): loss=0.3491135031887326\n",
      "Gradient Descent(517/999): loss=0.34909302431079886\n",
      "Gradient Descent(518/999): loss=0.3490725978162685\n",
      "Gradient Descent(519/999): loss=0.34905222353199333\n",
      "Gradient Descent(520/999): loss=0.34903190128578016\n",
      "Gradient Descent(521/999): loss=0.34901163090638054\n",
      "Gradient Descent(522/999): loss=0.3489914122234815\n",
      "Gradient Descent(523/999): loss=0.3489712450676962\n",
      "Gradient Descent(524/999): loss=0.34895112927055344\n",
      "Gradient Descent(525/999): loss=0.3489310646644899\n",
      "Gradient Descent(526/999): loss=0.34891105108283993\n",
      "Gradient Descent(527/999): loss=0.348891088359827\n",
      "Gradient Descent(528/999): loss=0.34887117633055453\n",
      "Gradient Descent(529/999): loss=0.3488513148309977\n",
      "Gradient Descent(530/999): loss=0.34883150369799415\n",
      "Gradient Descent(531/999): loss=0.3488117427692362\n",
      "Gradient Descent(532/999): loss=0.3487920318832618\n",
      "Gradient Descent(533/999): loss=0.34877237087944724\n",
      "Gradient Descent(534/999): loss=0.34875275959799795\n",
      "Gradient Descent(535/999): loss=0.34873319787994145\n",
      "Gradient Descent(536/999): loss=0.3487136855671187\n",
      "Gradient Descent(537/999): loss=0.34869422250217713\n",
      "Gradient Descent(538/999): loss=0.3486748085285624\n",
      "Gradient Descent(539/999): loss=0.34865544349051114\n",
      "Gradient Descent(540/999): loss=0.3486361272330433\n",
      "Gradient Descent(541/999): loss=0.34861685960195504\n",
      "Gradient Descent(542/999): loss=0.3485976404438117\n",
      "Gradient Descent(543/999): loss=0.3485784696059403\n",
      "Gradient Descent(544/999): loss=0.3485593469364223\n",
      "Gradient Descent(545/999): loss=0.3485402722840878\n",
      "Gradient Descent(546/999): loss=0.3485212454985074\n",
      "Gradient Descent(547/999): loss=0.3485022664299864\n",
      "Gradient Descent(548/999): loss=0.34848333492955763\n",
      "Gradient Descent(549/999): loss=0.3484644508489754\n",
      "Gradient Descent(550/999): loss=0.34844561404070856\n",
      "Gradient Descent(551/999): loss=0.3484268243579347\n",
      "Gradient Descent(552/999): loss=0.34840808165453346\n",
      "Gradient Descent(553/999): loss=0.34838938578507994\n",
      "Gradient Descent(554/999): loss=0.34837073660484036\n",
      "Gradient Descent(555/999): loss=0.3483521339697636\n",
      "Gradient Descent(556/999): loss=0.34833357773647744\n",
      "Gradient Descent(557/999): loss=0.3483150677622813\n",
      "Gradient Descent(558/999): loss=0.34829660390514117\n",
      "Gradient Descent(559/999): loss=0.34827818602368366\n",
      "Gradient Descent(560/999): loss=0.3482598139771908\n",
      "Gradient Descent(561/999): loss=0.3482414876255934\n",
      "Gradient Descent(562/999): loss=0.34822320682946717\n",
      "Gradient Descent(563/999): loss=0.34820497145002643\n",
      "Gradient Descent(564/999): loss=0.3481867813491184\n",
      "Gradient Descent(565/999): loss=0.3481686363892189\n",
      "Gradient Descent(566/999): loss=0.34815053643342664\n",
      "Gradient Descent(567/999): loss=0.3481324813454584\n",
      "Gradient Descent(568/999): loss=0.34811447098964327\n",
      "Gradient Descent(569/999): loss=0.3480965052309186\n",
      "Gradient Descent(570/999): loss=0.34807858393482505\n",
      "Gradient Descent(571/999): loss=0.34806070696750063\n",
      "Gradient Descent(572/999): loss=0.34804287419567737\n",
      "Gradient Descent(573/999): loss=0.3480250854866755\n",
      "Gradient Descent(574/999): loss=0.3480073407083996\n",
      "Gradient Descent(575/999): loss=0.34798963972933317\n",
      "Gradient Descent(576/999): loss=0.3479719824185353\n",
      "Gradient Descent(577/999): loss=0.34795436864563467\n",
      "Gradient Descent(578/999): loss=0.347936798280826\n",
      "Gradient Descent(579/999): loss=0.34791927119486626\n",
      "Gradient Descent(580/999): loss=0.347901787259069\n",
      "Gradient Descent(581/999): loss=0.3478843463453012\n",
      "Gradient Descent(582/999): loss=0.3478669483259781\n",
      "Gradient Descent(583/999): loss=0.34784959307406027\n",
      "Gradient Descent(584/999): loss=0.3478322804630484\n",
      "Gradient Descent(585/999): loss=0.34781501036697987\n",
      "Gradient Descent(586/999): loss=0.34779778266042444\n",
      "Gradient Descent(587/999): loss=0.34778059721848037\n",
      "Gradient Descent(588/999): loss=0.34776345391677105\n",
      "Gradient Descent(589/999): loss=0.34774635263144016\n",
      "Gradient Descent(590/999): loss=0.3477292932391482\n",
      "Gradient Descent(591/999): loss=0.34771227561706985\n",
      "Gradient Descent(592/999): loss=0.34769529964288837\n",
      "Gradient Descent(593/999): loss=0.34767836519479334\n",
      "Gradient Descent(594/999): loss=0.3476614721514764\n",
      "Gradient Descent(595/999): loss=0.347644620392128\n",
      "Gradient Descent(596/999): loss=0.34762780979643354\n",
      "Gradient Descent(597/999): loss=0.3476110402445699\n",
      "Gradient Descent(598/999): loss=0.3475943116172025\n",
      "Gradient Descent(599/999): loss=0.3475776237954812\n",
      "Gradient Descent(600/999): loss=0.3475609766610371\n",
      "Gradient Descent(601/999): loss=0.34754437009597977\n",
      "Gradient Descent(602/999): loss=0.34752780398289335\n",
      "Gradient Descent(603/999): loss=0.3475112782048331\n",
      "Gradient Descent(604/999): loss=0.34749479264532274\n",
      "Gradient Descent(605/999): loss=0.3474783471883512\n",
      "Gradient Descent(606/999): loss=0.347461941718369\n",
      "Gradient Descent(607/999): loss=0.3474455761202861\n",
      "Gradient Descent(608/999): loss=0.3474292502794672\n",
      "Gradient Descent(609/999): loss=0.3474129640817305\n",
      "Gradient Descent(610/999): loss=0.3473967174133436\n",
      "Gradient Descent(611/999): loss=0.34738051016102045\n",
      "Gradient Descent(612/999): loss=0.3473643422119193\n",
      "Gradient Descent(613/999): loss=0.34734821345363875\n",
      "Gradient Descent(614/999): loss=0.3473321237742162\n",
      "Gradient Descent(615/999): loss=0.34731607306212287\n",
      "Gradient Descent(616/999): loss=0.34730006120626333\n",
      "Gradient Descent(617/999): loss=0.34728408809597144\n",
      "Gradient Descent(618/999): loss=0.3472681536210075\n",
      "Gradient Descent(619/999): loss=0.3472522576715562\n",
      "Gradient Descent(620/999): loss=0.3472364001382236\n",
      "Gradient Descent(621/999): loss=0.34722058091203417\n",
      "Gradient Descent(622/999): loss=0.3472047998844285\n",
      "Gradient Descent(623/999): loss=0.3471890569472611\n",
      "Gradient Descent(624/999): loss=0.34717335199279664\n",
      "Gradient Descent(625/999): loss=0.3471576849137088\n",
      "Gradient Descent(626/999): loss=0.34714205560307604\n",
      "Gradient Descent(627/999): loss=0.34712646395438146\n",
      "Gradient Descent(628/999): loss=0.34711090986150817\n",
      "Gradient Descent(629/999): loss=0.3470953932187375\n",
      "Gradient Descent(630/999): loss=0.34707991392074705\n",
      "Gradient Descent(631/999): loss=0.3470644718626082\n",
      "Gradient Descent(632/999): loss=0.347049066939783\n",
      "Gradient Descent(633/999): loss=0.34703369904812276\n",
      "Gradient Descent(634/999): loss=0.34701836808386494\n",
      "Gradient Descent(635/999): loss=0.34700307394363156\n",
      "Gradient Descent(636/999): loss=0.34698781652442623\n",
      "Gradient Descent(637/999): loss=0.34697259572363265\n",
      "Gradient Descent(638/999): loss=0.34695741143901143\n",
      "Gradient Descent(639/999): loss=0.3469422635686991\n",
      "Gradient Descent(640/999): loss=0.34692715201120466\n",
      "Gradient Descent(641/999): loss=0.34691207666540824\n",
      "Gradient Descent(642/999): loss=0.34689703743055883\n",
      "Gradient Descent(643/999): loss=0.3468820342062716\n",
      "Gradient Descent(644/999): loss=0.3468670668925267\n",
      "Gradient Descent(645/999): loss=0.34685213538966647\n",
      "Gradient Descent(646/999): loss=0.346837239598394\n",
      "Gradient Descent(647/999): loss=0.3468223794197702\n",
      "Gradient Descent(648/999): loss=0.3468075547552123\n",
      "Gradient Descent(649/999): loss=0.3467927655064922\n",
      "Gradient Descent(650/999): loss=0.34677801157573396\n",
      "Gradient Descent(651/999): loss=0.34676329286541197\n",
      "Gradient Descent(652/999): loss=0.3467486092783492\n",
      "Gradient Descent(653/999): loss=0.34673396071771484\n",
      "Gradient Descent(654/999): loss=0.34671934708702273\n",
      "Gradient Descent(655/999): loss=0.3467047682901295\n",
      "Gradient Descent(656/999): loss=0.34669022423123247\n",
      "Gradient Descent(657/999): loss=0.34667571481486786\n",
      "Gradient Descent(658/999): loss=0.3466612399459093\n",
      "Gradient Descent(659/999): loss=0.3466467995295653\n",
      "Gradient Descent(660/999): loss=0.34663239347137814\n",
      "Gradient Descent(661/999): loss=0.34661802167722155\n",
      "Gradient Descent(662/999): loss=0.34660368405329944\n",
      "Gradient Descent(663/999): loss=0.3465893805061437\n",
      "Gradient Descent(664/999): loss=0.34657511094261245\n",
      "Gradient Descent(665/999): loss=0.3465608752698888\n",
      "Gradient Descent(666/999): loss=0.3465466733954789\n",
      "Gradient Descent(667/999): loss=0.34653250522720974\n",
      "Gradient Descent(668/999): loss=0.3465183706732284\n",
      "Gradient Descent(669/999): loss=0.3465042696419995\n",
      "Gradient Descent(670/999): loss=0.3464902020423041\n",
      "Gradient Descent(671/999): loss=0.3464761677832377\n",
      "Gradient Descent(672/999): loss=0.3464621667742094\n",
      "Gradient Descent(673/999): loss=0.34644819892493867\n",
      "Gradient Descent(674/999): loss=0.3464342641454559\n",
      "Gradient Descent(675/999): loss=0.34642036234609885\n",
      "Gradient Descent(676/999): loss=0.3464064934375125\n",
      "Gradient Descent(677/999): loss=0.3463926573306465\n",
      "Gradient Descent(678/999): loss=0.34637885393675427\n",
      "Gradient Descent(679/999): loss=0.3463650831673914\n",
      "Gradient Descent(680/999): loss=0.34635134493441366\n",
      "Gradient Descent(681/999): loss=0.34633763914997606\n",
      "Gradient Descent(682/999): loss=0.3463239657265309\n",
      "Gradient Descent(683/999): loss=0.34631032457682664\n",
      "Gradient Descent(684/999): loss=0.34629671561390657\n",
      "Gradient Descent(685/999): loss=0.3462831387511065\n",
      "Gradient Descent(686/999): loss=0.34626959390205453\n",
      "Gradient Descent(687/999): loss=0.34625608098066823\n",
      "Gradient Descent(688/999): loss=0.34624259990115475\n",
      "Gradient Descent(689/999): loss=0.34622915057800857\n",
      "Gradient Descent(690/999): loss=0.34621573292600943\n",
      "Gradient Descent(691/999): loss=0.3462023468602226\n",
      "Gradient Descent(692/999): loss=0.3461889922959961\n",
      "Gradient Descent(693/999): loss=0.3461756691489597\n",
      "Gradient Descent(694/999): loss=0.3461623773350242\n",
      "Gradient Descent(695/999): loss=0.34614911677037913\n",
      "Gradient Descent(696/999): loss=0.3461358873714921\n",
      "Gradient Descent(697/999): loss=0.3461226890551072\n",
      "Gradient Descent(698/999): loss=0.3461095217382439\n",
      "Gradient Descent(699/999): loss=0.346096385338195\n",
      "Gradient Descent(700/999): loss=0.34608327977252684\n",
      "Gradient Descent(701/999): loss=0.34607020495907614\n",
      "Gradient Descent(702/999): loss=0.34605716081595056\n",
      "Gradient Descent(703/999): loss=0.34604414726152566\n",
      "Gradient Descent(704/999): loss=0.3460311642144455\n",
      "Gradient Descent(705/999): loss=0.3460182115936198\n",
      "Gradient Descent(706/999): loss=0.3460052893182238\n",
      "Gradient Descent(707/999): loss=0.3459923973076959\n",
      "Gradient Descent(708/999): loss=0.34597953548173765\n",
      "Gradient Descent(709/999): loss=0.34596670376031236\n",
      "Gradient Descent(710/999): loss=0.345953902063643\n",
      "Gradient Descent(711/999): loss=0.3459411303122115\n",
      "Gradient Descent(712/999): loss=0.34592838842675816\n",
      "Gradient Descent(713/999): loss=0.3459156763282796\n",
      "Gradient Descent(714/999): loss=0.3459029939380281\n",
      "Gradient Descent(715/999): loss=0.3458903411775103\n",
      "Gradient Descent(716/999): loss=0.3458777179684861\n",
      "Gradient Descent(717/999): loss=0.34586512423296717\n",
      "Gradient Descent(718/999): loss=0.34585255989321684\n",
      "Gradient Descent(719/999): loss=0.3458400248717475\n",
      "Gradient Descent(720/999): loss=0.3458275190913206\n",
      "Gradient Descent(721/999): loss=0.34581504247494543\n",
      "Gradient Descent(722/999): loss=0.3458025949458773\n",
      "Gradient Descent(723/999): loss=0.34579017642761745\n",
      "Gradient Descent(724/999): loss=0.3457777868439109\n",
      "Gradient Descent(725/999): loss=0.34576542611874644\n",
      "Gradient Descent(726/999): loss=0.34575309417635486\n",
      "Gradient Descent(727/999): loss=0.3457407909412081\n",
      "Gradient Descent(728/999): loss=0.34572851633801815\n",
      "Gradient Descent(729/999): loss=0.34571627029173596\n",
      "Gradient Descent(730/999): loss=0.3457040527275507\n",
      "Gradient Descent(731/999): loss=0.3456918635708883\n",
      "Gradient Descent(732/999): loss=0.3456797027474106\n",
      "Gradient Descent(733/999): loss=0.3456675701830147\n",
      "Gradient Descent(734/999): loss=0.34565546580383094\n",
      "Gradient Descent(735/999): loss=0.34564338953622353\n",
      "Gradient Descent(736/999): loss=0.3456313413067877\n",
      "Gradient Descent(737/999): loss=0.34561932104234994\n",
      "Gradient Descent(738/999): loss=0.3456073286699668\n",
      "Gradient Descent(739/999): loss=0.34559536411692343\n",
      "Gradient Descent(740/999): loss=0.3455834273107336\n",
      "Gradient Descent(741/999): loss=0.3455715181791371\n",
      "Gradient Descent(742/999): loss=0.34555963665010075\n",
      "Gradient Descent(743/999): loss=0.34554778265181607\n",
      "Gradient Descent(744/999): loss=0.3455359561126982\n",
      "Gradient Descent(745/999): loss=0.3455241569613867\n",
      "Gradient Descent(746/999): loss=0.34551238512674215\n",
      "Gradient Descent(747/999): loss=0.3455006405378476\n",
      "Gradient Descent(748/999): loss=0.34548892312400553\n",
      "Gradient Descent(749/999): loss=0.3454772328147384\n",
      "Gradient Descent(750/999): loss=0.34546556953978746\n",
      "Gradient Descent(751/999): loss=0.3454539332291114\n",
      "Gradient Descent(752/999): loss=0.34544232381288553\n",
      "Gradient Descent(753/999): loss=0.3454307412215015\n",
      "Gradient Descent(754/999): loss=0.3454191853855659\n",
      "Gradient Descent(755/999): loss=0.34540765623589875\n",
      "Gradient Descent(756/999): loss=0.34539615370353416\n",
      "Gradient Descent(757/999): loss=0.34538467771971826\n",
      "Gradient Descent(758/999): loss=0.34537322821590893\n",
      "Gradient Descent(759/999): loss=0.34536180512377423\n",
      "Gradient Descent(760/999): loss=0.3453504083751924\n",
      "Gradient Descent(761/999): loss=0.3453390379022508\n",
      "Gradient Descent(762/999): loss=0.34532769363724397\n",
      "Gradient Descent(763/999): loss=0.3453163755126747\n",
      "Gradient Descent(764/999): loss=0.34530508346125216\n",
      "Gradient Descent(765/999): loss=0.34529381741589016\n",
      "Gradient Descent(766/999): loss=0.34528257730970846\n",
      "Gradient Descent(767/999): loss=0.3452713630760298\n",
      "Gradient Descent(768/999): loss=0.3452601746483806\n",
      "Gradient Descent(769/999): loss=0.34524901196048957\n",
      "Gradient Descent(770/999): loss=0.3452378749462868\n",
      "Gradient Descent(771/999): loss=0.3452267635399029\n",
      "Gradient Descent(772/999): loss=0.3452156776756685\n",
      "Gradient Descent(773/999): loss=0.34520461728811397\n",
      "Gradient Descent(774/999): loss=0.3451935823119672\n",
      "Gradient Descent(775/999): loss=0.34518257268215347\n",
      "Gradient Descent(776/999): loss=0.34517158833379613\n",
      "Gradient Descent(777/999): loss=0.3451606292022132\n",
      "Gradient Descent(778/999): loss=0.3451496952229184\n",
      "Gradient Descent(779/999): loss=0.34513878633162\n",
      "Gradient Descent(780/999): loss=0.3451279024642201\n",
      "Gradient Descent(781/999): loss=0.3451170435568132\n",
      "Gradient Descent(782/999): loss=0.3451062095456864\n",
      "Gradient Descent(783/999): loss=0.3450954003673186\n",
      "Gradient Descent(784/999): loss=0.3450846159583785\n",
      "Gradient Descent(785/999): loss=0.3450738562557255\n",
      "Gradient Descent(786/999): loss=0.3450631211964082\n",
      "Gradient Descent(787/999): loss=0.3450524107176631\n",
      "Gradient Descent(788/999): loss=0.3450417247569152\n",
      "Gradient Descent(789/999): loss=0.3450310632517759\n",
      "Gradient Descent(790/999): loss=0.3450204261400436\n",
      "Gradient Descent(791/999): loss=0.3450098133597018\n",
      "Gradient Descent(792/999): loss=0.3449992248489192\n",
      "Gradient Descent(793/999): loss=0.3449886605460483\n",
      "Gradient Descent(794/999): loss=0.34497812038962583\n",
      "Gradient Descent(795/999): loss=0.3449676043183706\n",
      "Gradient Descent(796/999): loss=0.3449571122711838\n",
      "Gradient Descent(797/999): loss=0.34494664418714815\n",
      "Gradient Descent(798/999): loss=0.3449362000055268\n",
      "Gradient Descent(799/999): loss=0.3449257796657633\n",
      "Gradient Descent(800/999): loss=0.34491538310748027\n",
      "Gradient Descent(801/999): loss=0.344905010270479\n",
      "Gradient Descent(802/999): loss=0.34489466109473915\n",
      "Gradient Descent(803/999): loss=0.3448843355204174\n",
      "Gradient Descent(804/999): loss=0.34487403348784706\n",
      "Gradient Descent(805/999): loss=0.3448637549375378\n",
      "Gradient Descent(806/999): loss=0.3448534998101741\n",
      "Gradient Descent(807/999): loss=0.3448432680466158\n",
      "Gradient Descent(808/999): loss=0.3448330595878963\n",
      "Gradient Descent(809/999): loss=0.3448228743752225\n",
      "Gradient Descent(810/999): loss=0.34481271234997424\n",
      "Gradient Descent(811/999): loss=0.3448025734537031\n",
      "Gradient Descent(812/999): loss=0.3447924576281325\n",
      "Gradient Descent(813/999): loss=0.3447823648151566\n",
      "Gradient Descent(814/999): loss=0.3447722949568397\n",
      "Gradient Descent(815/999): loss=0.34476224799541566\n",
      "Gradient Descent(816/999): loss=0.34475222387328724\n",
      "Gradient Descent(817/999): loss=0.3447422225330258\n",
      "Gradient Descent(818/999): loss=0.34473224391737034\n",
      "Gradient Descent(819/999): loss=0.3447222879692268\n",
      "Gradient Descent(820/999): loss=0.3447123546316674\n",
      "Gradient Descent(821/999): loss=0.3447024438479308\n",
      "Gradient Descent(822/999): loss=0.3446925555614205\n",
      "Gradient Descent(823/999): loss=0.34468268971570487\n",
      "Gradient Descent(824/999): loss=0.3446728462545162\n",
      "Gradient Descent(825/999): loss=0.3446630251217505\n",
      "Gradient Descent(826/999): loss=0.3446532262614663\n",
      "Gradient Descent(827/999): loss=0.3446434496178845\n",
      "Gradient Descent(828/999): loss=0.34463369513538805\n",
      "Gradient Descent(829/999): loss=0.34462396275852075\n",
      "Gradient Descent(830/999): loss=0.34461425243198684\n",
      "Gradient Descent(831/999): loss=0.34460456410065055\n",
      "Gradient Descent(832/999): loss=0.34459489770953555\n",
      "Gradient Descent(833/999): loss=0.3445852532038244\n",
      "Gradient Descent(834/999): loss=0.3445756305288577\n",
      "Gradient Descent(835/999): loss=0.34456602963013383\n",
      "Gradient Descent(836/999): loss=0.34455645045330835\n",
      "Gradient Descent(837/999): loss=0.34454689294419316\n",
      "Gradient Descent(838/999): loss=0.3445373570487561\n",
      "Gradient Descent(839/999): loss=0.3445278427131207\n",
      "Gradient Descent(840/999): loss=0.3445183498835651\n",
      "Gradient Descent(841/999): loss=0.34450887850652206\n",
      "Gradient Descent(842/999): loss=0.3444994285285777\n",
      "Gradient Descent(843/999): loss=0.34448999989647133\n",
      "Gradient Descent(844/999): loss=0.34448059255709557\n",
      "Gradient Descent(845/999): loss=0.3444712064574947\n",
      "Gradient Descent(846/999): loss=0.34446184154486464\n",
      "Gradient Descent(847/999): loss=0.3444524977665521\n",
      "Gradient Descent(848/999): loss=0.34444317507005495\n",
      "Gradient Descent(849/999): loss=0.3444338734030207\n",
      "Gradient Descent(850/999): loss=0.34442459271324594\n",
      "Gradient Descent(851/999): loss=0.34441533294867693\n",
      "Gradient Descent(852/999): loss=0.3444060940574083\n",
      "Gradient Descent(853/999): loss=0.34439687598768176\n",
      "Gradient Descent(854/999): loss=0.34438767868788706\n",
      "Gradient Descent(855/999): loss=0.34437850210656085\n",
      "Gradient Descent(856/999): loss=0.3443693461923859\n",
      "Gradient Descent(857/999): loss=0.34436021089419083\n",
      "Gradient Descent(858/999): loss=0.34435109616094983\n",
      "Gradient Descent(859/999): loss=0.3443420019417817\n",
      "Gradient Descent(860/999): loss=0.3443329281859495\n",
      "Gradient Descent(861/999): loss=0.34432387484286037\n",
      "Gradient Descent(862/999): loss=0.3443148418620645\n",
      "Gradient Descent(863/999): loss=0.34430582919325503\n",
      "Gradient Descent(864/999): loss=0.3442968367862675\n",
      "Gradient Descent(865/999): loss=0.3442878645910793\n",
      "Gradient Descent(866/999): loss=0.34427891255780885\n",
      "Gradient Descent(867/999): loss=0.34426998063671593\n",
      "Gradient Descent(868/999): loss=0.3442610687782002\n",
      "Gradient Descent(869/999): loss=0.34425217693280175\n",
      "Gradient Descent(870/999): loss=0.34424330505119954\n",
      "Gradient Descent(871/999): loss=0.3442344530842119\n",
      "Gradient Descent(872/999): loss=0.3442256209827953\n",
      "Gradient Descent(873/999): loss=0.3442168086980443\n",
      "Gradient Descent(874/999): loss=0.3442080161811911\n",
      "Gradient Descent(875/999): loss=0.3441992433836047\n",
      "Gradient Descent(876/999): loss=0.344190490256791\n",
      "Gradient Descent(877/999): loss=0.34418175675239177\n",
      "Gradient Descent(878/999): loss=0.3441730428221842\n",
      "Gradient Descent(879/999): loss=0.34416434841808136\n",
      "Gradient Descent(880/999): loss=0.3441556734921303\n",
      "Gradient Descent(881/999): loss=0.3441470179965127\n",
      "Gradient Descent(882/999): loss=0.3441383818835442\n",
      "Gradient Descent(883/999): loss=0.3441297651056734\n",
      "Gradient Descent(884/999): loss=0.34412116761548195\n",
      "Gradient Descent(885/999): loss=0.344112589365684\n",
      "Gradient Descent(886/999): loss=0.34410403030912595\n",
      "Gradient Descent(887/999): loss=0.3440954903987854\n",
      "Gradient Descent(888/999): loss=0.3440869695877712\n",
      "Gradient Descent(889/999): loss=0.34407846782932267\n",
      "Gradient Descent(890/999): loss=0.3440699850768098\n",
      "Gradient Descent(891/999): loss=0.34406152128373196\n",
      "Gradient Descent(892/999): loss=0.3440530764037182\n",
      "Gradient Descent(893/999): loss=0.34404465039052645\n",
      "Gradient Descent(894/999): loss=0.3440362431980427\n",
      "Gradient Descent(895/999): loss=0.34402785478028186\n",
      "Gradient Descent(896/999): loss=0.3440194850913855\n",
      "Gradient Descent(897/999): loss=0.3440111340856233\n",
      "Gradient Descent(898/999): loss=0.344002801717391\n",
      "Gradient Descent(899/999): loss=0.34399448794121107\n",
      "Gradient Descent(900/999): loss=0.34398619271173214\n",
      "Gradient Descent(901/999): loss=0.34397791598372773\n",
      "Gradient Descent(902/999): loss=0.3439696577120969\n",
      "Gradient Descent(903/999): loss=0.3439614178518636\n",
      "Gradient Descent(904/999): loss=0.34395319635817573\n",
      "Gradient Descent(905/999): loss=0.34394499318630484\n",
      "Gradient Descent(906/999): loss=0.34393680829164647\n",
      "Gradient Descent(907/999): loss=0.343928641629719\n",
      "Gradient Descent(908/999): loss=0.3439204931561635\n",
      "Gradient Descent(909/999): loss=0.3439123628267429\n",
      "Gradient Descent(910/999): loss=0.3439042505973425\n",
      "Gradient Descent(911/999): loss=0.34389615642396865\n",
      "Gradient Descent(912/999): loss=0.34388808026274903\n",
      "Gradient Descent(913/999): loss=0.3438800220699318\n",
      "Gradient Descent(914/999): loss=0.34387198180188505\n",
      "Gradient Descent(915/999): loss=0.3438639594150976\n",
      "Gradient Descent(916/999): loss=0.3438559548661767\n",
      "Gradient Descent(917/999): loss=0.34384796811184914\n",
      "Gradient Descent(918/999): loss=0.3438399991089606\n",
      "Gradient Descent(919/999): loss=0.3438320478144748\n",
      "Gradient Descent(920/999): loss=0.34382411418547304\n",
      "Gradient Descent(921/999): loss=0.3438161981791549\n",
      "Gradient Descent(922/999): loss=0.343808299752836\n",
      "Gradient Descent(923/999): loss=0.3438004188639501\n",
      "Gradient Descent(924/999): loss=0.34379255547004606\n",
      "Gradient Descent(925/999): loss=0.3437847095287892\n",
      "Gradient Descent(926/999): loss=0.343776880997961\n",
      "Gradient Descent(927/999): loss=0.3437690698354571\n",
      "Gradient Descent(928/999): loss=0.3437612759992888\n",
      "Gradient Descent(929/999): loss=0.34375349944758155\n",
      "Gradient Descent(930/999): loss=0.3437457401385753\n",
      "Gradient Descent(931/999): loss=0.3437379980306232\n",
      "Gradient Descent(932/999): loss=0.3437302730821922\n",
      "Gradient Descent(933/999): loss=0.34372256525186196\n",
      "Gradient Descent(934/999): loss=0.34371487449832516\n",
      "Gradient Descent(935/999): loss=0.34370720078038647\n",
      "Gradient Descent(936/999): loss=0.3436995440569626\n",
      "Gradient Descent(937/999): loss=0.34369190428708174\n",
      "Gradient Descent(938/999): loss=0.3436842814298835\n",
      "Gradient Descent(939/999): loss=0.343676675444618\n",
      "Gradient Descent(940/999): loss=0.34366908629064613\n",
      "Gradient Descent(941/999): loss=0.3436615139274388\n",
      "Gradient Descent(942/999): loss=0.34365395831457696\n",
      "Gradient Descent(943/999): loss=0.34364641941175045\n",
      "Gradient Descent(944/999): loss=0.3436388971787587\n",
      "Gradient Descent(945/999): loss=0.3436313915755097\n",
      "Gradient Descent(946/999): loss=0.3436239025620199\n",
      "Gradient Descent(947/999): loss=0.34361643009841364\n",
      "Gradient Descent(948/999): loss=0.343608974144923\n",
      "Gradient Descent(949/999): loss=0.34360153466188753\n",
      "Gradient Descent(950/999): loss=0.3435941116097536\n",
      "Gradient Descent(951/999): loss=0.34358670494907445\n",
      "Gradient Descent(952/999): loss=0.34357931464050984\n",
      "Gradient Descent(953/999): loss=0.3435719406448249\n",
      "Gradient Descent(954/999): loss=0.3435645829228909\n",
      "Gradient Descent(955/999): loss=0.34355724143568445\n",
      "Gradient Descent(956/999): loss=0.3435499161442868\n",
      "Gradient Descent(957/999): loss=0.34354260700988426\n",
      "Gradient Descent(958/999): loss=0.3435353139937674\n",
      "Gradient Descent(959/999): loss=0.34352803705733037\n",
      "Gradient Descent(960/999): loss=0.3435207761620716\n",
      "Gradient Descent(961/999): loss=0.34351353126959233\n",
      "Gradient Descent(962/999): loss=0.3435063023415973\n",
      "Gradient Descent(963/999): loss=0.3434990893398935\n",
      "Gradient Descent(964/999): loss=0.34349189222639065\n",
      "Gradient Descent(965/999): loss=0.3434847109631002\n",
      "Gradient Descent(966/999): loss=0.34347754551213566\n",
      "Gradient Descent(967/999): loss=0.3434703958357115\n",
      "Gradient Descent(968/999): loss=0.3434632618961438\n",
      "Gradient Descent(969/999): loss=0.3434561436558493\n",
      "Gradient Descent(970/999): loss=0.3434490410773447\n",
      "Gradient Descent(971/999): loss=0.3434419541232473\n",
      "Gradient Descent(972/999): loss=0.3434348827562744\n",
      "Gradient Descent(973/999): loss=0.3434278269392422\n",
      "Gradient Descent(974/999): loss=0.3434207866350666\n",
      "Gradient Descent(975/999): loss=0.34341376180676225\n",
      "Gradient Descent(976/999): loss=0.34340675241744256\n",
      "Gradient Descent(977/999): loss=0.3433997584303188\n",
      "Gradient Descent(978/999): loss=0.34339277980870064\n",
      "Gradient Descent(979/999): loss=0.3433858165159951\n",
      "Gradient Descent(980/999): loss=0.34337886851570676\n",
      "Gradient Descent(981/999): loss=0.3433719357714373\n",
      "Gradient Descent(982/999): loss=0.343365018246885\n",
      "Gradient Descent(983/999): loss=0.34335811590584464\n",
      "Gradient Descent(984/999): loss=0.3433512287122073\n",
      "Gradient Descent(985/999): loss=0.34334435662996\n",
      "Gradient Descent(986/999): loss=0.3433374996231846\n",
      "Gradient Descent(987/999): loss=0.34333065765605947\n",
      "Gradient Descent(988/999): loss=0.343323830692857\n",
      "Gradient Descent(989/999): loss=0.3433170186979443\n",
      "Gradient Descent(990/999): loss=0.34331022163578345\n",
      "Gradient Descent(991/999): loss=0.34330343947093006\n",
      "Gradient Descent(992/999): loss=0.3432966721680339\n",
      "Gradient Descent(993/999): loss=0.34328991969183814\n",
      "Gradient Descent(994/999): loss=0.3432831820071792\n",
      "Gradient Descent(995/999): loss=0.3432764590789867\n",
      "Gradient Descent(996/999): loss=0.34326975087228206\n",
      "Gradient Descent(997/999): loss=0.34326305735218005\n",
      "Gradient Descent(998/999): loss=0.34325637848388735\n",
      "Gradient Descent(999/999): loss=0.3432497142327018\n",
      "parameters w:  [-0.31465042  0.04197621 -0.23534325 -0.17314583  0.02012779 -0.01187588\n",
      "  0.26245187 -0.01749717  0.20186582 -0.02009936 -0.00310099 -0.12085995\n",
      "  0.1184256  -0.01337476  0.18706606 -0.00096492 -0.00162282  0.18152135\n",
      " -0.00082432  0.00249176  0.08814838  0.00115061 -0.07005346 -0.10721263\n",
      "  0.02615664  0.02936407  0.02937945 -0.01606007 -0.01384844 -0.01391513\n",
      " -0.08729817]\n"
     ]
    }
   ],
   "source": [
    "from gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "# start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tX, w_initial, max_iters, gamma)\n",
    "# end_time = datetime.datetime.now()\n",
    "\n",
    "print('parameters w: ',gradient_ws[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/999): loss=0.5\n",
      "Stochastic Gradient Descent(1/999): loss=0.49222589205740186\n",
      "Stochastic Gradient Descent(2/999): loss=0.49135157855815487\n",
      "Stochastic Gradient Descent(3/999): loss=0.4843033379971171\n",
      "Stochastic Gradient Descent(4/999): loss=0.47046065605389237\n",
      "Stochastic Gradient Descent(5/999): loss=0.43846784661884025\n",
      "Stochastic Gradient Descent(6/999): loss=0.45485477450700784\n",
      "Stochastic Gradient Descent(7/999): loss=0.4423179007530215\n",
      "Stochastic Gradient Descent(8/999): loss=0.4538152752224161\n",
      "Stochastic Gradient Descent(9/999): loss=0.45994359028621384\n",
      "Stochastic Gradient Descent(10/999): loss=0.44927270145487597\n",
      "Stochastic Gradient Descent(11/999): loss=0.46335742222087317\n",
      "Stochastic Gradient Descent(12/999): loss=0.47571407608318245\n",
      "Stochastic Gradient Descent(13/999): loss=0.42783913708759846\n",
      "Stochastic Gradient Descent(14/999): loss=0.500655125307837\n",
      "Stochastic Gradient Descent(15/999): loss=0.4609521743340496\n",
      "Stochastic Gradient Descent(16/999): loss=0.4589254909648915\n",
      "Stochastic Gradient Descent(17/999): loss=0.4403138818616602\n",
      "Stochastic Gradient Descent(18/999): loss=0.4524515641911288\n",
      "Stochastic Gradient Descent(19/999): loss=0.4391057796299265\n",
      "Stochastic Gradient Descent(20/999): loss=0.4539670912481189\n",
      "Stochastic Gradient Descent(21/999): loss=0.46524282517013177\n",
      "Stochastic Gradient Descent(22/999): loss=0.4081489607785469\n",
      "Stochastic Gradient Descent(23/999): loss=0.4424500909852005\n",
      "Stochastic Gradient Descent(24/999): loss=0.4614014378466271\n",
      "Stochastic Gradient Descent(25/999): loss=0.4438047253269982\n",
      "Stochastic Gradient Descent(26/999): loss=0.4081212897790658\n",
      "Stochastic Gradient Descent(27/999): loss=0.4450719994709918\n",
      "Stochastic Gradient Descent(28/999): loss=0.42221850968065605\n",
      "Stochastic Gradient Descent(29/999): loss=0.40868544578322696\n",
      "Stochastic Gradient Descent(30/999): loss=0.38268552916460746\n",
      "Stochastic Gradient Descent(31/999): loss=0.4170083826688075\n",
      "Stochastic Gradient Descent(32/999): loss=0.4180544533207465\n",
      "Stochastic Gradient Descent(33/999): loss=0.42323629782607347\n",
      "Stochastic Gradient Descent(34/999): loss=0.39717073849701395\n",
      "Stochastic Gradient Descent(35/999): loss=0.40216614264262773\n",
      "Stochastic Gradient Descent(36/999): loss=0.43168101806350506\n",
      "Stochastic Gradient Descent(37/999): loss=0.4198446941013074\n",
      "Stochastic Gradient Descent(38/999): loss=0.41378719397542135\n",
      "Stochastic Gradient Descent(39/999): loss=0.39750106258076207\n",
      "Stochastic Gradient Descent(40/999): loss=0.3706384806355303\n",
      "Stochastic Gradient Descent(41/999): loss=0.44584136382441997\n",
      "Stochastic Gradient Descent(42/999): loss=0.4288987153120424\n",
      "Stochastic Gradient Descent(43/999): loss=0.4272399004171045\n",
      "Stochastic Gradient Descent(44/999): loss=0.443223712228795\n",
      "Stochastic Gradient Descent(45/999): loss=0.3795599453164488\n",
      "Stochastic Gradient Descent(46/999): loss=0.39003532757263776\n",
      "Stochastic Gradient Descent(47/999): loss=0.39714240293739494\n",
      "Stochastic Gradient Descent(48/999): loss=0.38803810389425214\n",
      "Stochastic Gradient Descent(49/999): loss=0.441261939278297\n",
      "Stochastic Gradient Descent(50/999): loss=0.38351351295433056\n",
      "Stochastic Gradient Descent(51/999): loss=0.39263491228245995\n",
      "Stochastic Gradient Descent(52/999): loss=0.40100520558924174\n",
      "Stochastic Gradient Descent(53/999): loss=0.4394679646549362\n",
      "Stochastic Gradient Descent(54/999): loss=0.35587992292039156\n",
      "Stochastic Gradient Descent(55/999): loss=0.36251357090900804\n",
      "Stochastic Gradient Descent(56/999): loss=0.4168918200232363\n",
      "Stochastic Gradient Descent(57/999): loss=0.33894019505828044\n",
      "Stochastic Gradient Descent(58/999): loss=0.47714664873442075\n",
      "Stochastic Gradient Descent(59/999): loss=0.40667548321713887\n",
      "Stochastic Gradient Descent(60/999): loss=0.32836511241059385\n",
      "Stochastic Gradient Descent(61/999): loss=0.4472162892275099\n",
      "Stochastic Gradient Descent(62/999): loss=0.41038447328310473\n",
      "Stochastic Gradient Descent(63/999): loss=0.37271448590796685\n",
      "Stochastic Gradient Descent(64/999): loss=0.39155568341180486\n",
      "Stochastic Gradient Descent(65/999): loss=0.41804992110867495\n",
      "Stochastic Gradient Descent(66/999): loss=0.4033644420385399\n",
      "Stochastic Gradient Descent(67/999): loss=0.3030968506139561\n",
      "Stochastic Gradient Descent(68/999): loss=0.3913703196170022\n",
      "Stochastic Gradient Descent(69/999): loss=0.4132837064301793\n",
      "Stochastic Gradient Descent(70/999): loss=0.4380245707084562\n",
      "Stochastic Gradient Descent(71/999): loss=0.4150568611467103\n",
      "Stochastic Gradient Descent(72/999): loss=0.3396819721104622\n",
      "Stochastic Gradient Descent(73/999): loss=0.3500927695543898\n",
      "Stochastic Gradient Descent(74/999): loss=0.4041490668842394\n",
      "Stochastic Gradient Descent(75/999): loss=0.37522637830033895\n",
      "Stochastic Gradient Descent(76/999): loss=0.4081147610757588\n",
      "Stochastic Gradient Descent(77/999): loss=0.3749594649442869\n",
      "Stochastic Gradient Descent(78/999): loss=0.410146335962683\n",
      "Stochastic Gradient Descent(79/999): loss=0.3338178762885466\n",
      "Stochastic Gradient Descent(80/999): loss=0.3668013102328841\n",
      "Stochastic Gradient Descent(81/999): loss=0.36054358752495275\n",
      "Stochastic Gradient Descent(82/999): loss=0.3357730136370597\n",
      "Stochastic Gradient Descent(83/999): loss=0.3802612342358836\n",
      "Stochastic Gradient Descent(84/999): loss=0.37303211428394983\n",
      "Stochastic Gradient Descent(85/999): loss=0.39985352768505483\n",
      "Stochastic Gradient Descent(86/999): loss=0.3515150225638525\n",
      "Stochastic Gradient Descent(87/999): loss=0.3673869914847628\n",
      "Stochastic Gradient Descent(88/999): loss=0.32557484345102616\n",
      "Stochastic Gradient Descent(89/999): loss=0.4488895009683373\n",
      "Stochastic Gradient Descent(90/999): loss=0.4443166986272536\n",
      "Stochastic Gradient Descent(91/999): loss=0.42465278137966195\n",
      "Stochastic Gradient Descent(92/999): loss=0.45454228017935117\n",
      "Stochastic Gradient Descent(93/999): loss=0.437001040154592\n",
      "Stochastic Gradient Descent(94/999): loss=0.4455579153128869\n",
      "Stochastic Gradient Descent(95/999): loss=0.3696048772293594\n",
      "Stochastic Gradient Descent(96/999): loss=0.4366833106229774\n",
      "Stochastic Gradient Descent(97/999): loss=0.36630922515526665\n",
      "Stochastic Gradient Descent(98/999): loss=0.4278778192209925\n",
      "Stochastic Gradient Descent(99/999): loss=0.4211910438764573\n",
      "Stochastic Gradient Descent(100/999): loss=0.322776594601925\n",
      "Stochastic Gradient Descent(101/999): loss=0.35216917534150916\n",
      "Stochastic Gradient Descent(102/999): loss=0.4098465333556502\n",
      "Stochastic Gradient Descent(103/999): loss=0.3774865621447535\n",
      "Stochastic Gradient Descent(104/999): loss=0.3318130616954121\n",
      "Stochastic Gradient Descent(105/999): loss=0.35400219676403616\n",
      "Stochastic Gradient Descent(106/999): loss=0.40001793378503964\n",
      "Stochastic Gradient Descent(107/999): loss=0.32163656937780055\n",
      "Stochastic Gradient Descent(108/999): loss=0.4323660545633495\n",
      "Stochastic Gradient Descent(109/999): loss=0.30298830581327474\n",
      "Stochastic Gradient Descent(110/999): loss=0.3486269444847418\n",
      "Stochastic Gradient Descent(111/999): loss=0.33852474606805705\n",
      "Stochastic Gradient Descent(112/999): loss=0.368291453874299\n",
      "Stochastic Gradient Descent(113/999): loss=0.3180485449138755\n",
      "Stochastic Gradient Descent(114/999): loss=0.35369312864956703\n",
      "Stochastic Gradient Descent(115/999): loss=0.37293983227791583\n",
      "Stochastic Gradient Descent(116/999): loss=0.5031130109773623\n",
      "Stochastic Gradient Descent(117/999): loss=0.3489623168284554\n",
      "Stochastic Gradient Descent(118/999): loss=0.4178018215329546\n",
      "Stochastic Gradient Descent(119/999): loss=0.4615316343757779\n",
      "Stochastic Gradient Descent(120/999): loss=0.3542473680257862\n",
      "Stochastic Gradient Descent(121/999): loss=0.3426566553669682\n",
      "Stochastic Gradient Descent(122/999): loss=0.3573866031478948\n",
      "Stochastic Gradient Descent(123/999): loss=0.3732468941348558\n",
      "Stochastic Gradient Descent(124/999): loss=0.400373956998336\n",
      "Stochastic Gradient Descent(125/999): loss=0.34375808184444495\n",
      "Stochastic Gradient Descent(126/999): loss=0.3316984911342224\n",
      "Stochastic Gradient Descent(127/999): loss=0.3888150231891983\n",
      "Stochastic Gradient Descent(128/999): loss=0.2958333336884956\n",
      "Stochastic Gradient Descent(129/999): loss=0.3074332393260579\n",
      "Stochastic Gradient Descent(130/999): loss=0.41530250458014606\n",
      "Stochastic Gradient Descent(131/999): loss=0.40480513187151024\n",
      "Stochastic Gradient Descent(132/999): loss=0.32392264881452204\n",
      "Stochastic Gradient Descent(133/999): loss=0.36988354761563796\n",
      "Stochastic Gradient Descent(134/999): loss=0.34487237330242043\n",
      "Stochastic Gradient Descent(135/999): loss=0.4221807234900749\n",
      "Stochastic Gradient Descent(136/999): loss=0.4223075661299804\n",
      "Stochastic Gradient Descent(137/999): loss=0.42966459345604535\n",
      "Stochastic Gradient Descent(138/999): loss=0.3783112932237542\n",
      "Stochastic Gradient Descent(139/999): loss=0.45354594460968106\n",
      "Stochastic Gradient Descent(140/999): loss=0.3279982616727645\n",
      "Stochastic Gradient Descent(141/999): loss=0.33855644721954214\n",
      "Stochastic Gradient Descent(142/999): loss=0.37158097021248215\n",
      "Stochastic Gradient Descent(143/999): loss=0.37061597346102887\n",
      "Stochastic Gradient Descent(144/999): loss=0.38223293820682597\n",
      "Stochastic Gradient Descent(145/999): loss=0.32934045481604524\n",
      "Stochastic Gradient Descent(146/999): loss=0.3736060512808491\n",
      "Stochastic Gradient Descent(147/999): loss=0.3606907871152773\n",
      "Stochastic Gradient Descent(148/999): loss=0.32138178874407347\n",
      "Stochastic Gradient Descent(149/999): loss=0.35610181124173174\n",
      "Stochastic Gradient Descent(150/999): loss=0.3163775750458474\n",
      "Stochastic Gradient Descent(151/999): loss=0.35987449659842874\n",
      "Stochastic Gradient Descent(152/999): loss=0.2964099042944264\n",
      "Stochastic Gradient Descent(153/999): loss=0.3082695984420302\n",
      "Stochastic Gradient Descent(154/999): loss=0.34224103056569777\n",
      "Stochastic Gradient Descent(155/999): loss=0.354666936404086\n",
      "Stochastic Gradient Descent(156/999): loss=0.43887645303793976\n",
      "Stochastic Gradient Descent(157/999): loss=0.3375219851904036\n",
      "Stochastic Gradient Descent(158/999): loss=0.3517257415206823\n",
      "Stochastic Gradient Descent(159/999): loss=0.2747898256692756\n",
      "Stochastic Gradient Descent(160/999): loss=0.32453986725278766\n",
      "Stochastic Gradient Descent(161/999): loss=0.39055013600093375\n",
      "Stochastic Gradient Descent(162/999): loss=0.35816943512148003\n",
      "Stochastic Gradient Descent(163/999): loss=0.3716013918809791\n",
      "Stochastic Gradient Descent(164/999): loss=0.31334662212534004\n",
      "Stochastic Gradient Descent(165/999): loss=0.3807054005043372\n",
      "Stochastic Gradient Descent(166/999): loss=0.35169051795719375\n",
      "Stochastic Gradient Descent(167/999): loss=0.29278873551463946\n",
      "Stochastic Gradient Descent(168/999): loss=0.4561462517643917\n",
      "Stochastic Gradient Descent(169/999): loss=0.35427915846690444\n",
      "Stochastic Gradient Descent(170/999): loss=0.3545183802202058\n",
      "Stochastic Gradient Descent(171/999): loss=0.39168619094496987\n",
      "Stochastic Gradient Descent(172/999): loss=0.3442859585625033\n",
      "Stochastic Gradient Descent(173/999): loss=0.39405545409066145\n",
      "Stochastic Gradient Descent(174/999): loss=0.352099750433296\n",
      "Stochastic Gradient Descent(175/999): loss=0.45369723451872795\n",
      "Stochastic Gradient Descent(176/999): loss=0.35694086737910447\n",
      "Stochastic Gradient Descent(177/999): loss=0.34366424795309186\n",
      "Stochastic Gradient Descent(178/999): loss=0.3352525179942035\n",
      "Stochastic Gradient Descent(179/999): loss=0.351967187306303\n",
      "Stochastic Gradient Descent(180/999): loss=0.36247559311037136\n",
      "Stochastic Gradient Descent(181/999): loss=0.2931683505763144\n",
      "Stochastic Gradient Descent(182/999): loss=0.38526861822648095\n",
      "Stochastic Gradient Descent(183/999): loss=0.3345464761286939\n",
      "Stochastic Gradient Descent(184/999): loss=0.3353886690307717\n",
      "Stochastic Gradient Descent(185/999): loss=0.3771511334744699\n",
      "Stochastic Gradient Descent(186/999): loss=0.3228278855419531\n",
      "Stochastic Gradient Descent(187/999): loss=0.44568111674515765\n",
      "Stochastic Gradient Descent(188/999): loss=0.37672363082284355\n",
      "Stochastic Gradient Descent(189/999): loss=0.4178770752075372\n",
      "Stochastic Gradient Descent(190/999): loss=0.402274172436059\n",
      "Stochastic Gradient Descent(191/999): loss=0.4004108966719071\n",
      "Stochastic Gradient Descent(192/999): loss=0.38959345317648153\n",
      "Stochastic Gradient Descent(193/999): loss=0.3488504400341681\n",
      "Stochastic Gradient Descent(194/999): loss=0.359337951742012\n",
      "Stochastic Gradient Descent(195/999): loss=0.3336279501945617\n",
      "Stochastic Gradient Descent(196/999): loss=0.32451142395059773\n",
      "Stochastic Gradient Descent(197/999): loss=0.28606452355870565\n",
      "Stochastic Gradient Descent(198/999): loss=0.37504489632484117\n",
      "Stochastic Gradient Descent(199/999): loss=0.37619290131852395\n",
      "Stochastic Gradient Descent(200/999): loss=0.3226114165735038\n",
      "Stochastic Gradient Descent(201/999): loss=0.38370073041463043\n",
      "Stochastic Gradient Descent(202/999): loss=0.3785049259477239\n",
      "Stochastic Gradient Descent(203/999): loss=0.33712152727942063\n",
      "Stochastic Gradient Descent(204/999): loss=0.3841065123539979\n",
      "Stochastic Gradient Descent(205/999): loss=0.4314964450930585\n",
      "Stochastic Gradient Descent(206/999): loss=0.39347001989808633\n",
      "Stochastic Gradient Descent(207/999): loss=0.4461081158833822\n",
      "Stochastic Gradient Descent(208/999): loss=0.4873383687552183\n",
      "Stochastic Gradient Descent(209/999): loss=0.3842842697792581\n",
      "Stochastic Gradient Descent(210/999): loss=0.37424590013328546\n",
      "Stochastic Gradient Descent(211/999): loss=0.36082126519194885\n",
      "Stochastic Gradient Descent(212/999): loss=0.3241434282525774\n",
      "Stochastic Gradient Descent(213/999): loss=0.4143844829082687\n",
      "Stochastic Gradient Descent(214/999): loss=0.2512917590620624\n",
      "Stochastic Gradient Descent(215/999): loss=0.42141368687626934\n",
      "Stochastic Gradient Descent(216/999): loss=0.3417980813383993\n",
      "Stochastic Gradient Descent(217/999): loss=0.4367269331481613\n",
      "Stochastic Gradient Descent(218/999): loss=0.3438688983122143\n",
      "Stochastic Gradient Descent(219/999): loss=0.3583217308376503\n",
      "Stochastic Gradient Descent(220/999): loss=0.4169903940156165\n",
      "Stochastic Gradient Descent(221/999): loss=0.369410251352584\n",
      "Stochastic Gradient Descent(222/999): loss=0.38019736394289844\n",
      "Stochastic Gradient Descent(223/999): loss=0.3642008805775047\n",
      "Stochastic Gradient Descent(224/999): loss=0.3292342641404373\n",
      "Stochastic Gradient Descent(225/999): loss=0.3322589559093979\n",
      "Stochastic Gradient Descent(226/999): loss=0.38042572676855896\n",
      "Stochastic Gradient Descent(227/999): loss=0.3195337855312633\n",
      "Stochastic Gradient Descent(228/999): loss=0.2636624299326878\n",
      "Stochastic Gradient Descent(229/999): loss=0.37316179392116106\n",
      "Stochastic Gradient Descent(230/999): loss=0.44184831337430025\n",
      "Stochastic Gradient Descent(231/999): loss=0.4691857781992045\n",
      "Stochastic Gradient Descent(232/999): loss=0.4690157425726484\n",
      "Stochastic Gradient Descent(233/999): loss=0.34219298215079363\n",
      "Stochastic Gradient Descent(234/999): loss=0.40188153747271954\n",
      "Stochastic Gradient Descent(235/999): loss=0.41791511612824883\n",
      "Stochastic Gradient Descent(236/999): loss=0.3677251407283139\n",
      "Stochastic Gradient Descent(237/999): loss=0.43202511915878267\n",
      "Stochastic Gradient Descent(238/999): loss=0.37430990053321345\n",
      "Stochastic Gradient Descent(239/999): loss=0.27370979807042456\n",
      "Stochastic Gradient Descent(240/999): loss=0.3783248233344844\n",
      "Stochastic Gradient Descent(241/999): loss=0.38338802369560043\n",
      "Stochastic Gradient Descent(242/999): loss=0.331090699211365\n",
      "Stochastic Gradient Descent(243/999): loss=0.27469958022910923\n",
      "Stochastic Gradient Descent(244/999): loss=0.3813343685892247\n",
      "Stochastic Gradient Descent(245/999): loss=0.35441517486440866\n",
      "Stochastic Gradient Descent(246/999): loss=0.33469888201815734\n",
      "Stochastic Gradient Descent(247/999): loss=0.44022890346898186\n",
      "Stochastic Gradient Descent(248/999): loss=0.437391724065993\n",
      "Stochastic Gradient Descent(249/999): loss=0.3815540353719171\n",
      "Stochastic Gradient Descent(250/999): loss=0.4159030870154534\n",
      "Stochastic Gradient Descent(251/999): loss=0.36628727107097225\n",
      "Stochastic Gradient Descent(252/999): loss=0.3085550708850056\n",
      "Stochastic Gradient Descent(253/999): loss=0.33810105171789956\n",
      "Stochastic Gradient Descent(254/999): loss=0.36394784869208274\n",
      "Stochastic Gradient Descent(255/999): loss=0.37265698619338006\n",
      "Stochastic Gradient Descent(256/999): loss=0.36699346432687824\n",
      "Stochastic Gradient Descent(257/999): loss=0.3851341085326834\n",
      "Stochastic Gradient Descent(258/999): loss=0.3437115700677339\n",
      "Stochastic Gradient Descent(259/999): loss=0.30661455796245884\n",
      "Stochastic Gradient Descent(260/999): loss=0.3858791931784995\n",
      "Stochastic Gradient Descent(261/999): loss=0.30086351064137024\n",
      "Stochastic Gradient Descent(262/999): loss=0.3817069054406588\n",
      "Stochastic Gradient Descent(263/999): loss=0.29322788522227294\n",
      "Stochastic Gradient Descent(264/999): loss=0.37878905348772585\n",
      "Stochastic Gradient Descent(265/999): loss=0.3941510147046417\n",
      "Stochastic Gradient Descent(266/999): loss=0.26501378734303455\n",
      "Stochastic Gradient Descent(267/999): loss=0.4282237072017771\n",
      "Stochastic Gradient Descent(268/999): loss=0.28476428319777064\n",
      "Stochastic Gradient Descent(269/999): loss=0.2772016420272878\n",
      "Stochastic Gradient Descent(270/999): loss=0.42779935514710243\n",
      "Stochastic Gradient Descent(271/999): loss=0.3608021558821713\n",
      "Stochastic Gradient Descent(272/999): loss=0.3656042730280179\n",
      "Stochastic Gradient Descent(273/999): loss=0.33345588788488806\n",
      "Stochastic Gradient Descent(274/999): loss=0.2923804041026663\n",
      "Stochastic Gradient Descent(275/999): loss=0.37871354137812174\n",
      "Stochastic Gradient Descent(276/999): loss=0.3653525614181536\n",
      "Stochastic Gradient Descent(277/999): loss=0.35143042901385324\n",
      "Stochastic Gradient Descent(278/999): loss=0.3353225573738565\n",
      "Stochastic Gradient Descent(279/999): loss=0.4804954751906365\n",
      "Stochastic Gradient Descent(280/999): loss=0.3893527506603713\n",
      "Stochastic Gradient Descent(281/999): loss=0.36309016527764926\n",
      "Stochastic Gradient Descent(282/999): loss=0.37114637297675407\n",
      "Stochastic Gradient Descent(283/999): loss=0.40086375996127943\n",
      "Stochastic Gradient Descent(284/999): loss=0.35092987401715225\n",
      "Stochastic Gradient Descent(285/999): loss=0.3798340346220396\n",
      "Stochastic Gradient Descent(286/999): loss=0.3111052664748922\n",
      "Stochastic Gradient Descent(287/999): loss=0.40615748959626946\n",
      "Stochastic Gradient Descent(288/999): loss=0.37961102261480906\n",
      "Stochastic Gradient Descent(289/999): loss=0.32981093325356986\n",
      "Stochastic Gradient Descent(290/999): loss=0.30832815111863165\n",
      "Stochastic Gradient Descent(291/999): loss=0.33648641830944553\n",
      "Stochastic Gradient Descent(292/999): loss=0.36566284326321935\n",
      "Stochastic Gradient Descent(293/999): loss=0.42771805785298184\n",
      "Stochastic Gradient Descent(294/999): loss=0.35643130485358676\n",
      "Stochastic Gradient Descent(295/999): loss=0.42317693257917144\n",
      "Stochastic Gradient Descent(296/999): loss=0.38411733630948325\n",
      "Stochastic Gradient Descent(297/999): loss=0.32829094576543727\n",
      "Stochastic Gradient Descent(298/999): loss=0.3005882293265377\n",
      "Stochastic Gradient Descent(299/999): loss=0.26117692657121766\n",
      "Stochastic Gradient Descent(300/999): loss=0.26051956308346635\n",
      "Stochastic Gradient Descent(301/999): loss=0.3896395119402987\n",
      "Stochastic Gradient Descent(302/999): loss=0.38389271356283267\n",
      "Stochastic Gradient Descent(303/999): loss=0.30233437862471113\n",
      "Stochastic Gradient Descent(304/999): loss=0.2961008369951227\n",
      "Stochastic Gradient Descent(305/999): loss=0.3356619076071878\n",
      "Stochastic Gradient Descent(306/999): loss=0.2706066093591634\n",
      "Stochastic Gradient Descent(307/999): loss=0.3548631082678266\n",
      "Stochastic Gradient Descent(308/999): loss=0.36015571632386\n",
      "Stochastic Gradient Descent(309/999): loss=0.3914526315917434\n",
      "Stochastic Gradient Descent(310/999): loss=0.3841802109079114\n",
      "Stochastic Gradient Descent(311/999): loss=0.39495044424743725\n",
      "Stochastic Gradient Descent(312/999): loss=0.3710157898694002\n",
      "Stochastic Gradient Descent(313/999): loss=0.30804706636848533\n",
      "Stochastic Gradient Descent(314/999): loss=0.33431796322410195\n",
      "Stochastic Gradient Descent(315/999): loss=0.34969182728141246\n",
      "Stochastic Gradient Descent(316/999): loss=0.3258648940725149\n",
      "Stochastic Gradient Descent(317/999): loss=0.2817163045870717\n",
      "Stochastic Gradient Descent(318/999): loss=0.44039544805265324\n",
      "Stochastic Gradient Descent(319/999): loss=0.26654989667900464\n",
      "Stochastic Gradient Descent(320/999): loss=0.3251551849665039\n",
      "Stochastic Gradient Descent(321/999): loss=0.36917528828031904\n",
      "Stochastic Gradient Descent(322/999): loss=0.38541445392913254\n",
      "Stochastic Gradient Descent(323/999): loss=0.250987880446336\n",
      "Stochastic Gradient Descent(324/999): loss=0.3302725571499105\n",
      "Stochastic Gradient Descent(325/999): loss=0.34106863490425565\n",
      "Stochastic Gradient Descent(326/999): loss=0.34747261887249564\n",
      "Stochastic Gradient Descent(327/999): loss=0.358396352893771\n",
      "Stochastic Gradient Descent(328/999): loss=0.43357267414208095\n",
      "Stochastic Gradient Descent(329/999): loss=0.31146706574452937\n",
      "Stochastic Gradient Descent(330/999): loss=0.28535039781725413\n",
      "Stochastic Gradient Descent(331/999): loss=0.35939357652021725\n",
      "Stochastic Gradient Descent(332/999): loss=0.416321630685907\n",
      "Stochastic Gradient Descent(333/999): loss=0.3864409350645591\n",
      "Stochastic Gradient Descent(334/999): loss=0.3024089823171213\n",
      "Stochastic Gradient Descent(335/999): loss=0.3920389360850113\n",
      "Stochastic Gradient Descent(336/999): loss=0.3929063333073624\n",
      "Stochastic Gradient Descent(337/999): loss=0.36353710515435295\n",
      "Stochastic Gradient Descent(338/999): loss=0.33707286626660493\n",
      "Stochastic Gradient Descent(339/999): loss=0.24205514187011054\n",
      "Stochastic Gradient Descent(340/999): loss=0.26662035670829354\n",
      "Stochastic Gradient Descent(341/999): loss=0.35762501661352303\n",
      "Stochastic Gradient Descent(342/999): loss=0.28812184185582307\n",
      "Stochastic Gradient Descent(343/999): loss=0.415736507003874\n",
      "Stochastic Gradient Descent(344/999): loss=0.40323550203406405\n",
      "Stochastic Gradient Descent(345/999): loss=0.375666627993114\n",
      "Stochastic Gradient Descent(346/999): loss=0.2763280449244293\n",
      "Stochastic Gradient Descent(347/999): loss=0.38698263409903183\n",
      "Stochastic Gradient Descent(348/999): loss=0.3973823033801553\n",
      "Stochastic Gradient Descent(349/999): loss=0.38813148519541085\n",
      "Stochastic Gradient Descent(350/999): loss=0.434454944296262\n",
      "Stochastic Gradient Descent(351/999): loss=0.3512959621603755\n",
      "Stochastic Gradient Descent(352/999): loss=0.31794921599804904\n",
      "Stochastic Gradient Descent(353/999): loss=0.4007799128462651\n",
      "Stochastic Gradient Descent(354/999): loss=0.30523625896929196\n",
      "Stochastic Gradient Descent(355/999): loss=0.3999221624023456\n",
      "Stochastic Gradient Descent(356/999): loss=0.2982957991826909\n",
      "Stochastic Gradient Descent(357/999): loss=0.34267694738316495\n",
      "Stochastic Gradient Descent(358/999): loss=0.3564726395654244\n",
      "Stochastic Gradient Descent(359/999): loss=0.39031877042438334\n",
      "Stochastic Gradient Descent(360/999): loss=0.37342789516448477\n",
      "Stochastic Gradient Descent(361/999): loss=0.3395932985717589\n",
      "Stochastic Gradient Descent(362/999): loss=0.339692550134598\n",
      "Stochastic Gradient Descent(363/999): loss=0.33512011572371014\n",
      "Stochastic Gradient Descent(364/999): loss=0.324073085913727\n",
      "Stochastic Gradient Descent(365/999): loss=0.30528418262696083\n",
      "Stochastic Gradient Descent(366/999): loss=0.32333729609776646\n",
      "Stochastic Gradient Descent(367/999): loss=0.3818419667563699\n",
      "Stochastic Gradient Descent(368/999): loss=0.28451165973214815\n",
      "Stochastic Gradient Descent(369/999): loss=0.3259930501335793\n",
      "Stochastic Gradient Descent(370/999): loss=0.33787909423289186\n",
      "Stochastic Gradient Descent(371/999): loss=0.31196212200776996\n",
      "Stochastic Gradient Descent(372/999): loss=0.41615085111548644\n",
      "Stochastic Gradient Descent(373/999): loss=0.3161274048669859\n",
      "Stochastic Gradient Descent(374/999): loss=0.3479929965137465\n",
      "Stochastic Gradient Descent(375/999): loss=0.32604008854459615\n",
      "Stochastic Gradient Descent(376/999): loss=0.32759771501393964\n",
      "Stochastic Gradient Descent(377/999): loss=0.33142099123504204\n",
      "Stochastic Gradient Descent(378/999): loss=0.26946899946855196\n",
      "Stochastic Gradient Descent(379/999): loss=0.37150792347652895\n",
      "Stochastic Gradient Descent(380/999): loss=0.4191981075125987\n",
      "Stochastic Gradient Descent(381/999): loss=0.3250602546909971\n",
      "Stochastic Gradient Descent(382/999): loss=0.38757438532324445\n",
      "Stochastic Gradient Descent(383/999): loss=0.33193088798200515\n",
      "Stochastic Gradient Descent(384/999): loss=0.3913627419541604\n",
      "Stochastic Gradient Descent(385/999): loss=0.3536804667220079\n",
      "Stochastic Gradient Descent(386/999): loss=0.3217316928593811\n",
      "Stochastic Gradient Descent(387/999): loss=0.2912347106352393\n",
      "Stochastic Gradient Descent(388/999): loss=0.29877213671098035\n",
      "Stochastic Gradient Descent(389/999): loss=0.2822257393330977\n",
      "Stochastic Gradient Descent(390/999): loss=0.32778389467246055\n",
      "Stochastic Gradient Descent(391/999): loss=0.3740766283330919\n",
      "Stochastic Gradient Descent(392/999): loss=0.367192595978544\n",
      "Stochastic Gradient Descent(393/999): loss=0.372297307722757\n",
      "Stochastic Gradient Descent(394/999): loss=0.3301068636229995\n",
      "Stochastic Gradient Descent(395/999): loss=0.3956671443809832\n",
      "Stochastic Gradient Descent(396/999): loss=0.3021841348366456\n",
      "Stochastic Gradient Descent(397/999): loss=0.4525891437071989\n",
      "Stochastic Gradient Descent(398/999): loss=0.33031862765421804\n",
      "Stochastic Gradient Descent(399/999): loss=0.3444227356397825\n",
      "Stochastic Gradient Descent(400/999): loss=0.3313867544395828\n",
      "Stochastic Gradient Descent(401/999): loss=0.37300982927981763\n",
      "Stochastic Gradient Descent(402/999): loss=0.3280375288199084\n",
      "Stochastic Gradient Descent(403/999): loss=0.2901586488473534\n",
      "Stochastic Gradient Descent(404/999): loss=0.3795391149706891\n",
      "Stochastic Gradient Descent(405/999): loss=0.3174475047310272\n",
      "Stochastic Gradient Descent(406/999): loss=0.3243340935407295\n",
      "Stochastic Gradient Descent(407/999): loss=0.29939784194756336\n",
      "Stochastic Gradient Descent(408/999): loss=0.3588822540420938\n",
      "Stochastic Gradient Descent(409/999): loss=0.33370755771119254\n",
      "Stochastic Gradient Descent(410/999): loss=0.3993117818096688\n",
      "Stochastic Gradient Descent(411/999): loss=0.4234468676393732\n",
      "Stochastic Gradient Descent(412/999): loss=0.4027815311928048\n",
      "Stochastic Gradient Descent(413/999): loss=0.3355641322730291\n",
      "Stochastic Gradient Descent(414/999): loss=0.3639487361266772\n",
      "Stochastic Gradient Descent(415/999): loss=0.4140020836994527\n",
      "Stochastic Gradient Descent(416/999): loss=0.35095434664257463\n",
      "Stochastic Gradient Descent(417/999): loss=0.3869442067699495\n",
      "Stochastic Gradient Descent(418/999): loss=0.3467321968068147\n",
      "Stochastic Gradient Descent(419/999): loss=0.29862514469612184\n",
      "Stochastic Gradient Descent(420/999): loss=0.2819378643982371\n",
      "Stochastic Gradient Descent(421/999): loss=0.30635973413412126\n",
      "Stochastic Gradient Descent(422/999): loss=0.3640175944343608\n",
      "Stochastic Gradient Descent(423/999): loss=0.329472899256164\n",
      "Stochastic Gradient Descent(424/999): loss=0.29685610300848997\n",
      "Stochastic Gradient Descent(425/999): loss=0.33451708031085026\n",
      "Stochastic Gradient Descent(426/999): loss=0.35365478423968155\n",
      "Stochastic Gradient Descent(427/999): loss=0.35512890935083924\n",
      "Stochastic Gradient Descent(428/999): loss=0.3311301160472575\n",
      "Stochastic Gradient Descent(429/999): loss=0.3930035999526899\n",
      "Stochastic Gradient Descent(430/999): loss=0.3323260418257312\n",
      "Stochastic Gradient Descent(431/999): loss=0.3000372182185107\n",
      "Stochastic Gradient Descent(432/999): loss=0.3181180196606446\n",
      "Stochastic Gradient Descent(433/999): loss=0.4429975539363597\n",
      "Stochastic Gradient Descent(434/999): loss=0.42618939143860324\n",
      "Stochastic Gradient Descent(435/999): loss=0.3000497512530811\n",
      "Stochastic Gradient Descent(436/999): loss=0.3701893137011442\n",
      "Stochastic Gradient Descent(437/999): loss=0.2823202459911069\n",
      "Stochastic Gradient Descent(438/999): loss=0.420960097487107\n",
      "Stochastic Gradient Descent(439/999): loss=0.33051622547815973\n",
      "Stochastic Gradient Descent(440/999): loss=0.3490292956809704\n",
      "Stochastic Gradient Descent(441/999): loss=0.3333565438389396\n",
      "Stochastic Gradient Descent(442/999): loss=0.3913756786396048\n",
      "Stochastic Gradient Descent(443/999): loss=0.2872751361356443\n",
      "Stochastic Gradient Descent(444/999): loss=0.38097222858034735\n",
      "Stochastic Gradient Descent(445/999): loss=0.3413563544568122\n",
      "Stochastic Gradient Descent(446/999): loss=0.45283695813558594\n",
      "Stochastic Gradient Descent(447/999): loss=0.2918196671640629\n",
      "Stochastic Gradient Descent(448/999): loss=0.32015012189349307\n",
      "Stochastic Gradient Descent(449/999): loss=0.3284469388700613\n",
      "Stochastic Gradient Descent(450/999): loss=0.28882675365570215\n",
      "Stochastic Gradient Descent(451/999): loss=0.41519668949968136\n",
      "Stochastic Gradient Descent(452/999): loss=0.4057617611883546\n",
      "Stochastic Gradient Descent(453/999): loss=0.2957856594349477\n",
      "Stochastic Gradient Descent(454/999): loss=0.30882595997749634\n",
      "Stochastic Gradient Descent(455/999): loss=0.484038628829326\n",
      "Stochastic Gradient Descent(456/999): loss=0.28275576579219447\n",
      "Stochastic Gradient Descent(457/999): loss=0.283732911103624\n",
      "Stochastic Gradient Descent(458/999): loss=0.38647946752267304\n",
      "Stochastic Gradient Descent(459/999): loss=0.4365063443478958\n",
      "Stochastic Gradient Descent(460/999): loss=0.3177546297396397\n",
      "Stochastic Gradient Descent(461/999): loss=0.30493874755702466\n",
      "Stochastic Gradient Descent(462/999): loss=0.3481411474941667\n",
      "Stochastic Gradient Descent(463/999): loss=0.3154607793257064\n",
      "Stochastic Gradient Descent(464/999): loss=0.2614019627879147\n",
      "Stochastic Gradient Descent(465/999): loss=0.3654233143846934\n",
      "Stochastic Gradient Descent(466/999): loss=0.336581948687484\n",
      "Stochastic Gradient Descent(467/999): loss=0.34593706670194474\n",
      "Stochastic Gradient Descent(468/999): loss=0.3698717539009364\n",
      "Stochastic Gradient Descent(469/999): loss=0.2834589541951986\n",
      "Stochastic Gradient Descent(470/999): loss=0.30074489149027445\n",
      "Stochastic Gradient Descent(471/999): loss=0.4501578696986194\n",
      "Stochastic Gradient Descent(472/999): loss=0.41263713389591644\n",
      "Stochastic Gradient Descent(473/999): loss=0.3172556033930652\n",
      "Stochastic Gradient Descent(474/999): loss=0.3182806584089729\n",
      "Stochastic Gradient Descent(475/999): loss=0.36538324378822146\n",
      "Stochastic Gradient Descent(476/999): loss=0.2585892624139964\n",
      "Stochastic Gradient Descent(477/999): loss=0.3337634223148228\n",
      "Stochastic Gradient Descent(478/999): loss=0.3778711176817836\n",
      "Stochastic Gradient Descent(479/999): loss=0.3174729688126498\n",
      "Stochastic Gradient Descent(480/999): loss=0.3407729973077256\n",
      "Stochastic Gradient Descent(481/999): loss=0.2866447245586708\n",
      "Stochastic Gradient Descent(482/999): loss=0.2881540689594886\n",
      "Stochastic Gradient Descent(483/999): loss=0.324820049994133\n",
      "Stochastic Gradient Descent(484/999): loss=0.3617223046874031\n",
      "Stochastic Gradient Descent(485/999): loss=0.2811378976641467\n",
      "Stochastic Gradient Descent(486/999): loss=0.4274946568383849\n",
      "Stochastic Gradient Descent(487/999): loss=0.3290827893506831\n",
      "Stochastic Gradient Descent(488/999): loss=0.40284881704713577\n",
      "Stochastic Gradient Descent(489/999): loss=0.2780809841260597\n",
      "Stochastic Gradient Descent(490/999): loss=0.3323367520542583\n",
      "Stochastic Gradient Descent(491/999): loss=0.32858245717627377\n",
      "Stochastic Gradient Descent(492/999): loss=0.48303457244189474\n",
      "Stochastic Gradient Descent(493/999): loss=0.3420069797837565\n",
      "Stochastic Gradient Descent(494/999): loss=0.3142516673928181\n",
      "Stochastic Gradient Descent(495/999): loss=0.2879072436008927\n",
      "Stochastic Gradient Descent(496/999): loss=0.3347955922686722\n",
      "Stochastic Gradient Descent(497/999): loss=0.3503030091849512\n",
      "Stochastic Gradient Descent(498/999): loss=0.37846954400614924\n",
      "Stochastic Gradient Descent(499/999): loss=0.5000869844928113\n",
      "Stochastic Gradient Descent(500/999): loss=0.37962376264267633\n",
      "Stochastic Gradient Descent(501/999): loss=0.2645483080953133\n",
      "Stochastic Gradient Descent(502/999): loss=0.32158385425330316\n",
      "Stochastic Gradient Descent(503/999): loss=0.3660618305701533\n",
      "Stochastic Gradient Descent(504/999): loss=0.3029195251812686\n",
      "Stochastic Gradient Descent(505/999): loss=0.4091314816803515\n",
      "Stochastic Gradient Descent(506/999): loss=0.43514005130147576\n",
      "Stochastic Gradient Descent(507/999): loss=0.38474376991349774\n",
      "Stochastic Gradient Descent(508/999): loss=0.3309640424372231\n",
      "Stochastic Gradient Descent(509/999): loss=0.36053057345864553\n",
      "Stochastic Gradient Descent(510/999): loss=0.3443418741800725\n",
      "Stochastic Gradient Descent(511/999): loss=0.39751511864739497\n",
      "Stochastic Gradient Descent(512/999): loss=0.3934586025006045\n",
      "Stochastic Gradient Descent(513/999): loss=0.39480410573802543\n",
      "Stochastic Gradient Descent(514/999): loss=0.300607661444471\n",
      "Stochastic Gradient Descent(515/999): loss=0.448096316089616\n",
      "Stochastic Gradient Descent(516/999): loss=0.2812353055164217\n",
      "Stochastic Gradient Descent(517/999): loss=0.3840236873227873\n",
      "Stochastic Gradient Descent(518/999): loss=0.33003370374645896\n",
      "Stochastic Gradient Descent(519/999): loss=0.3572086690614815\n",
      "Stochastic Gradient Descent(520/999): loss=0.32977818647222035\n",
      "Stochastic Gradient Descent(521/999): loss=0.28434569584061115\n",
      "Stochastic Gradient Descent(522/999): loss=0.3182347248063319\n",
      "Stochastic Gradient Descent(523/999): loss=0.39344053913652954\n",
      "Stochastic Gradient Descent(524/999): loss=0.3459983842487185\n",
      "Stochastic Gradient Descent(525/999): loss=0.4363405414487891\n",
      "Stochastic Gradient Descent(526/999): loss=0.33366032006061197\n",
      "Stochastic Gradient Descent(527/999): loss=0.3771418614449618\n",
      "Stochastic Gradient Descent(528/999): loss=0.2750766824677211\n",
      "Stochastic Gradient Descent(529/999): loss=0.27760149273080253\n",
      "Stochastic Gradient Descent(530/999): loss=0.37483546885268254\n",
      "Stochastic Gradient Descent(531/999): loss=0.28076925045707257\n",
      "Stochastic Gradient Descent(532/999): loss=0.22685836574407858\n",
      "Stochastic Gradient Descent(533/999): loss=0.39010114893651227\n",
      "Stochastic Gradient Descent(534/999): loss=0.3731175215190159\n",
      "Stochastic Gradient Descent(535/999): loss=0.3676742434117324\n",
      "Stochastic Gradient Descent(536/999): loss=0.38607441598875597\n",
      "Stochastic Gradient Descent(537/999): loss=0.40357453766239265\n",
      "Stochastic Gradient Descent(538/999): loss=0.3175809316771692\n",
      "Stochastic Gradient Descent(539/999): loss=0.2731098618205145\n",
      "Stochastic Gradient Descent(540/999): loss=0.4375598927912944\n",
      "Stochastic Gradient Descent(541/999): loss=0.37025991463502916\n",
      "Stochastic Gradient Descent(542/999): loss=0.33186281317151944\n",
      "Stochastic Gradient Descent(543/999): loss=0.27742589887651287\n",
      "Stochastic Gradient Descent(544/999): loss=0.3638023632149374\n",
      "Stochastic Gradient Descent(545/999): loss=0.3935579234409481\n",
      "Stochastic Gradient Descent(546/999): loss=0.3328345725041127\n",
      "Stochastic Gradient Descent(547/999): loss=0.336323644865493\n",
      "Stochastic Gradient Descent(548/999): loss=0.3770756048538822\n",
      "Stochastic Gradient Descent(549/999): loss=0.3638977369229559\n",
      "Stochastic Gradient Descent(550/999): loss=0.42666418395454053\n",
      "Stochastic Gradient Descent(551/999): loss=0.27749959855405576\n",
      "Stochastic Gradient Descent(552/999): loss=0.3622034118726875\n",
      "Stochastic Gradient Descent(553/999): loss=0.2747518113666471\n",
      "Stochastic Gradient Descent(554/999): loss=0.34618163397005164\n",
      "Stochastic Gradient Descent(555/999): loss=0.3350220531678881\n",
      "Stochastic Gradient Descent(556/999): loss=0.35843767585894015\n",
      "Stochastic Gradient Descent(557/999): loss=0.3375539509464564\n",
      "Stochastic Gradient Descent(558/999): loss=0.37010599285246104\n",
      "Stochastic Gradient Descent(559/999): loss=0.36316442707959096\n",
      "Stochastic Gradient Descent(560/999): loss=0.35830192848982106\n",
      "Stochastic Gradient Descent(561/999): loss=0.30775060983459235\n",
      "Stochastic Gradient Descent(562/999): loss=0.37350050161476583\n",
      "Stochastic Gradient Descent(563/999): loss=0.29476670116985715\n",
      "Stochastic Gradient Descent(564/999): loss=0.2929851536049714\n",
      "Stochastic Gradient Descent(565/999): loss=0.33677955531075876\n",
      "Stochastic Gradient Descent(566/999): loss=0.37064081489167355\n",
      "Stochastic Gradient Descent(567/999): loss=0.30448313113191094\n",
      "Stochastic Gradient Descent(568/999): loss=0.3162551785118239\n",
      "Stochastic Gradient Descent(569/999): loss=0.3106453486736385\n",
      "Stochastic Gradient Descent(570/999): loss=0.33270830412802377\n",
      "Stochastic Gradient Descent(571/999): loss=0.45089532191092\n",
      "Stochastic Gradient Descent(572/999): loss=0.3422863289961942\n",
      "Stochastic Gradient Descent(573/999): loss=0.4434136138199489\n",
      "Stochastic Gradient Descent(574/999): loss=0.3498292912008042\n",
      "Stochastic Gradient Descent(575/999): loss=0.33374852983324255\n",
      "Stochastic Gradient Descent(576/999): loss=0.3478134365797865\n",
      "Stochastic Gradient Descent(577/999): loss=0.3237509427285842\n",
      "Stochastic Gradient Descent(578/999): loss=0.3183073500969778\n",
      "Stochastic Gradient Descent(579/999): loss=0.3197712202316387\n",
      "Stochastic Gradient Descent(580/999): loss=0.41634565367090537\n",
      "Stochastic Gradient Descent(581/999): loss=0.3793608003793269\n",
      "Stochastic Gradient Descent(582/999): loss=0.34085998533077133\n",
      "Stochastic Gradient Descent(583/999): loss=0.3273566816595532\n",
      "Stochastic Gradient Descent(584/999): loss=0.34646632146309453\n",
      "Stochastic Gradient Descent(585/999): loss=0.27698602796952804\n",
      "Stochastic Gradient Descent(586/999): loss=0.31969737824481986\n",
      "Stochastic Gradient Descent(587/999): loss=0.30589730074605836\n",
      "Stochastic Gradient Descent(588/999): loss=0.3865363037688813\n",
      "Stochastic Gradient Descent(589/999): loss=0.3564795153020557\n",
      "Stochastic Gradient Descent(590/999): loss=0.3499433750291614\n",
      "Stochastic Gradient Descent(591/999): loss=0.3150960389607174\n",
      "Stochastic Gradient Descent(592/999): loss=0.3307735599400474\n",
      "Stochastic Gradient Descent(593/999): loss=0.3273444261488698\n",
      "Stochastic Gradient Descent(594/999): loss=0.3962114292952525\n",
      "Stochastic Gradient Descent(595/999): loss=0.301156531356335\n",
      "Stochastic Gradient Descent(596/999): loss=0.41431098986331866\n",
      "Stochastic Gradient Descent(597/999): loss=0.4052743707129961\n",
      "Stochastic Gradient Descent(598/999): loss=0.3232826262166505\n",
      "Stochastic Gradient Descent(599/999): loss=0.35683124265440597\n",
      "Stochastic Gradient Descent(600/999): loss=0.35286292407942704\n",
      "Stochastic Gradient Descent(601/999): loss=0.2945578110348191\n",
      "Stochastic Gradient Descent(602/999): loss=0.3954694701752873\n",
      "Stochastic Gradient Descent(603/999): loss=0.4121300359247549\n",
      "Stochastic Gradient Descent(604/999): loss=0.35133474261111\n",
      "Stochastic Gradient Descent(605/999): loss=0.3422696537176115\n",
      "Stochastic Gradient Descent(606/999): loss=0.31224567872253556\n",
      "Stochastic Gradient Descent(607/999): loss=0.37648603884831955\n",
      "Stochastic Gradient Descent(608/999): loss=0.27706011937869673\n",
      "Stochastic Gradient Descent(609/999): loss=0.38009809514576354\n",
      "Stochastic Gradient Descent(610/999): loss=0.384955489944646\n",
      "Stochastic Gradient Descent(611/999): loss=0.39031420204574974\n",
      "Stochastic Gradient Descent(612/999): loss=0.36384570449645465\n",
      "Stochastic Gradient Descent(613/999): loss=0.3202467665564683\n",
      "Stochastic Gradient Descent(614/999): loss=0.44324821492846583\n",
      "Stochastic Gradient Descent(615/999): loss=0.3685974715726224\n",
      "Stochastic Gradient Descent(616/999): loss=0.31351721540525895\n",
      "Stochastic Gradient Descent(617/999): loss=0.29139440692028007\n",
      "Stochastic Gradient Descent(618/999): loss=0.376263992659112\n",
      "Stochastic Gradient Descent(619/999): loss=0.33326581006874156\n",
      "Stochastic Gradient Descent(620/999): loss=0.32471154092801785\n",
      "Stochastic Gradient Descent(621/999): loss=0.4036615915365542\n",
      "Stochastic Gradient Descent(622/999): loss=0.3518671860405693\n",
      "Stochastic Gradient Descent(623/999): loss=0.3187922585401509\n",
      "Stochastic Gradient Descent(624/999): loss=0.3320918519385735\n",
      "Stochastic Gradient Descent(625/999): loss=0.31480120108039406\n",
      "Stochastic Gradient Descent(626/999): loss=0.32004057663763175\n",
      "Stochastic Gradient Descent(627/999): loss=0.3518485413825693\n",
      "Stochastic Gradient Descent(628/999): loss=0.4275802011047392\n",
      "Stochastic Gradient Descent(629/999): loss=0.3725084456811274\n",
      "Stochastic Gradient Descent(630/999): loss=0.38392858403998503\n",
      "Stochastic Gradient Descent(631/999): loss=0.24523508107614903\n",
      "Stochastic Gradient Descent(632/999): loss=0.3148516749054895\n",
      "Stochastic Gradient Descent(633/999): loss=0.286894860065423\n",
      "Stochastic Gradient Descent(634/999): loss=0.2936228085836465\n",
      "Stochastic Gradient Descent(635/999): loss=0.3091384892233466\n",
      "Stochastic Gradient Descent(636/999): loss=0.30398606292339414\n",
      "Stochastic Gradient Descent(637/999): loss=0.42147254481184226\n",
      "Stochastic Gradient Descent(638/999): loss=0.4107244918767378\n",
      "Stochastic Gradient Descent(639/999): loss=0.4079848969059623\n",
      "Stochastic Gradient Descent(640/999): loss=0.4339267998656816\n",
      "Stochastic Gradient Descent(641/999): loss=0.3571913720780982\n",
      "Stochastic Gradient Descent(642/999): loss=0.355364528216254\n",
      "Stochastic Gradient Descent(643/999): loss=0.41767547850542086\n",
      "Stochastic Gradient Descent(644/999): loss=0.37465427949016933\n",
      "Stochastic Gradient Descent(645/999): loss=0.3193372694554194\n",
      "Stochastic Gradient Descent(646/999): loss=0.39088259410700177\n",
      "Stochastic Gradient Descent(647/999): loss=0.3606666696326448\n",
      "Stochastic Gradient Descent(648/999): loss=0.3299192664699328\n",
      "Stochastic Gradient Descent(649/999): loss=0.3691393362313853\n",
      "Stochastic Gradient Descent(650/999): loss=0.4464422018784823\n",
      "Stochastic Gradient Descent(651/999): loss=0.2643776065422497\n",
      "Stochastic Gradient Descent(652/999): loss=0.35375477099831193\n",
      "Stochastic Gradient Descent(653/999): loss=0.30916457780093176\n",
      "Stochastic Gradient Descent(654/999): loss=0.3222867550950671\n",
      "Stochastic Gradient Descent(655/999): loss=0.37796788331902925\n",
      "Stochastic Gradient Descent(656/999): loss=0.33993340663060884\n",
      "Stochastic Gradient Descent(657/999): loss=0.35711314212610745\n",
      "Stochastic Gradient Descent(658/999): loss=0.38025328424410243\n",
      "Stochastic Gradient Descent(659/999): loss=0.35952057404946525\n",
      "Stochastic Gradient Descent(660/999): loss=0.3548361138659423\n",
      "Stochastic Gradient Descent(661/999): loss=0.37058171415361474\n",
      "Stochastic Gradient Descent(662/999): loss=0.4493680677669963\n",
      "Stochastic Gradient Descent(663/999): loss=0.3997130547128995\n",
      "Stochastic Gradient Descent(664/999): loss=0.314741427998395\n",
      "Stochastic Gradient Descent(665/999): loss=0.29326800894539234\n",
      "Stochastic Gradient Descent(666/999): loss=0.3153291223202012\n",
      "Stochastic Gradient Descent(667/999): loss=0.30274117137630346\n",
      "Stochastic Gradient Descent(668/999): loss=0.35831434482853913\n",
      "Stochastic Gradient Descent(669/999): loss=0.30314504120403424\n",
      "Stochastic Gradient Descent(670/999): loss=0.3436088114570996\n",
      "Stochastic Gradient Descent(671/999): loss=0.2632725053183528\n",
      "Stochastic Gradient Descent(672/999): loss=0.25101668290735524\n",
      "Stochastic Gradient Descent(673/999): loss=0.23900954826234241\n",
      "Stochastic Gradient Descent(674/999): loss=0.3532287062891346\n",
      "Stochastic Gradient Descent(675/999): loss=0.35724832107978505\n",
      "Stochastic Gradient Descent(676/999): loss=0.34809973550767354\n",
      "Stochastic Gradient Descent(677/999): loss=0.29847289339105987\n",
      "Stochastic Gradient Descent(678/999): loss=0.2911189065503048\n",
      "Stochastic Gradient Descent(679/999): loss=0.4119870097237579\n",
      "Stochastic Gradient Descent(680/999): loss=0.36323282587133354\n",
      "Stochastic Gradient Descent(681/999): loss=0.3523314529422514\n",
      "Stochastic Gradient Descent(682/999): loss=0.33057156461150194\n",
      "Stochastic Gradient Descent(683/999): loss=0.2796074569878631\n",
      "Stochastic Gradient Descent(684/999): loss=0.32986293240876163\n",
      "Stochastic Gradient Descent(685/999): loss=0.4104643418461407\n",
      "Stochastic Gradient Descent(686/999): loss=0.3973807309234318\n",
      "Stochastic Gradient Descent(687/999): loss=0.32431281284344665\n",
      "Stochastic Gradient Descent(688/999): loss=0.2950448855947327\n",
      "Stochastic Gradient Descent(689/999): loss=0.2764382785988598\n",
      "Stochastic Gradient Descent(690/999): loss=0.46229021018960503\n",
      "Stochastic Gradient Descent(691/999): loss=0.37301209038906913\n",
      "Stochastic Gradient Descent(692/999): loss=0.27082060905814087\n",
      "Stochastic Gradient Descent(693/999): loss=0.2717389099875881\n",
      "Stochastic Gradient Descent(694/999): loss=0.43342875093683864\n",
      "Stochastic Gradient Descent(695/999): loss=0.4038416779688101\n",
      "Stochastic Gradient Descent(696/999): loss=0.46590323236736053\n",
      "Stochastic Gradient Descent(697/999): loss=0.3822767009220304\n",
      "Stochastic Gradient Descent(698/999): loss=0.38617295926491574\n",
      "Stochastic Gradient Descent(699/999): loss=0.34211781539738495\n",
      "Stochastic Gradient Descent(700/999): loss=0.26060200681303414\n",
      "Stochastic Gradient Descent(701/999): loss=0.4143941250091967\n",
      "Stochastic Gradient Descent(702/999): loss=0.3817614642365381\n",
      "Stochastic Gradient Descent(703/999): loss=0.3895196546512804\n",
      "Stochastic Gradient Descent(704/999): loss=0.24216221203210828\n",
      "Stochastic Gradient Descent(705/999): loss=0.2796944138527039\n",
      "Stochastic Gradient Descent(706/999): loss=0.31400564500168804\n",
      "Stochastic Gradient Descent(707/999): loss=0.3363944699275001\n",
      "Stochastic Gradient Descent(708/999): loss=0.39518384392194406\n",
      "Stochastic Gradient Descent(709/999): loss=0.34703743537887255\n",
      "Stochastic Gradient Descent(710/999): loss=0.3724273201067939\n",
      "Stochastic Gradient Descent(711/999): loss=0.31063954888700956\n",
      "Stochastic Gradient Descent(712/999): loss=0.36427936968579944\n",
      "Stochastic Gradient Descent(713/999): loss=0.3465806819877972\n",
      "Stochastic Gradient Descent(714/999): loss=0.36324694978462846\n",
      "Stochastic Gradient Descent(715/999): loss=0.420982103230295\n",
      "Stochastic Gradient Descent(716/999): loss=0.3300899301834131\n",
      "Stochastic Gradient Descent(717/999): loss=0.3966110216143727\n",
      "Stochastic Gradient Descent(718/999): loss=0.3501479396401022\n",
      "Stochastic Gradient Descent(719/999): loss=0.46188400707810684\n",
      "Stochastic Gradient Descent(720/999): loss=0.33274636600690216\n",
      "Stochastic Gradient Descent(721/999): loss=0.37162394822453754\n",
      "Stochastic Gradient Descent(722/999): loss=0.41057126625835766\n",
      "Stochastic Gradient Descent(723/999): loss=0.33561005566474067\n",
      "Stochastic Gradient Descent(724/999): loss=0.38477932676888765\n",
      "Stochastic Gradient Descent(725/999): loss=0.3038329551127218\n",
      "Stochastic Gradient Descent(726/999): loss=0.30957856071358597\n",
      "Stochastic Gradient Descent(727/999): loss=0.3209546001052337\n",
      "Stochastic Gradient Descent(728/999): loss=0.27568663717560254\n",
      "Stochastic Gradient Descent(729/999): loss=0.41366083436384515\n",
      "Stochastic Gradient Descent(730/999): loss=0.28589871256116356\n",
      "Stochastic Gradient Descent(731/999): loss=0.44571980657890803\n",
      "Stochastic Gradient Descent(732/999): loss=0.3666827972722574\n",
      "Stochastic Gradient Descent(733/999): loss=0.3916646611372435\n",
      "Stochastic Gradient Descent(734/999): loss=0.35018840400223533\n",
      "Stochastic Gradient Descent(735/999): loss=0.4139475174281956\n",
      "Stochastic Gradient Descent(736/999): loss=0.3826073579109844\n",
      "Stochastic Gradient Descent(737/999): loss=0.39981919350383427\n",
      "Stochastic Gradient Descent(738/999): loss=0.3409392980898564\n",
      "Stochastic Gradient Descent(739/999): loss=0.39517876088062215\n",
      "Stochastic Gradient Descent(740/999): loss=0.2977584315570347\n",
      "Stochastic Gradient Descent(741/999): loss=0.3270966788847007\n",
      "Stochastic Gradient Descent(742/999): loss=0.2766549527870055\n",
      "Stochastic Gradient Descent(743/999): loss=0.371810390899724\n",
      "Stochastic Gradient Descent(744/999): loss=0.26204879706523465\n",
      "Stochastic Gradient Descent(745/999): loss=0.4120887511317461\n",
      "Stochastic Gradient Descent(746/999): loss=0.2705964154324129\n",
      "Stochastic Gradient Descent(747/999): loss=0.3305311947596371\n",
      "Stochastic Gradient Descent(748/999): loss=0.31650992027053265\n",
      "Stochastic Gradient Descent(749/999): loss=0.2631533282190331\n",
      "Stochastic Gradient Descent(750/999): loss=0.35993628048007503\n",
      "Stochastic Gradient Descent(751/999): loss=0.33114174371087546\n",
      "Stochastic Gradient Descent(752/999): loss=0.2891230464214739\n",
      "Stochastic Gradient Descent(753/999): loss=0.29653586272854116\n",
      "Stochastic Gradient Descent(754/999): loss=0.3309209360688311\n",
      "Stochastic Gradient Descent(755/999): loss=0.31105358429525287\n",
      "Stochastic Gradient Descent(756/999): loss=0.3159632381095154\n",
      "Stochastic Gradient Descent(757/999): loss=0.28864570403122314\n",
      "Stochastic Gradient Descent(758/999): loss=0.32185006615094025\n",
      "Stochastic Gradient Descent(759/999): loss=0.36466733153912734\n",
      "Stochastic Gradient Descent(760/999): loss=0.33766369993406464\n",
      "Stochastic Gradient Descent(761/999): loss=0.2499037520474252\n",
      "Stochastic Gradient Descent(762/999): loss=0.36408388755553117\n",
      "Stochastic Gradient Descent(763/999): loss=0.33001225046163923\n",
      "Stochastic Gradient Descent(764/999): loss=0.4471149444275446\n",
      "Stochastic Gradient Descent(765/999): loss=0.31562765451661307\n",
      "Stochastic Gradient Descent(766/999): loss=0.36742581562714094\n",
      "Stochastic Gradient Descent(767/999): loss=0.3396711552863641\n",
      "Stochastic Gradient Descent(768/999): loss=0.3679942163705068\n",
      "Stochastic Gradient Descent(769/999): loss=0.36982310053382655\n",
      "Stochastic Gradient Descent(770/999): loss=0.3140768155665483\n",
      "Stochastic Gradient Descent(771/999): loss=0.3357508120639161\n",
      "Stochastic Gradient Descent(772/999): loss=0.2893407742398399\n",
      "Stochastic Gradient Descent(773/999): loss=0.3002066290768354\n",
      "Stochastic Gradient Descent(774/999): loss=0.36602914638292217\n",
      "Stochastic Gradient Descent(775/999): loss=0.3069056806529641\n",
      "Stochastic Gradient Descent(776/999): loss=0.28611630617368516\n",
      "Stochastic Gradient Descent(777/999): loss=0.35108900549958555\n",
      "Stochastic Gradient Descent(778/999): loss=0.26802950361215533\n",
      "Stochastic Gradient Descent(779/999): loss=0.37727718244876535\n",
      "Stochastic Gradient Descent(780/999): loss=0.3984895803906997\n",
      "Stochastic Gradient Descent(781/999): loss=0.37885169966855003\n",
      "Stochastic Gradient Descent(782/999): loss=0.3788847950026886\n",
      "Stochastic Gradient Descent(783/999): loss=0.3086180993239599\n",
      "Stochastic Gradient Descent(784/999): loss=0.3088478787406366\n",
      "Stochastic Gradient Descent(785/999): loss=0.2547315912153365\n",
      "Stochastic Gradient Descent(786/999): loss=0.30827099537763636\n",
      "Stochastic Gradient Descent(787/999): loss=0.3675783726868537\n",
      "Stochastic Gradient Descent(788/999): loss=0.3424973028037477\n",
      "Stochastic Gradient Descent(789/999): loss=0.364706808076846\n",
      "Stochastic Gradient Descent(790/999): loss=0.4352496606870755\n",
      "Stochastic Gradient Descent(791/999): loss=0.4365164234716483\n",
      "Stochastic Gradient Descent(792/999): loss=0.40197336939767986\n",
      "Stochastic Gradient Descent(793/999): loss=0.3309551980383928\n",
      "Stochastic Gradient Descent(794/999): loss=0.2912892143703624\n",
      "Stochastic Gradient Descent(795/999): loss=0.3326647939848016\n",
      "Stochastic Gradient Descent(796/999): loss=0.3753812398767934\n",
      "Stochastic Gradient Descent(797/999): loss=0.36547150034602927\n",
      "Stochastic Gradient Descent(798/999): loss=0.3734895552332516\n",
      "Stochastic Gradient Descent(799/999): loss=0.40205312391360437\n",
      "Stochastic Gradient Descent(800/999): loss=0.38049886713221015\n",
      "Stochastic Gradient Descent(801/999): loss=0.2746244369411832\n",
      "Stochastic Gradient Descent(802/999): loss=0.3191432718266967\n",
      "Stochastic Gradient Descent(803/999): loss=0.4168693570204176\n",
      "Stochastic Gradient Descent(804/999): loss=0.4262890794856171\n",
      "Stochastic Gradient Descent(805/999): loss=0.44463193220839226\n",
      "Stochastic Gradient Descent(806/999): loss=0.3236428004777926\n",
      "Stochastic Gradient Descent(807/999): loss=0.4112708582241684\n",
      "Stochastic Gradient Descent(808/999): loss=0.2804032853781822\n",
      "Stochastic Gradient Descent(809/999): loss=0.21914368835115652\n",
      "Stochastic Gradient Descent(810/999): loss=0.3170394680124846\n",
      "Stochastic Gradient Descent(811/999): loss=0.35540719064226445\n",
      "Stochastic Gradient Descent(812/999): loss=0.31839479071793525\n",
      "Stochastic Gradient Descent(813/999): loss=0.31313953748642037\n",
      "Stochastic Gradient Descent(814/999): loss=0.3376607224417605\n",
      "Stochastic Gradient Descent(815/999): loss=0.3683179662567626\n",
      "Stochastic Gradient Descent(816/999): loss=0.34933064274104875\n",
      "Stochastic Gradient Descent(817/999): loss=0.35949570123839225\n",
      "Stochastic Gradient Descent(818/999): loss=0.3852881559700901\n",
      "Stochastic Gradient Descent(819/999): loss=0.3849723138729962\n",
      "Stochastic Gradient Descent(820/999): loss=0.29707179698142155\n",
      "Stochastic Gradient Descent(821/999): loss=0.37138227586369077\n",
      "Stochastic Gradient Descent(822/999): loss=0.2169491033498872\n",
      "Stochastic Gradient Descent(823/999): loss=0.3831912292011734\n",
      "Stochastic Gradient Descent(824/999): loss=0.29575238406558035\n",
      "Stochastic Gradient Descent(825/999): loss=0.3515333550124713\n",
      "Stochastic Gradient Descent(826/999): loss=0.34920886025843395\n",
      "Stochastic Gradient Descent(827/999): loss=0.3720053764787762\n",
      "Stochastic Gradient Descent(828/999): loss=0.3852430141337894\n",
      "Stochastic Gradient Descent(829/999): loss=0.28649104860459684\n",
      "Stochastic Gradient Descent(830/999): loss=0.36135827065128495\n",
      "Stochastic Gradient Descent(831/999): loss=0.4011999085777461\n",
      "Stochastic Gradient Descent(832/999): loss=0.3345047080113114\n",
      "Stochastic Gradient Descent(833/999): loss=0.33480250023471386\n",
      "Stochastic Gradient Descent(834/999): loss=0.41263083311268384\n",
      "Stochastic Gradient Descent(835/999): loss=0.35818499471434834\n",
      "Stochastic Gradient Descent(836/999): loss=0.4222605442107571\n",
      "Stochastic Gradient Descent(837/999): loss=0.37701219668397257\n",
      "Stochastic Gradient Descent(838/999): loss=0.3446416161944746\n",
      "Stochastic Gradient Descent(839/999): loss=0.3705335032849157\n",
      "Stochastic Gradient Descent(840/999): loss=0.33430096204787774\n",
      "Stochastic Gradient Descent(841/999): loss=0.3436410579688619\n",
      "Stochastic Gradient Descent(842/999): loss=0.2664259561487592\n",
      "Stochastic Gradient Descent(843/999): loss=0.310962091415124\n",
      "Stochastic Gradient Descent(844/999): loss=0.33937096565602076\n",
      "Stochastic Gradient Descent(845/999): loss=0.23799278450738015\n",
      "Stochastic Gradient Descent(846/999): loss=0.35888576149708795\n",
      "Stochastic Gradient Descent(847/999): loss=0.4298529788977819\n",
      "Stochastic Gradient Descent(848/999): loss=0.2950891930153221\n",
      "Stochastic Gradient Descent(849/999): loss=0.3062270428481399\n",
      "Stochastic Gradient Descent(850/999): loss=0.3060855027450026\n",
      "Stochastic Gradient Descent(851/999): loss=0.2755753535968173\n",
      "Stochastic Gradient Descent(852/999): loss=0.4069198365124202\n",
      "Stochastic Gradient Descent(853/999): loss=0.29871815872337\n",
      "Stochastic Gradient Descent(854/999): loss=0.29716633898618755\n",
      "Stochastic Gradient Descent(855/999): loss=0.27161470985964153\n",
      "Stochastic Gradient Descent(856/999): loss=0.3861223661844591\n",
      "Stochastic Gradient Descent(857/999): loss=0.3364924697343227\n",
      "Stochastic Gradient Descent(858/999): loss=0.36163748676560153\n",
      "Stochastic Gradient Descent(859/999): loss=0.3700378297192261\n",
      "Stochastic Gradient Descent(860/999): loss=0.26209733402355245\n",
      "Stochastic Gradient Descent(861/999): loss=0.26732478279202104\n",
      "Stochastic Gradient Descent(862/999): loss=0.43615337855541186\n",
      "Stochastic Gradient Descent(863/999): loss=0.2863103931852051\n",
      "Stochastic Gradient Descent(864/999): loss=0.47219226027873523\n",
      "Stochastic Gradient Descent(865/999): loss=0.3226701181240587\n",
      "Stochastic Gradient Descent(866/999): loss=0.3310459322644299\n",
      "Stochastic Gradient Descent(867/999): loss=0.36469730419171925\n",
      "Stochastic Gradient Descent(868/999): loss=0.3511890944033665\n",
      "Stochastic Gradient Descent(869/999): loss=0.3135781592526571\n",
      "Stochastic Gradient Descent(870/999): loss=0.30227687350366655\n",
      "Stochastic Gradient Descent(871/999): loss=0.3231533782973618\n",
      "Stochastic Gradient Descent(872/999): loss=0.32119663790215613\n",
      "Stochastic Gradient Descent(873/999): loss=0.3993567967945254\n",
      "Stochastic Gradient Descent(874/999): loss=0.3000157565648238\n",
      "Stochastic Gradient Descent(875/999): loss=0.3830732901234106\n",
      "Stochastic Gradient Descent(876/999): loss=0.3120119022441951\n",
      "Stochastic Gradient Descent(877/999): loss=0.44955546569146376\n",
      "Stochastic Gradient Descent(878/999): loss=0.43638033172377805\n",
      "Stochastic Gradient Descent(879/999): loss=0.37492380247684\n",
      "Stochastic Gradient Descent(880/999): loss=0.272860050331528\n",
      "Stochastic Gradient Descent(881/999): loss=0.3596130593067964\n",
      "Stochastic Gradient Descent(882/999): loss=0.314202290097399\n",
      "Stochastic Gradient Descent(883/999): loss=0.4605717418906759\n",
      "Stochastic Gradient Descent(884/999): loss=0.3235852091121736\n",
      "Stochastic Gradient Descent(885/999): loss=0.4193288342246276\n",
      "Stochastic Gradient Descent(886/999): loss=0.32880918039000073\n",
      "Stochastic Gradient Descent(887/999): loss=0.349685272439433\n",
      "Stochastic Gradient Descent(888/999): loss=0.2883265175081447\n",
      "Stochastic Gradient Descent(889/999): loss=0.3293699872530025\n",
      "Stochastic Gradient Descent(890/999): loss=0.40478701828501856\n",
      "Stochastic Gradient Descent(891/999): loss=0.31708454009594733\n",
      "Stochastic Gradient Descent(892/999): loss=0.31646071990467467\n",
      "Stochastic Gradient Descent(893/999): loss=0.38413645268036617\n",
      "Stochastic Gradient Descent(894/999): loss=0.3786201292088937\n",
      "Stochastic Gradient Descent(895/999): loss=0.40247117006897737\n",
      "Stochastic Gradient Descent(896/999): loss=0.30932288171207695\n",
      "Stochastic Gradient Descent(897/999): loss=0.3241778434484565\n",
      "Stochastic Gradient Descent(898/999): loss=0.4640554333826125\n",
      "Stochastic Gradient Descent(899/999): loss=0.313283791173777\n",
      "Stochastic Gradient Descent(900/999): loss=0.23328195368488105\n",
      "Stochastic Gradient Descent(901/999): loss=0.42381206383778725\n",
      "Stochastic Gradient Descent(902/999): loss=0.34773270063432404\n",
      "Stochastic Gradient Descent(903/999): loss=0.3635667812582721\n",
      "Stochastic Gradient Descent(904/999): loss=0.3496006261712145\n",
      "Stochastic Gradient Descent(905/999): loss=0.3697671094782881\n",
      "Stochastic Gradient Descent(906/999): loss=0.2601356507763553\n",
      "Stochastic Gradient Descent(907/999): loss=0.32134663648345346\n",
      "Stochastic Gradient Descent(908/999): loss=0.3872803172910681\n",
      "Stochastic Gradient Descent(909/999): loss=0.3564377390895114\n",
      "Stochastic Gradient Descent(910/999): loss=0.4509935207942679\n",
      "Stochastic Gradient Descent(911/999): loss=0.2768654676222203\n",
      "Stochastic Gradient Descent(912/999): loss=0.2876838013342266\n",
      "Stochastic Gradient Descent(913/999): loss=0.2563874642331631\n",
      "Stochastic Gradient Descent(914/999): loss=0.4108946423797494\n",
      "Stochastic Gradient Descent(915/999): loss=0.29571490259652955\n",
      "Stochastic Gradient Descent(916/999): loss=0.3161102461859767\n",
      "Stochastic Gradient Descent(917/999): loss=0.36119717451127875\n",
      "Stochastic Gradient Descent(918/999): loss=0.33057721594709094\n",
      "Stochastic Gradient Descent(919/999): loss=0.34656453520578234\n",
      "Stochastic Gradient Descent(920/999): loss=0.3164128271013907\n",
      "Stochastic Gradient Descent(921/999): loss=0.27024332929482325\n",
      "Stochastic Gradient Descent(922/999): loss=0.4374686289953751\n",
      "Stochastic Gradient Descent(923/999): loss=0.33735021927281666\n",
      "Stochastic Gradient Descent(924/999): loss=0.32248724030239684\n",
      "Stochastic Gradient Descent(925/999): loss=0.3608868255147202\n",
      "Stochastic Gradient Descent(926/999): loss=0.35986706013638037\n",
      "Stochastic Gradient Descent(927/999): loss=0.35468848961097743\n",
      "Stochastic Gradient Descent(928/999): loss=0.35315542732052874\n",
      "Stochastic Gradient Descent(929/999): loss=0.2698811359886848\n",
      "Stochastic Gradient Descent(930/999): loss=0.36633454992069436\n",
      "Stochastic Gradient Descent(931/999): loss=0.2870214044830618\n",
      "Stochastic Gradient Descent(932/999): loss=0.30752673516934925\n",
      "Stochastic Gradient Descent(933/999): loss=0.3873420333515625\n",
      "Stochastic Gradient Descent(934/999): loss=0.304107787141153\n",
      "Stochastic Gradient Descent(935/999): loss=0.35038801842734024\n",
      "Stochastic Gradient Descent(936/999): loss=0.38446887399158586\n",
      "Stochastic Gradient Descent(937/999): loss=0.3023660817625442\n",
      "Stochastic Gradient Descent(938/999): loss=0.22017660727796962\n",
      "Stochastic Gradient Descent(939/999): loss=0.32805054808512624\n",
      "Stochastic Gradient Descent(940/999): loss=0.3723727131734902\n",
      "Stochastic Gradient Descent(941/999): loss=0.3357592248380388\n",
      "Stochastic Gradient Descent(942/999): loss=0.37912527956981257\n",
      "Stochastic Gradient Descent(943/999): loss=0.34943470219138606\n",
      "Stochastic Gradient Descent(944/999): loss=0.38808920958768534\n",
      "Stochastic Gradient Descent(945/999): loss=0.35336482842482264\n",
      "Stochastic Gradient Descent(946/999): loss=0.4006750165823057\n",
      "Stochastic Gradient Descent(947/999): loss=0.3117830951144754\n",
      "Stochastic Gradient Descent(948/999): loss=0.3307341732868702\n",
      "Stochastic Gradient Descent(949/999): loss=0.23073697529933132\n",
      "Stochastic Gradient Descent(950/999): loss=0.3559444311750012\n",
      "Stochastic Gradient Descent(951/999): loss=0.38826318166308893\n",
      "Stochastic Gradient Descent(952/999): loss=0.3200831337015246\n",
      "Stochastic Gradient Descent(953/999): loss=0.33910692105738555\n",
      "Stochastic Gradient Descent(954/999): loss=0.36477372992472995\n",
      "Stochastic Gradient Descent(955/999): loss=0.3305321749253503\n",
      "Stochastic Gradient Descent(956/999): loss=0.37774435807721574\n",
      "Stochastic Gradient Descent(957/999): loss=0.27030807360307046\n",
      "Stochastic Gradient Descent(958/999): loss=0.3540175575576747\n",
      "Stochastic Gradient Descent(959/999): loss=0.34539581761065236\n",
      "Stochastic Gradient Descent(960/999): loss=0.2983171751051022\n",
      "Stochastic Gradient Descent(961/999): loss=0.41002280602574365\n",
      "Stochastic Gradient Descent(962/999): loss=0.31577483072822154\n",
      "Stochastic Gradient Descent(963/999): loss=0.3226239941196344\n",
      "Stochastic Gradient Descent(964/999): loss=0.2783667508942853\n",
      "Stochastic Gradient Descent(965/999): loss=0.35116494889501665\n",
      "Stochastic Gradient Descent(966/999): loss=0.34833073832273515\n",
      "Stochastic Gradient Descent(967/999): loss=0.310850113317502\n",
      "Stochastic Gradient Descent(968/999): loss=0.293624884281187\n",
      "Stochastic Gradient Descent(969/999): loss=0.3787100324287526\n",
      "Stochastic Gradient Descent(970/999): loss=0.25818404040341814\n",
      "Stochastic Gradient Descent(971/999): loss=0.3644596436919917\n",
      "Stochastic Gradient Descent(972/999): loss=0.40048523211007103\n",
      "Stochastic Gradient Descent(973/999): loss=0.31896040147191007\n",
      "Stochastic Gradient Descent(974/999): loss=0.2831793819880168\n",
      "Stochastic Gradient Descent(975/999): loss=0.29110428846016717\n",
      "Stochastic Gradient Descent(976/999): loss=0.3222253315884175\n",
      "Stochastic Gradient Descent(977/999): loss=0.2943636931673189\n",
      "Stochastic Gradient Descent(978/999): loss=0.2866301598950481\n",
      "Stochastic Gradient Descent(979/999): loss=0.32259388217014967\n",
      "Stochastic Gradient Descent(980/999): loss=0.27902452737742767\n",
      "Stochastic Gradient Descent(981/999): loss=0.4001337094739249\n",
      "Stochastic Gradient Descent(982/999): loss=0.38778806826435286\n",
      "Stochastic Gradient Descent(983/999): loss=0.4479442451357361\n",
      "Stochastic Gradient Descent(984/999): loss=0.32305549957509055\n",
      "Stochastic Gradient Descent(985/999): loss=0.4316114183568295\n",
      "Stochastic Gradient Descent(986/999): loss=0.34322089648795556\n",
      "Stochastic Gradient Descent(987/999): loss=0.26428172980440257\n",
      "Stochastic Gradient Descent(988/999): loss=0.42161458966119303\n",
      "Stochastic Gradient Descent(989/999): loss=0.3253988207457211\n",
      "Stochastic Gradient Descent(990/999): loss=0.3407871003803101\n",
      "Stochastic Gradient Descent(991/999): loss=0.38780727743985005\n",
      "Stochastic Gradient Descent(992/999): loss=0.39257597911183884\n",
      "Stochastic Gradient Descent(993/999): loss=0.42657263933664924\n",
      "Stochastic Gradient Descent(994/999): loss=0.39593544971215877\n",
      "Stochastic Gradient Descent(995/999): loss=0.23661406378881408\n",
      "Stochastic Gradient Descent(996/999): loss=0.36565837310441857\n",
      "Stochastic Gradient Descent(997/999): loss=0.3555111312140267\n",
      "Stochastic Gradient Descent(998/999): loss=0.36195301182921524\n",
      "Stochastic Gradient Descent(999/999): loss=0.40465699545248524\n",
      "[-0.30617627  0.04205927 -0.25050055 -0.18800911  0.02527486 -0.01104871\n",
      "  0.2604996  -0.01673776  0.19723445 -0.0019036   0.00300594 -0.11465057\n",
      "  0.11414396 -0.01253902  0.19968456 -0.01216832 -0.00501776  0.1780927\n",
      " -0.00210418  0.00974789  0.0905144   0.00334646 -0.05374834 -0.09860111\n",
      "  0.0357387   0.03725472  0.03757144 -0.01633675 -0.01284768 -0.0131498\n",
      " -0.08220261]\n"
     ]
    }
   ],
   "source": [
    "from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "batch_size = 50\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "# start_time = datetime.datetime.now()\n",
    "stoch_gradient_losses, stoch_gradient_ws = stochastic_gradient_descent(y, tX, w_initial, batch_size, max_iters, gamma)\n",
    "# end_time = datetime.datetime.now()\n",
    "\n",
    "print(stoch_gradient_ws[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.339445598499\n",
      "[ -3.14664000e-01   2.93790611e-02  -2.52531197e-01  -2.54790177e-01\n",
      "  -3.03690297e-02  -1.40091635e+00   2.95716877e-01  -1.07873855e+01\n",
      "   2.67880203e-01  -2.44944270e-03  -3.28831618e+02  -1.82647939e-01\n",
      "   1.14039609e-01   2.05024908e+01   6.38861252e+01  -3.18964649e-04\n",
      "  -1.80884217e-03   6.29952650e+01  -4.48641572e-04   1.54379298e-03\n",
      "   1.21462700e-01   3.95268716e-04  -6.33223476e-02  -2.06747093e-01\n",
      "  -1.16655766e-01   9.86256504e-02   1.67907699e-01  -3.35146241e-02\n",
      "  -2.98358674e+00  -5.36388102e+00   2.78482381e+02]\n"
     ]
    }
   ],
   "source": [
    "from least_squares import *\n",
    "\n",
    "# start_ls_time = datetime.datetime.now()\n",
    "ls_wopt, ls_loss = least_squares(y,tX)\n",
    "print(ls_loss)\n",
    "print(ls_wopt)\n",
    "# end_ls_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.30849412  0.03572402 -0.24077855 -0.21610016 -0.01054923 -0.01919922\n",
      "  0.34814992 -0.03018983  0.23504977 -0.01013674 -0.00087247 -0.15674991\n",
      "  0.11455458 -0.02161131  0.18503225 -0.00076269 -0.00130455  0.23850622\n",
      " -0.00086979  0.00249252  0.10383837  0.00113401 -0.06198121 -0.14991608\n",
      "  0.03645409  0.04367213  0.0436772  -0.02328488 -0.02309077 -0.0234825\n",
      " -0.09703028]\n"
     ]
    }
   ],
   "source": [
    "from ridge_regression import ridge_regression\n",
    "\n",
    "# lambdas = np.logspace(-3, 1, 10)      \n",
    "# _x = build_poly(x, degree)\n",
    "# x_train, x_test, y_train, y_test = split_data(tX, y, ratio, seed)\n",
    "    \n",
    "#     for lamb in lambdas:\n",
    "\n",
    "w_ridge = ridge_regression(y, tX, 0.01)\n",
    "\n",
    "print(w_ridge)\n",
    "\n",
    "# rmse_tr = np.sqrt(2*compute_loss(y, tX, w_ridge))\n",
    "# rmse_te = np.sqrt(2*compute_loss(y, tX, w_ridge))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = \"../Data/test.csv\" # TODO: download test data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
