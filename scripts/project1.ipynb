{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCML Project-1 ~ Team #60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from costs import compute_loss\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "\n",
    "DATA_TRAIN_PATH = \"../Data/train.csv\" # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "#print the shape of the offset x matrix.\n",
    "print('original tX shape: ',tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized tX shape:  (250000, 31)\n",
      "tX mean shape:  (30,)\n",
      "tX std shape:  (30,)\n"
     ]
    }
   ],
   "source": [
    "#standardization\n",
    "tX, mean_x, std_x = standardize(tX, mean_x=None, std_x=None)\n",
    "print('standardized tX shape: ',tX.shape)\n",
    "print('tX mean shape: ',mean_x.shape)\n",
    "print('tX std shape: ',std_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of output y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Histogram of output y\n",
    "plt.hist(y, bins=10, align='mid')\n",
    "plt.title(\"Histogram of output y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of y as a function of all its features (one by one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Analyse y as a function of all the other features (one by one)\n",
    "for feature in range(31):\n",
    "    print('feature: ',feature)\n",
    "    plt.scatter(tX[:,feature], y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.5\n",
      "Gradient Descent(1/999): loss=0.4930403211757793\n",
      "Gradient Descent(2/999): loss=0.4870098501022072\n",
      "Gradient Descent(3/999): loss=0.4817168268664753\n",
      "Gradient Descent(4/999): loss=0.4770128047663221\n",
      "Gradient Descent(5/999): loss=0.4727827104481938\n",
      "Gradient Descent(6/999): loss=0.4689371916527971\n",
      "Gradient Descent(7/999): loss=0.4654067258061777\n",
      "Gradient Descent(8/999): loss=0.4621370840030196\n",
      "Gradient Descent(9/999): loss=0.45908583830059513\n",
      "Gradient Descent(10/999): loss=0.45621967211044767\n",
      "Gradient Descent(11/999): loss=0.4535123087925472\n",
      "Gradient Descent(12/999): loss=0.45094291613495796\n",
      "Gradient Descent(13/999): loss=0.4484948771748208\n",
      "Gradient Descent(14/999): loss=0.4461548430418665\n",
      "Gradient Descent(15/999): loss=0.4439120029218537\n",
      "Gradient Descent(16/999): loss=0.4417575211821269\n",
      "Gradient Descent(17/999): loss=0.4396841032046329\n",
      "Gradient Descent(18/999): loss=0.43768566032584444\n",
      "Gradient Descent(19/999): loss=0.43575705109814233\n",
      "Gradient Descent(20/999): loss=0.4338938813329179\n",
      "Gradient Descent(21/999): loss=0.4320923494233507\n",
      "Gradient Descent(22/999): loss=0.4303491265527386\n",
      "Gradient Descent(23/999): loss=0.4286612637864871\n",
      "Gradient Descent(24/999): loss=0.4270261198872494\n",
      "Gradient Descent(25/999): loss=0.42544130511008876\n",
      "Gradient Descent(26/999): loss=0.42390463732556116\n",
      "Gradient Descent(27/999): loss=0.4224141076584347\n",
      "Gradient Descent(28/999): loss=0.4209678534762368\n",
      "Gradient Descent(29/999): loss=0.4195641370594624\n",
      "Gradient Descent(30/999): loss=0.4182013286683655\n",
      "Gradient Descent(31/999): loss=0.41687789301616646\n",
      "Gradient Descent(32/999): loss=0.4155923783855599\n",
      "Gradient Descent(33/999): loss=0.4143434078001904\n",
      "Gradient Descent(34/999): loss=0.41312967179736826\n",
      "Gradient Descent(35/999): loss=0.4119499224519139\n",
      "Gradient Descent(36/999): loss=0.4108029683808321\n",
      "Gradient Descent(37/999): loss=0.4096876705199781\n",
      "Gradient Descent(38/999): loss=0.4086029385112189\n",
      "Gradient Descent(39/999): loss=0.4075477275750668\n",
      "Gradient Descent(40/999): loss=0.40652103577187293\n",
      "Gradient Descent(41/999): loss=0.4055219015763233\n",
      "Gradient Descent(42/999): loss=0.4045494017066949\n",
      "Gradient Descent(43/999): loss=0.4036026491632055\n",
      "Gradient Descent(44/999): loss=0.4026807914397397\n",
      "Gradient Descent(45/999): loss=0.40178300888091195\n",
      "Gradient Descent(46/999): loss=0.4009085131623653\n",
      "Gradient Descent(47/999): loss=0.40005654587679157\n",
      "Gradient Descent(48/999): loss=0.3992263772117204\n",
      "Gradient Descent(49/999): loss=0.3984173047078789\n",
      "Gradient Descent(50/999): loss=0.3976286520890715\n",
      "Gradient Descent(51/999): loss=0.3968597681561937\n",
      "Gradient Descent(52/999): loss=0.3961100257393025\n",
      "Gradient Descent(53/999): loss=0.3953788207026833\n",
      "Gradient Descent(54/999): loss=0.3946655709986549\n",
      "Gradient Descent(55/999): loss=0.3939697157664929\n",
      "Gradient Descent(56/999): loss=0.3932907144733497\n",
      "Gradient Descent(57/999): loss=0.3926280460944537\n",
      "Gradient Descent(58/999): loss=0.39198120833019573\n",
      "Gradient Descent(59/999): loss=0.39134971685796766\n",
      "Gradient Descent(60/999): loss=0.3907331046168399\n",
      "Gradient Descent(61/999): loss=0.3901309211233352\n",
      "Gradient Descent(62/999): loss=0.38954273181671084\n",
      "Gradient Descent(63/999): loss=0.38896811743228027\n",
      "Gradient Descent(64/999): loss=0.38840667340141793\n",
      "Gradient Descent(65/999): loss=0.3878580092769802\n",
      "Gradient Descent(66/999): loss=0.3873217481829583\n",
      "Gradient Descent(67/999): loss=0.3867975262872486\n",
      "Gradient Descent(68/999): loss=0.3862849922964908\n",
      "Gradient Descent(69/999): loss=0.38578380697198617\n",
      "Gradient Descent(70/999): loss=0.3852936426657501\n",
      "Gradient Descent(71/999): loss=0.38481418287581554\n",
      "Gradient Descent(72/999): loss=0.38434512181993413\n",
      "Gradient Descent(73/999): loss=0.3838861640268767\n",
      "Gradient Descent(74/999): loss=0.3834370239445601\n",
      "Gradient Descent(75/999): loss=0.3829974255642747\n",
      "Gradient Descent(76/999): loss=0.38256710206031536\n",
      "Gradient Descent(77/999): loss=0.382145795444348\n",
      "Gradient Descent(78/999): loss=0.3817332562338833\n",
      "Gradient Descent(79/999): loss=0.38132924313424604\n",
      "Gradient Descent(80/999): loss=0.38093352273346764\n",
      "Gradient Descent(81/999): loss=0.38054586920954475\n",
      "Gradient Descent(82/999): loss=0.38016606404953845\n",
      "Gradient Descent(83/999): loss=0.37979389578000927\n",
      "Gradient Descent(84/999): loss=0.379429159708306\n",
      "Gradient Descent(85/999): loss=0.3790716576742464\n",
      "Gradient Descent(86/999): loss=0.3787211978117502\n",
      "Gradient Descent(87/999): loss=0.37837759432000334\n",
      "Gradient Descent(88/999): loss=0.37804066724374963\n",
      "Gradient Descent(89/999): loss=0.37771024226232447\n",
      "Gradient Descent(90/999): loss=0.3773861504870649\n",
      "Gradient Descent(91/999): loss=0.37706822826673775\n",
      "Gradient Descent(92/999): loss=0.37675631700065787\n",
      "Gradient Descent(93/999): loss=0.3764502629591642\n",
      "Gradient Descent(94/999): loss=0.376149917111152\n",
      "Gradient Descent(95/999): loss=0.37585513495836603\n",
      "Gradient Descent(96/999): loss=0.37556577637616756\n",
      "Gradient Descent(97/999): loss=0.3752817054605107\n",
      "Gradient Descent(98/999): loss=0.37500279038086753\n",
      "Gradient Descent(99/999): loss=0.3747289032388546\n",
      "Gradient Descent(100/999): loss=0.3744599199323246\n",
      "Gradient Descent(101/999): loss=0.3741957200246976\n",
      "Gradient Descent(102/999): loss=0.3739361866193146\n",
      "Gradient Descent(103/999): loss=0.37368120623860707\n",
      "Gradient Descent(104/999): loss=0.373430668707883\n",
      "Gradient Descent(105/999): loss=0.37318446704354036\n",
      "Gradient Descent(106/999): loss=0.3729424973455276\n",
      "Gradient Descent(107/999): loss=0.3727046586938736\n",
      "Gradient Descent(108/999): loss=0.3724708530491261\n",
      "Gradient Descent(109/999): loss=0.3722409851565345\n",
      "Gradient Descent(110/999): loss=0.3720149624538261\n",
      "Gradient Descent(111/999): loss=0.3717926949824323\n",
      "Gradient Descent(112/999): loss=0.3715740953020202\n",
      "Gradient Descent(113/999): loss=0.37135907840820054\n",
      "Gradient Descent(114/999): loss=0.3711475616532802\n",
      "Gradient Descent(115/999): loss=0.37093946466993916\n",
      "Gradient Descent(116/999): loss=0.370734709297712\n",
      "Gradient Descent(117/999): loss=0.37053321951216506\n",
      "Gradient Descent(118/999): loss=0.3703349213566558\n",
      "Gradient Descent(119/999): loss=0.37013974287657847\n",
      "Gradient Descent(120/999): loss=0.3699476140559895\n",
      "Gradient Descent(121/999): loss=0.3697584667565246\n",
      "Gradient Descent(122/999): loss=0.3695722346585125\n",
      "Gradient Descent(123/999): loss=0.36938885320419956\n",
      "Gradient Descent(124/999): loss=0.36920825954300374\n",
      "Gradient Descent(125/999): loss=0.3690303924787152\n",
      "Gradient Descent(126/999): loss=0.368855192418569\n",
      "Gradient Descent(127/999): loss=0.368682601324117\n",
      "Gradient Descent(128/999): loss=0.3685125626638262\n",
      "Gradient Descent(129/999): loss=0.36834502136734015\n",
      "Gradient Descent(130/999): loss=0.3681799237813348\n",
      "Gradient Descent(131/999): loss=0.3680172176269095\n",
      "Gradient Descent(132/999): loss=0.3678568519584542\n",
      "Gradient Descent(133/999): loss=0.3676987771239343\n",
      "Gradient Descent(134/999): loss=0.36754294472653914\n",
      "Gradient Descent(135/999): loss=0.3673893075876433\n",
      "Gradient Descent(136/999): loss=0.36723781971102837\n",
      "Gradient Descent(137/999): loss=0.3670884362483197\n",
      "Gradient Descent(138/999): loss=0.3669411134655903\n",
      "Gradient Descent(139/999): loss=0.36679580871108647\n",
      "Gradient Descent(140/999): loss=0.36665248038403814\n",
      "Gradient Descent(141/999): loss=0.36651108790450554\n",
      "Gradient Descent(142/999): loss=0.36637159168423006\n",
      "Gradient Descent(143/999): loss=0.36623395309844803\n",
      "Gradient Descent(144/999): loss=0.3660981344586321\n",
      "Gradient Descent(145/999): loss=0.3659640989861277\n",
      "Gradient Descent(146/999): loss=0.36583181078664845\n",
      "Gradient Descent(147/999): loss=0.36570123482560074\n",
      "Gradient Descent(148/999): loss=0.3655723369042077\n",
      "Gradient Descent(149/999): loss=0.36544508363640094\n",
      "Gradient Descent(150/999): loss=0.3653194424264537\n",
      "Gradient Descent(151/999): loss=0.36519538144732827\n",
      "Gradient Descent(152/999): loss=0.3650728696197119\n",
      "Gradient Descent(153/999): loss=0.36495187659171485\n",
      "Gradient Descent(154/999): loss=0.3648323727192085\n",
      "Gradient Descent(155/999): loss=0.36471432904678064\n",
      "Gradient Descent(156/999): loss=0.3645977172892844\n",
      "Gradient Descent(157/999): loss=0.36448250981395985\n",
      "Gradient Descent(158/999): loss=0.3643686796231107\n",
      "Gradient Descent(159/999): loss=0.3642562003373118\n",
      "Gradient Descent(160/999): loss=0.3641450461791334\n",
      "Gradient Descent(161/999): loss=0.3640351919573606\n",
      "Gradient Descent(162/999): loss=0.3639266130516928\n",
      "Gradient Descent(163/999): loss=0.36381928539790576\n",
      "Gradient Descent(164/999): loss=0.3637131854734592\n",
      "Gradient Descent(165/999): loss=0.36360829028353725\n",
      "Gradient Descent(166/999): loss=0.363504577347503\n",
      "Gradient Descent(167/999): loss=0.36340202468575744\n",
      "Gradient Descent(168/999): loss=0.3633006108069846\n",
      "Gradient Descent(169/999): loss=0.36320031469577313\n",
      "Gradient Descent(170/999): loss=0.36310111580059984\n",
      "Gradient Descent(171/999): loss=0.36300299402216324\n",
      "Gradient Descent(172/999): loss=0.3629059297020566\n",
      "Gradient Descent(173/999): loss=0.36280990361176635\n",
      "Gradient Descent(174/999): loss=0.36271489694198844\n",
      "Gradient Descent(175/999): loss=0.36262089129224945\n",
      "Gradient Descent(176/999): loss=0.3625278686608239\n",
      "Gradient Descent(177/999): loss=0.3624358114349372\n",
      "Gradient Descent(178/999): loss=0.36234470238124566\n",
      "Gradient Descent(179/999): loss=0.3622545246365834\n",
      "Gradient Descent(180/999): loss=0.36216526169896857\n",
      "Gradient Descent(181/999): loss=0.36207689741886084\n",
      "Gradient Descent(182/999): loss=0.3619894159906598\n",
      "Gradient Descent(183/999): loss=0.3619028019444396\n",
      "Gradient Descent(184/999): loss=0.36181704013790955\n",
      "Gradient Descent(185/999): loss=0.3617321157485961\n",
      "Gradient Descent(186/999): loss=0.36164801426623705\n",
      "Gradient Descent(187/999): loss=0.361564721485382\n",
      "Gradient Descent(188/999): loss=0.36148222349819314\n",
      "Gradient Descent(189/999): loss=0.36140050668744006\n",
      "Gradient Descent(190/999): loss=0.36131955771968066\n",
      "Gradient Descent(191/999): loss=0.36123936353862673\n",
      "Gradient Descent(192/999): loss=0.3611599113586822\n",
      "Gradient Descent(193/999): loss=0.3610811886586557\n",
      "Gradient Descent(194/999): loss=0.36100318317563623\n",
      "Gradient Descent(195/999): loss=0.360925882899031\n",
      "Gradient Descent(196/999): loss=0.36084927606475836\n",
      "Gradient Descent(197/999): loss=0.36077335114959186\n",
      "Gradient Descent(198/999): loss=0.3606980968656512\n",
      "Gradient Descent(199/999): loss=0.3606235021550349\n",
      "Gradient Descent(200/999): loss=0.36054955618459134\n",
      "Gradient Descent(201/999): loss=0.3604762483408235\n",
      "Gradient Descent(202/999): loss=0.36040356822492364\n",
      "Gradient Descent(203/999): loss=0.3603315056479341\n",
      "Gradient Descent(204/999): loss=0.360260050626031\n",
      "Gradient Descent(205/999): loss=0.36018919337592575\n",
      "Gradient Descent(206/999): loss=0.36011892431038384\n",
      "Gradient Descent(207/999): loss=0.360049234033853\n",
      "Gradient Descent(208/999): loss=0.3599801133382045\n",
      "Gradient Descent(209/999): loss=0.35991155319857493\n",
      "Gradient Descent(210/999): loss=0.3598435447693159\n",
      "Gradient Descent(211/999): loss=0.3597760793800403\n",
      "Gradient Descent(212/999): loss=0.3597091485317665\n",
      "Gradient Descent(213/999): loss=0.3596427438931587\n",
      "Gradient Descent(214/999): loss=0.3595768572968554\n",
      "Gradient Descent(215/999): loss=0.359511480735891\n",
      "Gradient Descent(216/999): loss=0.3594466063602014\n",
      "Gradient Descent(217/999): loss=0.3593822264732148\n",
      "Gradient Descent(218/999): loss=0.35931833352852505\n",
      "Gradient Descent(219/999): loss=0.35925492012664373\n",
      "Gradient Descent(220/999): loss=0.3591919790118313\n",
      "Gradient Descent(221/999): loss=0.35912950306900165\n",
      "Gradient Descent(222/999): loss=0.359067485320703\n",
      "Gradient Descent(223/999): loss=0.35900591892416667\n",
      "Gradient Descent(224/999): loss=0.35894479716842903\n",
      "Gradient Descent(225/999): loss=0.3588841134715191\n",
      "Gradient Descent(226/999): loss=0.3588238613777123\n",
      "Gradient Descent(227/999): loss=0.3587640345548489\n",
      "Gradient Descent(228/999): loss=0.3587046267917148\n",
      "Gradient Descent(229/999): loss=0.35864563199548294\n",
      "Gradient Descent(230/999): loss=0.3585870441892144\n",
      "Gradient Descent(231/999): loss=0.358528857509417\n",
      "Gradient Descent(232/999): loss=0.3584710662036595\n",
      "Gradient Descent(233/999): loss=0.35841366462824226\n",
      "Gradient Descent(234/999): loss=0.3583566472459196\n",
      "Gradient Descent(235/999): loss=0.3583000086236748\n",
      "Gradient Descent(236/999): loss=0.35824374343054627\n",
      "Gradient Descent(237/999): loss=0.3581878464355023\n",
      "Gradient Descent(238/999): loss=0.3581323125053643\n",
      "Gradient Descent(239/999): loss=0.35807713660277707\n",
      "Gradient Descent(240/999): loss=0.35802231378422483\n",
      "Gradient Descent(241/999): loss=0.35796783919809194\n",
      "Gradient Descent(242/999): loss=0.35791370808276646\n",
      "Gradient Descent(243/999): loss=0.3578599157647867\n",
      "Gradient Descent(244/999): loss=0.3578064576570296\n",
      "Gradient Descent(245/999): loss=0.3577533292569382\n",
      "Gradient Descent(246/999): loss=0.35770052614478975\n",
      "Gradient Descent(247/999): loss=0.35764804398200095\n",
      "Gradient Descent(248/999): loss=0.35759587850947183\n",
      "Gradient Descent(249/999): loss=0.3575440255459655\n",
      "Gradient Descent(250/999): loss=0.35749248098652353\n",
      "Gradient Descent(251/999): loss=0.35744124080091616\n",
      "Gradient Descent(252/999): loss=0.3573903010321269\n",
      "Gradient Descent(253/999): loss=0.35733965779486965\n",
      "Gradient Descent(254/999): loss=0.3572893072741385\n",
      "Gradient Descent(255/999): loss=0.35723924572378896\n",
      "Gradient Descent(256/999): loss=0.35718946946514973\n",
      "Gradient Descent(257/999): loss=0.35713997488566607\n",
      "Gradient Descent(258/999): loss=0.3570907584375692\n",
      "Gradient Descent(259/999): loss=0.3570418166365786\n",
      "Gradient Descent(260/999): loss=0.3569931460606292\n",
      "Gradient Descent(261/999): loss=0.35694474334862736\n",
      "Gradient Descent(262/999): loss=0.35689660519923194\n",
      "Gradient Descent(263/999): loss=0.3568487283696644\n",
      "Gradient Descent(264/999): loss=0.35680110967454143\n",
      "Gradient Descent(265/999): loss=0.35675374598473397\n",
      "Gradient Descent(266/999): loss=0.35670663422625015\n",
      "Gradient Descent(267/999): loss=0.3566597713791422\n",
      "Gradient Descent(268/999): loss=0.35661315447643593\n",
      "Gradient Descent(269/999): loss=0.3565667806030839\n",
      "Gradient Descent(270/999): loss=0.3565206468949402\n",
      "Gradient Descent(271/999): loss=0.35647475053775607\n",
      "Gradient Descent(272/999): loss=0.35642908876619833\n",
      "Gradient Descent(273/999): loss=0.3563836588628875\n",
      "Gradient Descent(274/999): loss=0.3563384581574552\n",
      "Gradient Descent(275/999): loss=0.35629348402562394\n",
      "Gradient Descent(276/999): loss=0.3562487338883035\n",
      "Gradient Descent(277/999): loss=0.3562042052107074\n",
      "Gradient Descent(278/999): loss=0.35615989550148786\n",
      "Gradient Descent(279/999): loss=0.35611580231188905\n",
      "Gradient Descent(280/999): loss=0.35607192323491677\n",
      "Gradient Descent(281/999): loss=0.35602825590452614\n",
      "Gradient Descent(282/999): loss=0.35598479799482746\n",
      "Gradient Descent(283/999): loss=0.3559415472193049\n",
      "Gradient Descent(284/999): loss=0.35589850133005607\n",
      "Gradient Descent(285/999): loss=0.35585565811704317\n",
      "Gradient Descent(286/999): loss=0.35581301540736215\n",
      "Gradient Descent(287/999): loss=0.35577057106452603\n",
      "Gradient Descent(288/999): loss=0.3557283229877625\n",
      "Gradient Descent(289/999): loss=0.35568626911132706\n",
      "Gradient Descent(290/999): loss=0.3556444074038291\n",
      "Gradient Descent(291/999): loss=0.3556027358675728\n",
      "Gradient Descent(292/999): loss=0.35556125253791093\n",
      "Gradient Descent(293/999): loss=0.3555199554826118\n",
      "Gradient Descent(294/999): loss=0.3554788428012393\n",
      "Gradient Descent(295/999): loss=0.35543791262454616\n",
      "Gradient Descent(296/999): loss=0.35539716311387776\n",
      "Gradient Descent(297/999): loss=0.35535659246059076\n",
      "Gradient Descent(298/999): loss=0.3553161988854811\n",
      "Gradient Descent(299/999): loss=0.35527598063822463\n",
      "Gradient Descent(300/999): loss=0.3552359359968292\n",
      "Gradient Descent(301/999): loss=0.35519606326709785\n",
      "Gradient Descent(302/999): loss=0.35515636078210233\n",
      "Gradient Descent(303/999): loss=0.3551168269016672\n",
      "Gradient Descent(304/999): loss=0.35507746001186546\n",
      "Gradient Descent(305/999): loss=0.35503825852452336\n",
      "Gradient Descent(306/999): loss=0.35499922087673463\n",
      "Gradient Descent(307/999): loss=0.35496034553038647\n",
      "Gradient Descent(308/999): loss=0.3549216309716924\n",
      "Gradient Descent(309/999): loss=0.35488307571073713\n",
      "Gradient Descent(310/999): loss=0.3548446782810285\n",
      "Gradient Descent(311/999): loss=0.3548064372390596\n",
      "Gradient Descent(312/999): loss=0.3547683511638784\n",
      "Gradient Descent(313/999): loss=0.3547304186566685\n",
      "Gradient Descent(314/999): loss=0.35469263834033454\n",
      "Gradient Descent(315/999): loss=0.3546550088590989\n",
      "Gradient Descent(316/999): loss=0.3546175288781055\n",
      "Gradient Descent(317/999): loss=0.35458019708303123\n",
      "Gradient Descent(318/999): loss=0.3545430121797042\n",
      "Gradient Descent(319/999): loss=0.3545059728937328\n",
      "Gradient Descent(320/999): loss=0.3544690779701374\n",
      "Gradient Descent(321/999): loss=0.3544323261729936\n",
      "Gradient Descent(322/999): loss=0.3543957162850801\n",
      "Gradient Descent(323/999): loss=0.3543592471075344\n",
      "Gradient Descent(324/999): loss=0.35432291745951505\n",
      "Gradient Descent(325/999): loss=0.3542867261778701\n",
      "Gradient Descent(326/999): loss=0.3542506721168139\n",
      "Gradient Descent(327/999): loss=0.35421475414760745\n",
      "Gradient Descent(328/999): loss=0.35417897115824787\n",
      "Gradient Descent(329/999): loss=0.35414332205316146\n",
      "Gradient Descent(330/999): loss=0.3541078057529047\n",
      "Gradient Descent(331/999): loss=0.35407242119387083\n",
      "Gradient Descent(332/999): loss=0.3540371673280007\n",
      "Gradient Descent(333/999): loss=0.3540020431225006\n",
      "Gradient Descent(334/999): loss=0.3539670475595658\n",
      "Gradient Descent(335/999): loss=0.35393217963610896\n",
      "Gradient Descent(336/999): loss=0.35389743836349347\n",
      "Gradient Descent(337/999): loss=0.3538628227672725\n",
      "Gradient Descent(338/999): loss=0.3538283318869333\n",
      "Gradient Descent(339/999): loss=0.35379396477564634\n",
      "Gradient Descent(340/999): loss=0.3537597205000189\n",
      "Gradient Descent(341/999): loss=0.35372559813985394\n",
      "Gradient Descent(342/999): loss=0.35369159678791334\n",
      "Gradient Descent(343/999): loss=0.35365771554968634\n",
      "Gradient Descent(344/999): loss=0.35362395354316145\n",
      "Gradient Descent(345/999): loss=0.35359030989860346\n",
      "Gradient Descent(346/999): loss=0.353556783758335\n",
      "Gradient Descent(347/999): loss=0.35352337427652175\n",
      "Gradient Descent(348/999): loss=0.3534900806189613\n",
      "Gradient Descent(349/999): loss=0.3534569019628787\n",
      "Gradient Descent(350/999): loss=0.3534238374967217\n",
      "Gradient Descent(351/999): loss=0.35339088641996413\n",
      "Gradient Descent(352/999): loss=0.35335804794291004\n",
      "Gradient Descent(353/999): loss=0.35332532128650357\n",
      "Gradient Descent(354/999): loss=0.35329270568214116\n",
      "Gradient Descent(355/999): loss=0.3532602003714881\n",
      "Gradient Descent(356/999): loss=0.35322780460629866\n",
      "Gradient Descent(357/999): loss=0.35319551764823864\n",
      "Gradient Descent(358/999): loss=0.3531633387687132\n",
      "Gradient Descent(359/999): loss=0.3531312672486956\n",
      "Gradient Descent(360/999): loss=0.3530993023785613\n",
      "Gradient Descent(361/999): loss=0.3530674434579245\n",
      "Gradient Descent(362/999): loss=0.3530356897954769\n",
      "Gradient Descent(363/999): loss=0.35300404070883123\n",
      "Gradient Descent(364/999): loss=0.352972495524366\n",
      "Gradient Descent(365/999): loss=0.35294105357707517\n",
      "Gradient Descent(366/999): loss=0.3529097142104183\n",
      "Gradient Descent(367/999): loss=0.3528784767761758\n",
      "Gradient Descent(368/999): loss=0.35284734063430556\n",
      "Gradient Descent(369/999): loss=0.3528163051528026\n",
      "Gradient Descent(370/999): loss=0.3527853697075618\n",
      "Gradient Descent(371/999): loss=0.35275453368224263\n",
      "Gradient Descent(372/999): loss=0.3527237964681363\n",
      "Gradient Descent(373/999): loss=0.35269315746403723\n",
      "Gradient Descent(374/999): loss=0.35266261607611377\n",
      "Gradient Descent(375/999): loss=0.3526321717177843\n",
      "Gradient Descent(376/999): loss=0.3526018238095945\n",
      "Gradient Descent(377/999): loss=0.3525715717790957\n",
      "Gradient Descent(378/999): loss=0.3525414150607282\n",
      "Gradient Descent(379/999): loss=0.3525113530957052\n",
      "Gradient Descent(380/999): loss=0.3524813853318976\n",
      "Gradient Descent(381/999): loss=0.3524515112237242\n",
      "Gradient Descent(382/999): loss=0.35242173023204093\n",
      "Gradient Descent(383/999): loss=0.35239204182403383\n",
      "Gradient Descent(384/999): loss=0.3523624454731139\n",
      "Gradient Descent(385/999): loss=0.35233294065881293\n",
      "Gradient Descent(386/999): loss=0.352303526866682\n",
      "Gradient Descent(387/999): loss=0.3522742035881928\n",
      "Gradient Descent(388/999): loss=0.35224497032063845\n",
      "Gradient Descent(389/999): loss=0.3522158265670387\n",
      "Gradient Descent(390/999): loss=0.35218677183604463\n",
      "Gradient Descent(391/999): loss=0.3521578056418474\n",
      "Gradient Descent(392/999): loss=0.3521289275040861\n",
      "Gradient Descent(393/999): loss=0.3521001369477601\n",
      "Gradient Descent(394/999): loss=0.3520714335031412\n",
      "Gradient Descent(395/999): loss=0.35204281670568666\n",
      "Gradient Descent(396/999): loss=0.3520142860959571\n",
      "Gradient Descent(397/999): loss=0.35198584121953214\n",
      "Gradient Descent(398/999): loss=0.35195748162693036\n",
      "Gradient Descent(399/999): loss=0.35192920687352874\n",
      "Gradient Descent(400/999): loss=0.3519010165194856\n",
      "Gradient Descent(401/999): loss=0.35187291012966265\n",
      "Gradient Descent(402/999): loss=0.3518448872735513\n",
      "Gradient Descent(403/999): loss=0.3518169475251968\n",
      "Gradient Descent(404/999): loss=0.3517890904631274\n",
      "Gradient Descent(405/999): loss=0.3517613156702817\n",
      "Gradient Descent(406/999): loss=0.35173362273394027\n",
      "Gradient Descent(407/999): loss=0.3517060112456552\n",
      "Gradient Descent(408/999): loss=0.3516784808011846\n",
      "Gradient Descent(409/999): loss=0.35165103100042494\n",
      "Gradient Descent(410/999): loss=0.3516236614473468\n",
      "Gradient Descent(411/999): loss=0.35159637174993086\n",
      "Gradient Descent(412/999): loss=0.3515691615201055\n",
      "Gradient Descent(413/999): loss=0.35154203037368537\n",
      "Gradient Descent(414/999): loss=0.3515149779303101\n",
      "Gradient Descent(415/999): loss=0.35148800381338724\n",
      "Gradient Descent(416/999): loss=0.3514611076500311\n",
      "Gradient Descent(417/999): loss=0.35143428907100843\n",
      "Gradient Descent(418/999): loss=0.35140754771068017\n",
      "Gradient Descent(419/999): loss=0.35138088320694755\n",
      "Gradient Descent(420/999): loss=0.35135429520119793\n",
      "Gradient Descent(421/999): loss=0.35132778333825126\n",
      "Gradient Descent(422/999): loss=0.351301347266308\n",
      "Gradient Descent(423/999): loss=0.35127498663689793\n",
      "Gradient Descent(424/999): loss=0.35124870110483036\n",
      "Gradient Descent(425/999): loss=0.3512224903281439\n",
      "Gradient Descent(426/999): loss=0.3511963539680584\n",
      "Gradient Descent(427/999): loss=0.3511702916889271\n",
      "Gradient Descent(428/999): loss=0.3511443031581904\n",
      "Gradient Descent(429/999): loss=0.3511183880463287\n",
      "Gradient Descent(430/999): loss=0.351092546026819\n",
      "Gradient Descent(431/999): loss=0.3510667767760885\n",
      "Gradient Descent(432/999): loss=0.35104107997347306\n",
      "Gradient Descent(433/999): loss=0.3510154553011727\n",
      "Gradient Descent(434/999): loss=0.3509899024442112\n",
      "Gradient Descent(435/999): loss=0.35096442109039294\n",
      "Gradient Descent(436/999): loss=0.3509390109302644\n",
      "Gradient Descent(437/999): loss=0.3509136716570729\n",
      "Gradient Descent(438/999): loss=0.35088840296672785\n",
      "Gradient Descent(439/999): loss=0.3508632045577625\n",
      "Gradient Descent(440/999): loss=0.3508380761312959\n",
      "Gradient Descent(441/999): loss=0.35081301739099635\n",
      "Gradient Descent(442/999): loss=0.3507880280430443\n",
      "Gradient Descent(443/999): loss=0.35076310779609676\n",
      "Gradient Descent(444/999): loss=0.35073825636125233\n",
      "Gradient Descent(445/999): loss=0.35071347345201676\n",
      "Gradient Descent(446/999): loss=0.3506887587842679\n",
      "Gradient Descent(447/999): loss=0.3506641120762241\n",
      "Gradient Descent(448/999): loss=0.3506395330484102\n",
      "Gradient Descent(449/999): loss=0.3506150214236251\n",
      "Gradient Descent(450/999): loss=0.3505905769269115\n",
      "Gradient Descent(451/999): loss=0.35056619928552296\n",
      "Gradient Descent(452/999): loss=0.35054188822889515\n",
      "Gradient Descent(453/999): loss=0.3505176434886143\n",
      "Gradient Descent(454/999): loss=0.35049346479838855\n",
      "Gradient Descent(455/999): loss=0.35046935189401873\n",
      "Gradient Descent(456/999): loss=0.3504453045133699\n",
      "Gradient Descent(457/999): loss=0.3504213223963432\n",
      "Gradient Descent(458/999): loss=0.35039740528484825\n",
      "Gradient Descent(459/999): loss=0.35037355292277633\n",
      "Gradient Descent(460/999): loss=0.35034976505597315\n",
      "Gradient Descent(461/999): loss=0.35032604143221413\n",
      "Gradient Descent(462/999): loss=0.3503023818011773\n",
      "Gradient Descent(463/999): loss=0.3502787859144179\n",
      "Gradient Descent(464/999): loss=0.35025525352534487\n",
      "Gradient Descent(465/999): loss=0.3502317843891957\n",
      "Gradient Descent(466/999): loss=0.3502083782630117\n",
      "Gradient Descent(467/999): loss=0.3501850349056159\n",
      "Gradient Descent(468/999): loss=0.35016175407758837\n",
      "Gradient Descent(469/999): loss=0.3501385355412451\n",
      "Gradient Descent(470/999): loss=0.3501153790606133\n",
      "Gradient Descent(471/999): loss=0.35009228440141155\n",
      "Gradient Descent(472/999): loss=0.3500692513310271\n",
      "Gradient Descent(473/999): loss=0.35004627961849444\n",
      "Gradient Descent(474/999): loss=0.3500233690344746\n",
      "Gradient Descent(475/999): loss=0.3500005193512345\n",
      "Gradient Descent(476/999): loss=0.3499777303426269\n",
      "Gradient Descent(477/999): loss=0.34995500178407013\n",
      "Gradient Descent(478/999): loss=0.3499323334525286\n",
      "Gradient Descent(479/999): loss=0.3499097251264935\n",
      "Gradient Descent(480/999): loss=0.34988717658596435\n",
      "Gradient Descent(481/999): loss=0.349864687612429\n",
      "Gradient Descent(482/999): loss=0.34984225798884744\n",
      "Gradient Descent(483/999): loss=0.34981988749963155\n",
      "Gradient Descent(484/999): loss=0.34979757593062877\n",
      "Gradient Descent(485/999): loss=0.3497753230691042\n",
      "Gradient Descent(486/999): loss=0.34975312870372344\n",
      "Gradient Descent(487/999): loss=0.34973099262453583\n",
      "Gradient Descent(488/999): loss=0.34970891462295767\n",
      "Gradient Descent(489/999): loss=0.349686894491756\n",
      "Gradient Descent(490/999): loss=0.34966493202503274\n",
      "Gradient Descent(491/999): loss=0.34964302701820843\n",
      "Gradient Descent(492/999): loss=0.3496211792680067\n",
      "Gradient Descent(493/999): loss=0.34959938857243983\n",
      "Gradient Descent(494/999): loss=0.34957765473079233\n",
      "Gradient Descent(495/999): loss=0.3495559775436071\n",
      "Gradient Descent(496/999): loss=0.34953435681267087\n",
      "Gradient Descent(497/999): loss=0.34951279234099925\n",
      "Gradient Descent(498/999): loss=0.34949128393282286\n",
      "Gradient Descent(499/999): loss=0.34946983139357374\n",
      "Gradient Descent(500/999): loss=0.3494484345298713\n",
      "Gradient Descent(501/999): loss=0.34942709314950904\n",
      "Gradient Descent(502/999): loss=0.34940580706144153\n",
      "Gradient Descent(503/999): loss=0.349384576075771\n",
      "Gradient Descent(504/999): loss=0.3493634000037345\n",
      "Gradient Descent(505/999): loss=0.34934227865769135\n",
      "Gradient Descent(506/999): loss=0.3493212118511114\n",
      "Gradient Descent(507/999): loss=0.34930019939856155\n",
      "Gradient Descent(508/999): loss=0.34927924111569464\n",
      "Gradient Descent(509/999): loss=0.3492583368192376\n",
      "Gradient Descent(510/999): loss=0.3492374863269786\n",
      "Gradient Descent(511/999): loss=0.3492166894577575\n",
      "Gradient Descent(512/999): loss=0.3491959460314529\n",
      "Gradient Descent(513/999): loss=0.3491752558689716\n",
      "Gradient Descent(514/999): loss=0.34915461879223814\n",
      "Gradient Descent(515/999): loss=0.349134034624183\n",
      "Gradient Descent(516/999): loss=0.3491135031887326\n",
      "Gradient Descent(517/999): loss=0.34909302431079886\n",
      "Gradient Descent(518/999): loss=0.3490725978162685\n",
      "Gradient Descent(519/999): loss=0.34905222353199333\n",
      "Gradient Descent(520/999): loss=0.34903190128578016\n",
      "Gradient Descent(521/999): loss=0.34901163090638054\n",
      "Gradient Descent(522/999): loss=0.3489914122234815\n",
      "Gradient Descent(523/999): loss=0.3489712450676962\n",
      "Gradient Descent(524/999): loss=0.34895112927055344\n",
      "Gradient Descent(525/999): loss=0.3489310646644899\n",
      "Gradient Descent(526/999): loss=0.34891105108283993\n",
      "Gradient Descent(527/999): loss=0.348891088359827\n",
      "Gradient Descent(528/999): loss=0.34887117633055453\n",
      "Gradient Descent(529/999): loss=0.3488513148309977\n",
      "Gradient Descent(530/999): loss=0.34883150369799415\n",
      "Gradient Descent(531/999): loss=0.3488117427692362\n",
      "Gradient Descent(532/999): loss=0.3487920318832618\n",
      "Gradient Descent(533/999): loss=0.34877237087944724\n",
      "Gradient Descent(534/999): loss=0.34875275959799795\n",
      "Gradient Descent(535/999): loss=0.34873319787994145\n",
      "Gradient Descent(536/999): loss=0.3487136855671187\n",
      "Gradient Descent(537/999): loss=0.34869422250217713\n",
      "Gradient Descent(538/999): loss=0.3486748085285624\n",
      "Gradient Descent(539/999): loss=0.34865544349051114\n",
      "Gradient Descent(540/999): loss=0.3486361272330433\n",
      "Gradient Descent(541/999): loss=0.34861685960195504\n",
      "Gradient Descent(542/999): loss=0.3485976404438117\n",
      "Gradient Descent(543/999): loss=0.3485784696059403\n",
      "Gradient Descent(544/999): loss=0.3485593469364223\n",
      "Gradient Descent(545/999): loss=0.3485402722840878\n",
      "Gradient Descent(546/999): loss=0.3485212454985074\n",
      "Gradient Descent(547/999): loss=0.3485022664299864\n",
      "Gradient Descent(548/999): loss=0.34848333492955763\n",
      "Gradient Descent(549/999): loss=0.3484644508489754\n",
      "Gradient Descent(550/999): loss=0.34844561404070856\n",
      "Gradient Descent(551/999): loss=0.3484268243579347\n",
      "Gradient Descent(552/999): loss=0.34840808165453346\n",
      "Gradient Descent(553/999): loss=0.34838938578507994\n",
      "Gradient Descent(554/999): loss=0.34837073660484036\n",
      "Gradient Descent(555/999): loss=0.3483521339697636\n",
      "Gradient Descent(556/999): loss=0.34833357773647744\n",
      "Gradient Descent(557/999): loss=0.3483150677622813\n",
      "Gradient Descent(558/999): loss=0.34829660390514117\n",
      "Gradient Descent(559/999): loss=0.34827818602368366\n",
      "Gradient Descent(560/999): loss=0.3482598139771908\n",
      "Gradient Descent(561/999): loss=0.3482414876255934\n",
      "Gradient Descent(562/999): loss=0.34822320682946717\n",
      "Gradient Descent(563/999): loss=0.34820497145002643\n",
      "Gradient Descent(564/999): loss=0.3481867813491184\n",
      "Gradient Descent(565/999): loss=0.3481686363892189\n",
      "Gradient Descent(566/999): loss=0.34815053643342664\n",
      "Gradient Descent(567/999): loss=0.3481324813454584\n",
      "Gradient Descent(568/999): loss=0.34811447098964327\n",
      "Gradient Descent(569/999): loss=0.3480965052309186\n",
      "Gradient Descent(570/999): loss=0.34807858393482505\n",
      "Gradient Descent(571/999): loss=0.34806070696750063\n",
      "Gradient Descent(572/999): loss=0.34804287419567737\n",
      "Gradient Descent(573/999): loss=0.3480250854866755\n",
      "Gradient Descent(574/999): loss=0.3480073407083996\n",
      "Gradient Descent(575/999): loss=0.34798963972933317\n",
      "Gradient Descent(576/999): loss=0.3479719824185353\n",
      "Gradient Descent(577/999): loss=0.34795436864563467\n",
      "Gradient Descent(578/999): loss=0.347936798280826\n",
      "Gradient Descent(579/999): loss=0.34791927119486626\n",
      "Gradient Descent(580/999): loss=0.347901787259069\n",
      "Gradient Descent(581/999): loss=0.3478843463453012\n",
      "Gradient Descent(582/999): loss=0.3478669483259781\n",
      "Gradient Descent(583/999): loss=0.34784959307406027\n",
      "Gradient Descent(584/999): loss=0.3478322804630484\n",
      "Gradient Descent(585/999): loss=0.34781501036697987\n",
      "Gradient Descent(586/999): loss=0.34779778266042444\n",
      "Gradient Descent(587/999): loss=0.34778059721848037\n",
      "Gradient Descent(588/999): loss=0.34776345391677105\n",
      "Gradient Descent(589/999): loss=0.34774635263144016\n",
      "Gradient Descent(590/999): loss=0.3477292932391482\n",
      "Gradient Descent(591/999): loss=0.34771227561706985\n",
      "Gradient Descent(592/999): loss=0.34769529964288837\n",
      "Gradient Descent(593/999): loss=0.34767836519479334\n",
      "Gradient Descent(594/999): loss=0.3476614721514764\n",
      "Gradient Descent(595/999): loss=0.347644620392128\n",
      "Gradient Descent(596/999): loss=0.34762780979643354\n",
      "Gradient Descent(597/999): loss=0.3476110402445699\n",
      "Gradient Descent(598/999): loss=0.3475943116172025\n",
      "Gradient Descent(599/999): loss=0.3475776237954812\n",
      "Gradient Descent(600/999): loss=0.3475609766610371\n",
      "Gradient Descent(601/999): loss=0.34754437009597977\n",
      "Gradient Descent(602/999): loss=0.34752780398289335\n",
      "Gradient Descent(603/999): loss=0.3475112782048331\n",
      "Gradient Descent(604/999): loss=0.34749479264532274\n",
      "Gradient Descent(605/999): loss=0.3474783471883512\n",
      "Gradient Descent(606/999): loss=0.347461941718369\n",
      "Gradient Descent(607/999): loss=0.3474455761202861\n",
      "Gradient Descent(608/999): loss=0.3474292502794672\n",
      "Gradient Descent(609/999): loss=0.3474129640817305\n",
      "Gradient Descent(610/999): loss=0.3473967174133436\n",
      "Gradient Descent(611/999): loss=0.34738051016102045\n",
      "Gradient Descent(612/999): loss=0.3473643422119193\n",
      "Gradient Descent(613/999): loss=0.34734821345363875\n",
      "Gradient Descent(614/999): loss=0.3473321237742162\n",
      "Gradient Descent(615/999): loss=0.34731607306212287\n",
      "Gradient Descent(616/999): loss=0.34730006120626333\n",
      "Gradient Descent(617/999): loss=0.34728408809597144\n",
      "Gradient Descent(618/999): loss=0.3472681536210075\n",
      "Gradient Descent(619/999): loss=0.3472522576715562\n",
      "Gradient Descent(620/999): loss=0.3472364001382236\n",
      "Gradient Descent(621/999): loss=0.34722058091203417\n",
      "Gradient Descent(622/999): loss=0.3472047998844285\n",
      "Gradient Descent(623/999): loss=0.3471890569472611\n",
      "Gradient Descent(624/999): loss=0.34717335199279664\n",
      "Gradient Descent(625/999): loss=0.3471576849137088\n",
      "Gradient Descent(626/999): loss=0.34714205560307604\n",
      "Gradient Descent(627/999): loss=0.34712646395438146\n",
      "Gradient Descent(628/999): loss=0.34711090986150817\n",
      "Gradient Descent(629/999): loss=0.3470953932187375\n",
      "Gradient Descent(630/999): loss=0.34707991392074705\n",
      "Gradient Descent(631/999): loss=0.3470644718626082\n",
      "Gradient Descent(632/999): loss=0.347049066939783\n",
      "Gradient Descent(633/999): loss=0.34703369904812276\n",
      "Gradient Descent(634/999): loss=0.34701836808386494\n",
      "Gradient Descent(635/999): loss=0.34700307394363156\n",
      "Gradient Descent(636/999): loss=0.34698781652442623\n",
      "Gradient Descent(637/999): loss=0.34697259572363265\n",
      "Gradient Descent(638/999): loss=0.34695741143901143\n",
      "Gradient Descent(639/999): loss=0.3469422635686991\n",
      "Gradient Descent(640/999): loss=0.34692715201120466\n",
      "Gradient Descent(641/999): loss=0.34691207666540824\n",
      "Gradient Descent(642/999): loss=0.34689703743055883\n",
      "Gradient Descent(643/999): loss=0.3468820342062716\n",
      "Gradient Descent(644/999): loss=0.3468670668925267\n",
      "Gradient Descent(645/999): loss=0.34685213538966647\n",
      "Gradient Descent(646/999): loss=0.346837239598394\n",
      "Gradient Descent(647/999): loss=0.3468223794197702\n",
      "Gradient Descent(648/999): loss=0.3468075547552123\n",
      "Gradient Descent(649/999): loss=0.3467927655064922\n",
      "Gradient Descent(650/999): loss=0.34677801157573396\n",
      "Gradient Descent(651/999): loss=0.34676329286541197\n",
      "Gradient Descent(652/999): loss=0.3467486092783492\n",
      "Gradient Descent(653/999): loss=0.34673396071771484\n",
      "Gradient Descent(654/999): loss=0.34671934708702273\n",
      "Gradient Descent(655/999): loss=0.3467047682901295\n",
      "Gradient Descent(656/999): loss=0.34669022423123247\n",
      "Gradient Descent(657/999): loss=0.34667571481486786\n",
      "Gradient Descent(658/999): loss=0.3466612399459093\n",
      "Gradient Descent(659/999): loss=0.3466467995295653\n",
      "Gradient Descent(660/999): loss=0.34663239347137814\n",
      "Gradient Descent(661/999): loss=0.34661802167722155\n",
      "Gradient Descent(662/999): loss=0.34660368405329944\n",
      "Gradient Descent(663/999): loss=0.3465893805061437\n",
      "Gradient Descent(664/999): loss=0.34657511094261245\n",
      "Gradient Descent(665/999): loss=0.3465608752698888\n",
      "Gradient Descent(666/999): loss=0.3465466733954789\n",
      "Gradient Descent(667/999): loss=0.34653250522720974\n",
      "Gradient Descent(668/999): loss=0.3465183706732284\n",
      "Gradient Descent(669/999): loss=0.3465042696419995\n",
      "Gradient Descent(670/999): loss=0.3464902020423041\n",
      "Gradient Descent(671/999): loss=0.3464761677832377\n",
      "Gradient Descent(672/999): loss=0.3464621667742094\n",
      "Gradient Descent(673/999): loss=0.34644819892493867\n",
      "Gradient Descent(674/999): loss=0.3464342641454559\n",
      "Gradient Descent(675/999): loss=0.34642036234609885\n",
      "Gradient Descent(676/999): loss=0.3464064934375125\n",
      "Gradient Descent(677/999): loss=0.3463926573306465\n",
      "Gradient Descent(678/999): loss=0.34637885393675427\n",
      "Gradient Descent(679/999): loss=0.3463650831673914\n",
      "Gradient Descent(680/999): loss=0.34635134493441366\n",
      "Gradient Descent(681/999): loss=0.34633763914997606\n",
      "Gradient Descent(682/999): loss=0.3463239657265309\n",
      "Gradient Descent(683/999): loss=0.34631032457682664\n",
      "Gradient Descent(684/999): loss=0.34629671561390657\n",
      "Gradient Descent(685/999): loss=0.3462831387511065\n",
      "Gradient Descent(686/999): loss=0.34626959390205453\n",
      "Gradient Descent(687/999): loss=0.34625608098066823\n",
      "Gradient Descent(688/999): loss=0.34624259990115475\n",
      "Gradient Descent(689/999): loss=0.34622915057800857\n",
      "Gradient Descent(690/999): loss=0.34621573292600943\n",
      "Gradient Descent(691/999): loss=0.3462023468602226\n",
      "Gradient Descent(692/999): loss=0.3461889922959961\n",
      "Gradient Descent(693/999): loss=0.3461756691489597\n",
      "Gradient Descent(694/999): loss=0.3461623773350242\n",
      "Gradient Descent(695/999): loss=0.34614911677037913\n",
      "Gradient Descent(696/999): loss=0.3461358873714921\n",
      "Gradient Descent(697/999): loss=0.3461226890551072\n",
      "Gradient Descent(698/999): loss=0.3461095217382439\n",
      "Gradient Descent(699/999): loss=0.346096385338195\n",
      "Gradient Descent(700/999): loss=0.34608327977252684\n",
      "Gradient Descent(701/999): loss=0.34607020495907614\n",
      "Gradient Descent(702/999): loss=0.34605716081595056\n",
      "Gradient Descent(703/999): loss=0.34604414726152566\n",
      "Gradient Descent(704/999): loss=0.3460311642144455\n",
      "Gradient Descent(705/999): loss=0.3460182115936198\n",
      "Gradient Descent(706/999): loss=0.3460052893182238\n",
      "Gradient Descent(707/999): loss=0.3459923973076959\n",
      "Gradient Descent(708/999): loss=0.34597953548173765\n",
      "Gradient Descent(709/999): loss=0.34596670376031236\n",
      "Gradient Descent(710/999): loss=0.345953902063643\n",
      "Gradient Descent(711/999): loss=0.3459411303122115\n",
      "Gradient Descent(712/999): loss=0.34592838842675816\n",
      "Gradient Descent(713/999): loss=0.3459156763282796\n",
      "Gradient Descent(714/999): loss=0.3459029939380281\n",
      "Gradient Descent(715/999): loss=0.3458903411775103\n",
      "Gradient Descent(716/999): loss=0.3458777179684861\n",
      "Gradient Descent(717/999): loss=0.34586512423296717\n",
      "Gradient Descent(718/999): loss=0.34585255989321684\n",
      "Gradient Descent(719/999): loss=0.3458400248717475\n",
      "Gradient Descent(720/999): loss=0.3458275190913206\n",
      "Gradient Descent(721/999): loss=0.34581504247494543\n",
      "Gradient Descent(722/999): loss=0.3458025949458773\n",
      "Gradient Descent(723/999): loss=0.34579017642761745\n",
      "Gradient Descent(724/999): loss=0.3457777868439109\n",
      "Gradient Descent(725/999): loss=0.34576542611874644\n",
      "Gradient Descent(726/999): loss=0.34575309417635486\n",
      "Gradient Descent(727/999): loss=0.3457407909412081\n",
      "Gradient Descent(728/999): loss=0.34572851633801815\n",
      "Gradient Descent(729/999): loss=0.34571627029173596\n",
      "Gradient Descent(730/999): loss=0.3457040527275507\n",
      "Gradient Descent(731/999): loss=0.3456918635708883\n",
      "Gradient Descent(732/999): loss=0.3456797027474106\n",
      "Gradient Descent(733/999): loss=0.3456675701830147\n",
      "Gradient Descent(734/999): loss=0.34565546580383094\n",
      "Gradient Descent(735/999): loss=0.34564338953622353\n",
      "Gradient Descent(736/999): loss=0.3456313413067877\n",
      "Gradient Descent(737/999): loss=0.34561932104234994\n",
      "Gradient Descent(738/999): loss=0.3456073286699668\n",
      "Gradient Descent(739/999): loss=0.34559536411692343\n",
      "Gradient Descent(740/999): loss=0.3455834273107336\n",
      "Gradient Descent(741/999): loss=0.3455715181791371\n",
      "Gradient Descent(742/999): loss=0.34555963665010075\n",
      "Gradient Descent(743/999): loss=0.34554778265181607\n",
      "Gradient Descent(744/999): loss=0.3455359561126982\n",
      "Gradient Descent(745/999): loss=0.3455241569613867\n",
      "Gradient Descent(746/999): loss=0.34551238512674215\n",
      "Gradient Descent(747/999): loss=0.3455006405378476\n",
      "Gradient Descent(748/999): loss=0.34548892312400553\n",
      "Gradient Descent(749/999): loss=0.3454772328147384\n",
      "Gradient Descent(750/999): loss=0.34546556953978746\n",
      "Gradient Descent(751/999): loss=0.3454539332291114\n",
      "Gradient Descent(752/999): loss=0.34544232381288553\n",
      "Gradient Descent(753/999): loss=0.3454307412215015\n",
      "Gradient Descent(754/999): loss=0.3454191853855659\n",
      "Gradient Descent(755/999): loss=0.34540765623589875\n",
      "Gradient Descent(756/999): loss=0.34539615370353416\n",
      "Gradient Descent(757/999): loss=0.34538467771971826\n",
      "Gradient Descent(758/999): loss=0.34537322821590893\n",
      "Gradient Descent(759/999): loss=0.34536180512377423\n",
      "Gradient Descent(760/999): loss=0.3453504083751924\n",
      "Gradient Descent(761/999): loss=0.3453390379022508\n",
      "Gradient Descent(762/999): loss=0.34532769363724397\n",
      "Gradient Descent(763/999): loss=0.3453163755126747\n",
      "Gradient Descent(764/999): loss=0.34530508346125216\n",
      "Gradient Descent(765/999): loss=0.34529381741589016\n",
      "Gradient Descent(766/999): loss=0.34528257730970846\n",
      "Gradient Descent(767/999): loss=0.3452713630760298\n",
      "Gradient Descent(768/999): loss=0.3452601746483806\n",
      "Gradient Descent(769/999): loss=0.34524901196048957\n",
      "Gradient Descent(770/999): loss=0.3452378749462868\n",
      "Gradient Descent(771/999): loss=0.3452267635399029\n",
      "Gradient Descent(772/999): loss=0.3452156776756685\n",
      "Gradient Descent(773/999): loss=0.34520461728811397\n",
      "Gradient Descent(774/999): loss=0.3451935823119672\n",
      "Gradient Descent(775/999): loss=0.34518257268215347\n",
      "Gradient Descent(776/999): loss=0.34517158833379613\n",
      "Gradient Descent(777/999): loss=0.3451606292022132\n",
      "Gradient Descent(778/999): loss=0.3451496952229184\n",
      "Gradient Descent(779/999): loss=0.34513878633162\n",
      "Gradient Descent(780/999): loss=0.3451279024642201\n",
      "Gradient Descent(781/999): loss=0.3451170435568132\n",
      "Gradient Descent(782/999): loss=0.3451062095456864\n",
      "Gradient Descent(783/999): loss=0.3450954003673186\n",
      "Gradient Descent(784/999): loss=0.3450846159583785\n",
      "Gradient Descent(785/999): loss=0.3450738562557255\n",
      "Gradient Descent(786/999): loss=0.3450631211964082\n",
      "Gradient Descent(787/999): loss=0.3450524107176631\n",
      "Gradient Descent(788/999): loss=0.3450417247569152\n",
      "Gradient Descent(789/999): loss=0.3450310632517759\n",
      "Gradient Descent(790/999): loss=0.3450204261400436\n",
      "Gradient Descent(791/999): loss=0.3450098133597018\n",
      "Gradient Descent(792/999): loss=0.3449992248489192\n",
      "Gradient Descent(793/999): loss=0.3449886605460483\n",
      "Gradient Descent(794/999): loss=0.34497812038962583\n",
      "Gradient Descent(795/999): loss=0.3449676043183706\n",
      "Gradient Descent(796/999): loss=0.3449571122711838\n",
      "Gradient Descent(797/999): loss=0.34494664418714815\n",
      "Gradient Descent(798/999): loss=0.3449362000055268\n",
      "Gradient Descent(799/999): loss=0.3449257796657633\n",
      "Gradient Descent(800/999): loss=0.34491538310748027\n",
      "Gradient Descent(801/999): loss=0.344905010270479\n",
      "Gradient Descent(802/999): loss=0.34489466109473915\n",
      "Gradient Descent(803/999): loss=0.3448843355204174\n",
      "Gradient Descent(804/999): loss=0.34487403348784706\n",
      "Gradient Descent(805/999): loss=0.3448637549375378\n",
      "Gradient Descent(806/999): loss=0.3448534998101741\n",
      "Gradient Descent(807/999): loss=0.3448432680466158\n",
      "Gradient Descent(808/999): loss=0.3448330595878963\n",
      "Gradient Descent(809/999): loss=0.3448228743752225\n",
      "Gradient Descent(810/999): loss=0.34481271234997424\n",
      "Gradient Descent(811/999): loss=0.3448025734537031\n",
      "Gradient Descent(812/999): loss=0.3447924576281325\n",
      "Gradient Descent(813/999): loss=0.3447823648151566\n",
      "Gradient Descent(814/999): loss=0.3447722949568397\n",
      "Gradient Descent(815/999): loss=0.34476224799541566\n",
      "Gradient Descent(816/999): loss=0.34475222387328724\n",
      "Gradient Descent(817/999): loss=0.3447422225330258\n",
      "Gradient Descent(818/999): loss=0.34473224391737034\n",
      "Gradient Descent(819/999): loss=0.3447222879692268\n",
      "Gradient Descent(820/999): loss=0.3447123546316674\n",
      "Gradient Descent(821/999): loss=0.3447024438479308\n",
      "Gradient Descent(822/999): loss=0.3446925555614205\n",
      "Gradient Descent(823/999): loss=0.34468268971570487\n",
      "Gradient Descent(824/999): loss=0.3446728462545162\n",
      "Gradient Descent(825/999): loss=0.3446630251217505\n",
      "Gradient Descent(826/999): loss=0.3446532262614663\n",
      "Gradient Descent(827/999): loss=0.3446434496178845\n",
      "Gradient Descent(828/999): loss=0.34463369513538805\n",
      "Gradient Descent(829/999): loss=0.34462396275852075\n",
      "Gradient Descent(830/999): loss=0.34461425243198684\n",
      "Gradient Descent(831/999): loss=0.34460456410065055\n",
      "Gradient Descent(832/999): loss=0.34459489770953555\n",
      "Gradient Descent(833/999): loss=0.3445852532038244\n",
      "Gradient Descent(834/999): loss=0.3445756305288577\n",
      "Gradient Descent(835/999): loss=0.34456602963013383\n",
      "Gradient Descent(836/999): loss=0.34455645045330835\n",
      "Gradient Descent(837/999): loss=0.34454689294419316\n",
      "Gradient Descent(838/999): loss=0.3445373570487561\n",
      "Gradient Descent(839/999): loss=0.3445278427131207\n",
      "Gradient Descent(840/999): loss=0.3445183498835651\n",
      "Gradient Descent(841/999): loss=0.34450887850652206\n",
      "Gradient Descent(842/999): loss=0.3444994285285777\n",
      "Gradient Descent(843/999): loss=0.34448999989647133\n",
      "Gradient Descent(844/999): loss=0.34448059255709557\n",
      "Gradient Descent(845/999): loss=0.3444712064574947\n",
      "Gradient Descent(846/999): loss=0.34446184154486464\n",
      "Gradient Descent(847/999): loss=0.3444524977665521\n",
      "Gradient Descent(848/999): loss=0.34444317507005495\n",
      "Gradient Descent(849/999): loss=0.3444338734030207\n",
      "Gradient Descent(850/999): loss=0.34442459271324594\n",
      "Gradient Descent(851/999): loss=0.34441533294867693\n",
      "Gradient Descent(852/999): loss=0.3444060940574083\n",
      "Gradient Descent(853/999): loss=0.34439687598768176\n",
      "Gradient Descent(854/999): loss=0.34438767868788706\n",
      "Gradient Descent(855/999): loss=0.34437850210656085\n",
      "Gradient Descent(856/999): loss=0.3443693461923859\n",
      "Gradient Descent(857/999): loss=0.34436021089419083\n",
      "Gradient Descent(858/999): loss=0.34435109616094983\n",
      "Gradient Descent(859/999): loss=0.3443420019417817\n",
      "Gradient Descent(860/999): loss=0.3443329281859495\n",
      "Gradient Descent(861/999): loss=0.34432387484286037\n",
      "Gradient Descent(862/999): loss=0.3443148418620645\n",
      "Gradient Descent(863/999): loss=0.34430582919325503\n",
      "Gradient Descent(864/999): loss=0.3442968367862675\n",
      "Gradient Descent(865/999): loss=0.3442878645910793\n",
      "Gradient Descent(866/999): loss=0.34427891255780885\n",
      "Gradient Descent(867/999): loss=0.34426998063671593\n",
      "Gradient Descent(868/999): loss=0.3442610687782002\n",
      "Gradient Descent(869/999): loss=0.34425217693280175\n",
      "Gradient Descent(870/999): loss=0.34424330505119954\n",
      "Gradient Descent(871/999): loss=0.3442344530842119\n",
      "Gradient Descent(872/999): loss=0.3442256209827953\n",
      "Gradient Descent(873/999): loss=0.3442168086980443\n",
      "Gradient Descent(874/999): loss=0.3442080161811911\n",
      "Gradient Descent(875/999): loss=0.3441992433836047\n",
      "Gradient Descent(876/999): loss=0.344190490256791\n",
      "Gradient Descent(877/999): loss=0.34418175675239177\n",
      "Gradient Descent(878/999): loss=0.3441730428221842\n",
      "Gradient Descent(879/999): loss=0.34416434841808136\n",
      "Gradient Descent(880/999): loss=0.3441556734921303\n",
      "Gradient Descent(881/999): loss=0.3441470179965127\n",
      "Gradient Descent(882/999): loss=0.3441383818835442\n",
      "Gradient Descent(883/999): loss=0.3441297651056734\n",
      "Gradient Descent(884/999): loss=0.34412116761548195\n",
      "Gradient Descent(885/999): loss=0.344112589365684\n",
      "Gradient Descent(886/999): loss=0.34410403030912595\n",
      "Gradient Descent(887/999): loss=0.3440954903987854\n",
      "Gradient Descent(888/999): loss=0.3440869695877712\n",
      "Gradient Descent(889/999): loss=0.34407846782932267\n",
      "Gradient Descent(890/999): loss=0.3440699850768098\n",
      "Gradient Descent(891/999): loss=0.34406152128373196\n",
      "Gradient Descent(892/999): loss=0.3440530764037182\n",
      "Gradient Descent(893/999): loss=0.34404465039052645\n",
      "Gradient Descent(894/999): loss=0.3440362431980427\n",
      "Gradient Descent(895/999): loss=0.34402785478028186\n",
      "Gradient Descent(896/999): loss=0.3440194850913855\n",
      "Gradient Descent(897/999): loss=0.3440111340856233\n",
      "Gradient Descent(898/999): loss=0.344002801717391\n",
      "Gradient Descent(899/999): loss=0.34399448794121107\n",
      "Gradient Descent(900/999): loss=0.34398619271173214\n",
      "Gradient Descent(901/999): loss=0.34397791598372773\n",
      "Gradient Descent(902/999): loss=0.3439696577120969\n",
      "Gradient Descent(903/999): loss=0.3439614178518636\n",
      "Gradient Descent(904/999): loss=0.34395319635817573\n",
      "Gradient Descent(905/999): loss=0.34394499318630484\n",
      "Gradient Descent(906/999): loss=0.34393680829164647\n",
      "Gradient Descent(907/999): loss=0.343928641629719\n",
      "Gradient Descent(908/999): loss=0.3439204931561635\n",
      "Gradient Descent(909/999): loss=0.3439123628267429\n",
      "Gradient Descent(910/999): loss=0.3439042505973425\n",
      "Gradient Descent(911/999): loss=0.34389615642396865\n",
      "Gradient Descent(912/999): loss=0.34388808026274903\n",
      "Gradient Descent(913/999): loss=0.3438800220699318\n",
      "Gradient Descent(914/999): loss=0.34387198180188505\n",
      "Gradient Descent(915/999): loss=0.3438639594150976\n",
      "Gradient Descent(916/999): loss=0.3438559548661767\n",
      "Gradient Descent(917/999): loss=0.34384796811184914\n",
      "Gradient Descent(918/999): loss=0.3438399991089606\n",
      "Gradient Descent(919/999): loss=0.3438320478144748\n",
      "Gradient Descent(920/999): loss=0.34382411418547304\n",
      "Gradient Descent(921/999): loss=0.3438161981791549\n",
      "Gradient Descent(922/999): loss=0.343808299752836\n",
      "Gradient Descent(923/999): loss=0.3438004188639501\n",
      "Gradient Descent(924/999): loss=0.34379255547004606\n",
      "Gradient Descent(925/999): loss=0.3437847095287892\n",
      "Gradient Descent(926/999): loss=0.343776880997961\n",
      "Gradient Descent(927/999): loss=0.3437690698354571\n",
      "Gradient Descent(928/999): loss=0.3437612759992888\n",
      "Gradient Descent(929/999): loss=0.34375349944758155\n",
      "Gradient Descent(930/999): loss=0.3437457401385753\n",
      "Gradient Descent(931/999): loss=0.3437379980306232\n",
      "Gradient Descent(932/999): loss=0.3437302730821922\n",
      "Gradient Descent(933/999): loss=0.34372256525186196\n",
      "Gradient Descent(934/999): loss=0.34371487449832516\n",
      "Gradient Descent(935/999): loss=0.34370720078038647\n",
      "Gradient Descent(936/999): loss=0.3436995440569626\n",
      "Gradient Descent(937/999): loss=0.34369190428708174\n",
      "Gradient Descent(938/999): loss=0.3436842814298835\n",
      "Gradient Descent(939/999): loss=0.343676675444618\n",
      "Gradient Descent(940/999): loss=0.34366908629064613\n",
      "Gradient Descent(941/999): loss=0.3436615139274388\n",
      "Gradient Descent(942/999): loss=0.34365395831457696\n",
      "Gradient Descent(943/999): loss=0.34364641941175045\n",
      "Gradient Descent(944/999): loss=0.3436388971787587\n",
      "Gradient Descent(945/999): loss=0.3436313915755097\n",
      "Gradient Descent(946/999): loss=0.3436239025620199\n",
      "Gradient Descent(947/999): loss=0.34361643009841364\n",
      "Gradient Descent(948/999): loss=0.343608974144923\n",
      "Gradient Descent(949/999): loss=0.34360153466188753\n",
      "Gradient Descent(950/999): loss=0.3435941116097536\n",
      "Gradient Descent(951/999): loss=0.34358670494907445\n",
      "Gradient Descent(952/999): loss=0.34357931464050984\n",
      "Gradient Descent(953/999): loss=0.3435719406448249\n",
      "Gradient Descent(954/999): loss=0.3435645829228909\n",
      "Gradient Descent(955/999): loss=0.34355724143568445\n",
      "Gradient Descent(956/999): loss=0.3435499161442868\n",
      "Gradient Descent(957/999): loss=0.34354260700988426\n",
      "Gradient Descent(958/999): loss=0.3435353139937674\n",
      "Gradient Descent(959/999): loss=0.34352803705733037\n",
      "Gradient Descent(960/999): loss=0.3435207761620716\n",
      "Gradient Descent(961/999): loss=0.34351353126959233\n",
      "Gradient Descent(962/999): loss=0.3435063023415973\n",
      "Gradient Descent(963/999): loss=0.3434990893398935\n",
      "Gradient Descent(964/999): loss=0.34349189222639065\n",
      "Gradient Descent(965/999): loss=0.3434847109631002\n",
      "Gradient Descent(966/999): loss=0.34347754551213566\n",
      "Gradient Descent(967/999): loss=0.3434703958357115\n",
      "Gradient Descent(968/999): loss=0.3434632618961438\n",
      "Gradient Descent(969/999): loss=0.3434561436558493\n",
      "Gradient Descent(970/999): loss=0.3434490410773447\n",
      "Gradient Descent(971/999): loss=0.3434419541232473\n",
      "Gradient Descent(972/999): loss=0.3434348827562744\n",
      "Gradient Descent(973/999): loss=0.3434278269392422\n",
      "Gradient Descent(974/999): loss=0.3434207866350666\n",
      "Gradient Descent(975/999): loss=0.34341376180676225\n",
      "Gradient Descent(976/999): loss=0.34340675241744256\n",
      "Gradient Descent(977/999): loss=0.3433997584303188\n",
      "Gradient Descent(978/999): loss=0.34339277980870064\n",
      "Gradient Descent(979/999): loss=0.3433858165159951\n",
      "Gradient Descent(980/999): loss=0.34337886851570676\n",
      "Gradient Descent(981/999): loss=0.3433719357714373\n",
      "Gradient Descent(982/999): loss=0.343365018246885\n",
      "Gradient Descent(983/999): loss=0.34335811590584464\n",
      "Gradient Descent(984/999): loss=0.3433512287122073\n",
      "Gradient Descent(985/999): loss=0.34334435662996\n",
      "Gradient Descent(986/999): loss=0.3433374996231846\n",
      "Gradient Descent(987/999): loss=0.34333065765605947\n",
      "Gradient Descent(988/999): loss=0.343323830692857\n",
      "Gradient Descent(989/999): loss=0.3433170186979443\n",
      "Gradient Descent(990/999): loss=0.34331022163578345\n",
      "Gradient Descent(991/999): loss=0.34330343947093006\n",
      "Gradient Descent(992/999): loss=0.3432966721680339\n",
      "Gradient Descent(993/999): loss=0.34328991969183814\n",
      "Gradient Descent(994/999): loss=0.3432831820071792\n",
      "Gradient Descent(995/999): loss=0.3432764590789867\n",
      "Gradient Descent(996/999): loss=0.34326975087228206\n",
      "Gradient Descent(997/999): loss=0.34326305735218005\n",
      "Gradient Descent(998/999): loss=0.34325637848388735\n",
      "Gradient Descent(999/999): loss=0.3432497142327018\n",
      "parameters w:  [-0.31465042  0.04197621 -0.23534325 -0.17314583  0.02012779 -0.01187588\n",
      "  0.26245187 -0.01749717  0.20186582 -0.02009936 -0.00310099 -0.12085995\n",
      "  0.1184256  -0.01337476  0.18706606 -0.00096492 -0.00162282  0.18152135\n",
      " -0.00082432  0.00249176  0.08814838  0.00115061 -0.07005346 -0.10721263\n",
      "  0.02615664  0.02936407  0.02937945 -0.01606007 -0.01384844 -0.01391513\n",
      " -0.08729817]\n"
     ]
    }
   ],
   "source": [
    "from gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "# start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tX, w_initial, max_iters, gamma)\n",
    "# end_time = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/999): loss=0.5\n",
      "Stochastic Gradient Descent(1/999): loss=0.49446046879748506\n",
      "Stochastic Gradient Descent(2/999): loss=0.4903029763712601\n",
      "Stochastic Gradient Descent(3/999): loss=0.48372740455296853\n",
      "Stochastic Gradient Descent(4/999): loss=0.47416627163588976\n",
      "Stochastic Gradient Descent(5/999): loss=0.47383099454765776\n",
      "Stochastic Gradient Descent(6/999): loss=0.46841594783766943\n",
      "Stochastic Gradient Descent(7/999): loss=0.46387991991533967\n",
      "Stochastic Gradient Descent(8/999): loss=0.4610776392508727\n",
      "Stochastic Gradient Descent(9/999): loss=0.4570946036717584\n",
      "Stochastic Gradient Descent(10/999): loss=0.4551631613930794\n",
      "Stochastic Gradient Descent(11/999): loss=0.4525531092879887\n",
      "Stochastic Gradient Descent(12/999): loss=0.44965623438963453\n",
      "Stochastic Gradient Descent(13/999): loss=0.44644373187730724\n",
      "Stochastic Gradient Descent(14/999): loss=0.444014677098891\n",
      "Stochastic Gradient Descent(15/999): loss=0.4417144946204278\n",
      "Stochastic Gradient Descent(16/999): loss=0.4384074611270079\n",
      "Stochastic Gradient Descent(17/999): loss=0.4371052975401137\n",
      "Stochastic Gradient Descent(18/999): loss=0.4348478812011729\n",
      "Stochastic Gradient Descent(19/999): loss=0.43317422472082634\n",
      "Stochastic Gradient Descent(20/999): loss=0.4331864624228445\n",
      "Stochastic Gradient Descent(21/999): loss=0.43188426709238253\n",
      "Stochastic Gradient Descent(22/999): loss=0.4306542106133584\n",
      "Stochastic Gradient Descent(23/999): loss=0.42805055243331\n",
      "Stochastic Gradient Descent(24/999): loss=0.4262712001457088\n",
      "Stochastic Gradient Descent(25/999): loss=0.4253789795737998\n",
      "Stochastic Gradient Descent(26/999): loss=0.4240203789650636\n",
      "Stochastic Gradient Descent(27/999): loss=0.422835054049939\n",
      "Stochastic Gradient Descent(28/999): loss=0.4210583654320569\n",
      "Stochastic Gradient Descent(29/999): loss=0.42053155008946674\n",
      "Stochastic Gradient Descent(30/999): loss=0.4200572476234451\n",
      "Stochastic Gradient Descent(31/999): loss=0.4184420443778952\n",
      "Stochastic Gradient Descent(32/999): loss=0.41650957693533186\n",
      "Stochastic Gradient Descent(33/999): loss=0.41499670731752597\n",
      "Stochastic Gradient Descent(34/999): loss=0.41391675333794387\n",
      "Stochastic Gradient Descent(35/999): loss=0.41323027962383085\n",
      "Stochastic Gradient Descent(36/999): loss=0.41158434873828065\n",
      "Stochastic Gradient Descent(37/999): loss=0.40946509510224094\n",
      "Stochastic Gradient Descent(38/999): loss=0.4085430312629459\n",
      "Stochastic Gradient Descent(39/999): loss=0.4082430382958946\n",
      "Stochastic Gradient Descent(40/999): loss=0.40741669213532616\n",
      "Stochastic Gradient Descent(41/999): loss=0.40540171797964714\n",
      "Stochastic Gradient Descent(42/999): loss=0.40450841458628795\n",
      "Stochastic Gradient Descent(43/999): loss=0.403591002230563\n",
      "Stochastic Gradient Descent(44/999): loss=0.4023441592087238\n",
      "Stochastic Gradient Descent(45/999): loss=0.4022678597257867\n",
      "Stochastic Gradient Descent(46/999): loss=0.4004505771748713\n",
      "Stochastic Gradient Descent(47/999): loss=0.3994240316865119\n",
      "Stochastic Gradient Descent(48/999): loss=0.39800384764524027\n",
      "Stochastic Gradient Descent(49/999): loss=0.39737782091828033\n",
      "Stochastic Gradient Descent(50/999): loss=0.396657796922328\n",
      "Stochastic Gradient Descent(51/999): loss=0.39749025779842956\n",
      "Stochastic Gradient Descent(52/999): loss=0.39661859814200856\n",
      "Stochastic Gradient Descent(53/999): loss=0.3957020445414098\n",
      "Stochastic Gradient Descent(54/999): loss=0.3943933141241773\n",
      "Stochastic Gradient Descent(55/999): loss=0.3939514064512552\n",
      "Stochastic Gradient Descent(56/999): loss=0.3935286620504495\n",
      "Stochastic Gradient Descent(57/999): loss=0.39378443080640485\n",
      "Stochastic Gradient Descent(58/999): loss=0.3917470123660138\n",
      "Stochastic Gradient Descent(59/999): loss=0.3923488222590747\n",
      "Stochastic Gradient Descent(60/999): loss=0.39117305590212964\n",
      "Stochastic Gradient Descent(61/999): loss=0.39072527547720703\n",
      "Stochastic Gradient Descent(62/999): loss=0.38978971009436353\n",
      "Stochastic Gradient Descent(63/999): loss=0.388700161942276\n",
      "Stochastic Gradient Descent(64/999): loss=0.38842594001200387\n",
      "Stochastic Gradient Descent(65/999): loss=0.38701532988054643\n",
      "Stochastic Gradient Descent(66/999): loss=0.3868636295460562\n",
      "Stochastic Gradient Descent(67/999): loss=0.38630778331253557\n",
      "Stochastic Gradient Descent(68/999): loss=0.38589592885810353\n",
      "Stochastic Gradient Descent(69/999): loss=0.38586732228939363\n",
      "Stochastic Gradient Descent(70/999): loss=0.38543426749803683\n",
      "Stochastic Gradient Descent(71/999): loss=0.3853189547795705\n",
      "Stochastic Gradient Descent(72/999): loss=0.38548895016056284\n",
      "Stochastic Gradient Descent(73/999): loss=0.38502113863998416\n",
      "Stochastic Gradient Descent(74/999): loss=0.3844804358770573\n",
      "Stochastic Gradient Descent(75/999): loss=0.3841380353912218\n",
      "Stochastic Gradient Descent(76/999): loss=0.38348490816258196\n",
      "Stochastic Gradient Descent(77/999): loss=0.38267688759073215\n",
      "Stochastic Gradient Descent(78/999): loss=0.38206426035420155\n",
      "Stochastic Gradient Descent(79/999): loss=0.3818685395637876\n",
      "Stochastic Gradient Descent(80/999): loss=0.3814119773699184\n",
      "Stochastic Gradient Descent(81/999): loss=0.3812665154572697\n",
      "Stochastic Gradient Descent(82/999): loss=0.3809561233463908\n",
      "Stochastic Gradient Descent(83/999): loss=0.38114589237498053\n",
      "Stochastic Gradient Descent(84/999): loss=0.3804643167015414\n",
      "Stochastic Gradient Descent(85/999): loss=0.38127253644538023\n",
      "Stochastic Gradient Descent(86/999): loss=0.3807081841146406\n",
      "Stochastic Gradient Descent(87/999): loss=0.379912120490343\n",
      "Stochastic Gradient Descent(88/999): loss=0.38029858748466777\n",
      "Stochastic Gradient Descent(89/999): loss=0.37879838399512233\n",
      "Stochastic Gradient Descent(90/999): loss=0.37886398682937766\n",
      "Stochastic Gradient Descent(91/999): loss=0.3785966719028399\n",
      "Stochastic Gradient Descent(92/999): loss=0.3778793825668178\n",
      "Stochastic Gradient Descent(93/999): loss=0.37763270771721635\n",
      "Stochastic Gradient Descent(94/999): loss=0.37743624268642173\n",
      "Stochastic Gradient Descent(95/999): loss=0.3770675407480779\n",
      "Stochastic Gradient Descent(96/999): loss=0.37679401861106404\n",
      "Stochastic Gradient Descent(97/999): loss=0.37718092136540965\n",
      "Stochastic Gradient Descent(98/999): loss=0.3768789555835173\n",
      "Stochastic Gradient Descent(99/999): loss=0.37652544429668006\n",
      "Stochastic Gradient Descent(100/999): loss=0.3762308425229413\n",
      "Stochastic Gradient Descent(101/999): loss=0.37585437736254357\n",
      "Stochastic Gradient Descent(102/999): loss=0.3751684359352319\n",
      "Stochastic Gradient Descent(103/999): loss=0.3755555666621424\n",
      "Stochastic Gradient Descent(104/999): loss=0.3751362587347509\n",
      "Stochastic Gradient Descent(105/999): loss=0.3746456032010504\n",
      "Stochastic Gradient Descent(106/999): loss=0.3748150994221821\n",
      "Stochastic Gradient Descent(107/999): loss=0.374208291295195\n",
      "Stochastic Gradient Descent(108/999): loss=0.37355361836477147\n",
      "Stochastic Gradient Descent(109/999): loss=0.3734249158258801\n",
      "Stochastic Gradient Descent(110/999): loss=0.3734272180825436\n",
      "Stochastic Gradient Descent(111/999): loss=0.37280959723587664\n",
      "Stochastic Gradient Descent(112/999): loss=0.3724632483524157\n",
      "Stochastic Gradient Descent(113/999): loss=0.37191717820128234\n",
      "Stochastic Gradient Descent(114/999): loss=0.37158478161845365\n",
      "Stochastic Gradient Descent(115/999): loss=0.3715965413718814\n",
      "Stochastic Gradient Descent(116/999): loss=0.3726665919851102\n",
      "Stochastic Gradient Descent(117/999): loss=0.37124635195839156\n",
      "Stochastic Gradient Descent(118/999): loss=0.3709774278134489\n",
      "Stochastic Gradient Descent(119/999): loss=0.37075374055758836\n",
      "Stochastic Gradient Descent(120/999): loss=0.3705228443800232\n",
      "Stochastic Gradient Descent(121/999): loss=0.3704212011361728\n",
      "Stochastic Gradient Descent(122/999): loss=0.3702341126132043\n",
      "Stochastic Gradient Descent(123/999): loss=0.3701981301819013\n",
      "Stochastic Gradient Descent(124/999): loss=0.3700893717321372\n",
      "Stochastic Gradient Descent(125/999): loss=0.3696137650189454\n",
      "Stochastic Gradient Descent(126/999): loss=0.36937031760781214\n",
      "Stochastic Gradient Descent(127/999): loss=0.3695936191519218\n",
      "Stochastic Gradient Descent(128/999): loss=0.3700336155849346\n",
      "Stochastic Gradient Descent(129/999): loss=0.3701854727389717\n",
      "Stochastic Gradient Descent(130/999): loss=0.37056592928359877\n",
      "Stochastic Gradient Descent(131/999): loss=0.370348896553672\n",
      "Stochastic Gradient Descent(132/999): loss=0.3702531432902165\n",
      "Stochastic Gradient Descent(133/999): loss=0.3699640054075567\n",
      "Stochastic Gradient Descent(134/999): loss=0.369717565239402\n",
      "Stochastic Gradient Descent(135/999): loss=0.3690307743450781\n",
      "Stochastic Gradient Descent(136/999): loss=0.36875417860123416\n",
      "Stochastic Gradient Descent(137/999): loss=0.3685777234192471\n",
      "Stochastic Gradient Descent(138/999): loss=0.3682263141383381\n",
      "Stochastic Gradient Descent(139/999): loss=0.368014524171103\n",
      "Stochastic Gradient Descent(140/999): loss=0.3677921078497132\n",
      "Stochastic Gradient Descent(141/999): loss=0.36779162252953257\n",
      "Stochastic Gradient Descent(142/999): loss=0.36768670764864575\n",
      "Stochastic Gradient Descent(143/999): loss=0.36748117458151724\n",
      "Stochastic Gradient Descent(144/999): loss=0.3675568850201535\n",
      "Stochastic Gradient Descent(145/999): loss=0.36721139453404955\n",
      "Stochastic Gradient Descent(146/999): loss=0.36702060339137127\n",
      "Stochastic Gradient Descent(147/999): loss=0.36697511749903844\n",
      "Stochastic Gradient Descent(148/999): loss=0.366599074801889\n",
      "Stochastic Gradient Descent(149/999): loss=0.36653871270806254\n",
      "Stochastic Gradient Descent(150/999): loss=0.36650482889096697\n",
      "Stochastic Gradient Descent(151/999): loss=0.3662191828386308\n",
      "Stochastic Gradient Descent(152/999): loss=0.36618198028514604\n",
      "Stochastic Gradient Descent(153/999): loss=0.3660830739908677\n",
      "Stochastic Gradient Descent(154/999): loss=0.3665388689027495\n",
      "Stochastic Gradient Descent(155/999): loss=0.3676658863451158\n",
      "Stochastic Gradient Descent(156/999): loss=0.36723863216352876\n",
      "Stochastic Gradient Descent(157/999): loss=0.366349596581186\n",
      "Stochastic Gradient Descent(158/999): loss=0.3658683034624987\n",
      "Stochastic Gradient Descent(159/999): loss=0.36566663082282036\n",
      "Stochastic Gradient Descent(160/999): loss=0.3656401735159144\n",
      "Stochastic Gradient Descent(161/999): loss=0.36548233192008106\n",
      "Stochastic Gradient Descent(162/999): loss=0.3655899608767929\n",
      "Stochastic Gradient Descent(163/999): loss=0.3655321278961846\n",
      "Stochastic Gradient Descent(164/999): loss=0.36536453070555164\n",
      "Stochastic Gradient Descent(165/999): loss=0.3652437551303582\n",
      "Stochastic Gradient Descent(166/999): loss=0.3649765820069085\n",
      "Stochastic Gradient Descent(167/999): loss=0.3649975635141975\n",
      "Stochastic Gradient Descent(168/999): loss=0.36492913741212923\n",
      "Stochastic Gradient Descent(169/999): loss=0.36470731491002034\n",
      "Stochastic Gradient Descent(170/999): loss=0.3646094658924702\n",
      "Stochastic Gradient Descent(171/999): loss=0.36442387393587256\n",
      "Stochastic Gradient Descent(172/999): loss=0.3643232453254759\n",
      "Stochastic Gradient Descent(173/999): loss=0.3645689435728847\n",
      "Stochastic Gradient Descent(174/999): loss=0.36412993473325317\n",
      "Stochastic Gradient Descent(175/999): loss=0.3639039066179923\n",
      "Stochastic Gradient Descent(176/999): loss=0.36372183115476125\n",
      "Stochastic Gradient Descent(177/999): loss=0.3635862123406665\n",
      "Stochastic Gradient Descent(178/999): loss=0.36352492713437057\n",
      "Stochastic Gradient Descent(179/999): loss=0.3630696540364836\n",
      "Stochastic Gradient Descent(180/999): loss=0.36390227195308145\n",
      "Stochastic Gradient Descent(181/999): loss=0.3644617552027756\n",
      "Stochastic Gradient Descent(182/999): loss=0.3637969185427862\n",
      "Stochastic Gradient Descent(183/999): loss=0.36376530947485614\n",
      "Stochastic Gradient Descent(184/999): loss=0.36378973588084684\n",
      "Stochastic Gradient Descent(185/999): loss=0.36348319111525546\n",
      "Stochastic Gradient Descent(186/999): loss=0.36322024520450463\n",
      "Stochastic Gradient Descent(187/999): loss=0.36309700167199194\n",
      "Stochastic Gradient Descent(188/999): loss=0.36329627999961994\n",
      "Stochastic Gradient Descent(189/999): loss=0.3633296413830972\n",
      "Stochastic Gradient Descent(190/999): loss=0.36336419097885453\n",
      "Stochastic Gradient Descent(191/999): loss=0.36390892708703865\n",
      "Stochastic Gradient Descent(192/999): loss=0.36368951243120623\n",
      "Stochastic Gradient Descent(193/999): loss=0.362969163057656\n",
      "Stochastic Gradient Descent(194/999): loss=0.36258888434335285\n",
      "Stochastic Gradient Descent(195/999): loss=0.3628052831079902\n",
      "Stochastic Gradient Descent(196/999): loss=0.3623793727827442\n",
      "Stochastic Gradient Descent(197/999): loss=0.36199622916541924\n",
      "Stochastic Gradient Descent(198/999): loss=0.36205703313894244\n",
      "Stochastic Gradient Descent(199/999): loss=0.3619338724391873\n",
      "Stochastic Gradient Descent(200/999): loss=0.361804172297339\n",
      "Stochastic Gradient Descent(201/999): loss=0.3616115934398727\n",
      "Stochastic Gradient Descent(202/999): loss=0.36143935549648615\n",
      "Stochastic Gradient Descent(203/999): loss=0.36164149763738945\n",
      "Stochastic Gradient Descent(204/999): loss=0.3614514591284338\n",
      "Stochastic Gradient Descent(205/999): loss=0.3626403682853402\n",
      "Stochastic Gradient Descent(206/999): loss=0.3632967654957934\n",
      "Stochastic Gradient Descent(207/999): loss=0.3631347337857286\n",
      "Stochastic Gradient Descent(208/999): loss=0.3629908047216958\n",
      "Stochastic Gradient Descent(209/999): loss=0.36203143387321046\n",
      "Stochastic Gradient Descent(210/999): loss=0.36187743312726883\n",
      "Stochastic Gradient Descent(211/999): loss=0.36319938361257226\n",
      "Stochastic Gradient Descent(212/999): loss=0.3618278299259647\n",
      "Stochastic Gradient Descent(213/999): loss=0.3618225853656378\n",
      "Stochastic Gradient Descent(214/999): loss=0.360848210191768\n",
      "Stochastic Gradient Descent(215/999): loss=0.36048766926777587\n",
      "Stochastic Gradient Descent(216/999): loss=0.3602851143627279\n",
      "Stochastic Gradient Descent(217/999): loss=0.3602452705757461\n",
      "Stochastic Gradient Descent(218/999): loss=0.3600473658422079\n",
      "Stochastic Gradient Descent(219/999): loss=0.360467070935938\n",
      "Stochastic Gradient Descent(220/999): loss=0.3600658912665151\n",
      "Stochastic Gradient Descent(221/999): loss=0.36009892577154445\n",
      "Stochastic Gradient Descent(222/999): loss=0.3602245780999614\n",
      "Stochastic Gradient Descent(223/999): loss=0.3599580992157604\n",
      "Stochastic Gradient Descent(224/999): loss=0.35999359651005797\n",
      "Stochastic Gradient Descent(225/999): loss=0.35988005765229925\n",
      "Stochastic Gradient Descent(226/999): loss=0.35995207912315413\n",
      "Stochastic Gradient Descent(227/999): loss=0.3598392357788106\n",
      "Stochastic Gradient Descent(228/999): loss=0.360389308914036\n",
      "Stochastic Gradient Descent(229/999): loss=0.36029299662201963\n",
      "Stochastic Gradient Descent(230/999): loss=0.360153614775271\n",
      "Stochastic Gradient Descent(231/999): loss=0.35984788290952985\n",
      "Stochastic Gradient Descent(232/999): loss=0.35982576264606536\n",
      "Stochastic Gradient Descent(233/999): loss=0.3599382047401024\n",
      "Stochastic Gradient Descent(234/999): loss=0.3597604146510294\n",
      "Stochastic Gradient Descent(235/999): loss=0.3595042970622244\n",
      "Stochastic Gradient Descent(236/999): loss=0.35946887398451566\n",
      "Stochastic Gradient Descent(237/999): loss=0.35945277528168773\n",
      "Stochastic Gradient Descent(238/999): loss=0.35943383315194066\n",
      "Stochastic Gradient Descent(239/999): loss=0.3594106556040079\n",
      "Stochastic Gradient Descent(240/999): loss=0.3593890286412\n",
      "Stochastic Gradient Descent(241/999): loss=0.35922257383822487\n",
      "Stochastic Gradient Descent(242/999): loss=0.3592901898289208\n",
      "Stochastic Gradient Descent(243/999): loss=0.35913389080142905\n",
      "Stochastic Gradient Descent(244/999): loss=0.3594121401928174\n",
      "Stochastic Gradient Descent(245/999): loss=0.3590156162774625\n",
      "Stochastic Gradient Descent(246/999): loss=0.3587357692533623\n",
      "Stochastic Gradient Descent(247/999): loss=0.35867387696206854\n",
      "Stochastic Gradient Descent(248/999): loss=0.3587001373974993\n",
      "Stochastic Gradient Descent(249/999): loss=0.35880389665118423\n",
      "Stochastic Gradient Descent(250/999): loss=0.35861694547676\n",
      "Stochastic Gradient Descent(251/999): loss=0.3586745634122274\n",
      "Stochastic Gradient Descent(252/999): loss=0.358734839250711\n",
      "Stochastic Gradient Descent(253/999): loss=0.359090932837616\n",
      "Stochastic Gradient Descent(254/999): loss=0.3586880927858585\n",
      "Stochastic Gradient Descent(255/999): loss=0.35862842265621053\n",
      "Stochastic Gradient Descent(256/999): loss=0.35858229863420055\n",
      "Stochastic Gradient Descent(257/999): loss=0.3587848001711758\n",
      "Stochastic Gradient Descent(258/999): loss=0.3593702895698795\n",
      "Stochastic Gradient Descent(259/999): loss=0.35900411161291385\n",
      "Stochastic Gradient Descent(260/999): loss=0.35933773143973696\n",
      "Stochastic Gradient Descent(261/999): loss=0.3589447823229568\n",
      "Stochastic Gradient Descent(262/999): loss=0.3591273131428971\n",
      "Stochastic Gradient Descent(263/999): loss=0.3588217854857197\n",
      "Stochastic Gradient Descent(264/999): loss=0.35875182968293584\n",
      "Stochastic Gradient Descent(265/999): loss=0.36010514481674255\n",
      "Stochastic Gradient Descent(266/999): loss=0.36004680680653184\n",
      "Stochastic Gradient Descent(267/999): loss=0.35984906355181995\n",
      "Stochastic Gradient Descent(268/999): loss=0.35870576326503234\n",
      "Stochastic Gradient Descent(269/999): loss=0.3584963488781313\n",
      "Stochastic Gradient Descent(270/999): loss=0.35851901877972686\n",
      "Stochastic Gradient Descent(271/999): loss=0.35781618552859884\n",
      "Stochastic Gradient Descent(272/999): loss=0.3580127338358073\n",
      "Stochastic Gradient Descent(273/999): loss=0.35795684473058054\n",
      "Stochastic Gradient Descent(274/999): loss=0.3579615036153788\n",
      "Stochastic Gradient Descent(275/999): loss=0.35785115832125997\n",
      "Stochastic Gradient Descent(276/999): loss=0.3576832680850755\n",
      "Stochastic Gradient Descent(277/999): loss=0.3575767255984848\n",
      "Stochastic Gradient Descent(278/999): loss=0.3580533982229126\n",
      "Stochastic Gradient Descent(279/999): loss=0.35770441186091906\n",
      "Stochastic Gradient Descent(280/999): loss=0.3575605669509602\n",
      "Stochastic Gradient Descent(281/999): loss=0.35765830175511204\n",
      "Stochastic Gradient Descent(282/999): loss=0.3576677183023324\n",
      "Stochastic Gradient Descent(283/999): loss=0.3584039995262975\n",
      "Stochastic Gradient Descent(284/999): loss=0.3586008120267365\n",
      "Stochastic Gradient Descent(285/999): loss=0.35877784811990354\n",
      "Stochastic Gradient Descent(286/999): loss=0.35850784665737107\n",
      "Stochastic Gradient Descent(287/999): loss=0.3588210871456043\n",
      "Stochastic Gradient Descent(288/999): loss=0.35819086582725795\n",
      "Stochastic Gradient Descent(289/999): loss=0.35742100057487014\n",
      "Stochastic Gradient Descent(290/999): loss=0.35728567236260184\n",
      "Stochastic Gradient Descent(291/999): loss=0.357330753518317\n",
      "Stochastic Gradient Descent(292/999): loss=0.35817231303444025\n",
      "Stochastic Gradient Descent(293/999): loss=0.3589808264900131\n",
      "Stochastic Gradient Descent(294/999): loss=0.3586765993257201\n",
      "Stochastic Gradient Descent(295/999): loss=0.35728624453861085\n",
      "Stochastic Gradient Descent(296/999): loss=0.3573900965074654\n",
      "Stochastic Gradient Descent(297/999): loss=0.3568008211825618\n",
      "Stochastic Gradient Descent(298/999): loss=0.3566310831606907\n",
      "Stochastic Gradient Descent(299/999): loss=0.3565937071549365\n",
      "Stochastic Gradient Descent(300/999): loss=0.35740830956788555\n",
      "Stochastic Gradient Descent(301/999): loss=0.35696557965856934\n",
      "Stochastic Gradient Descent(302/999): loss=0.35620577175831825\n",
      "Stochastic Gradient Descent(303/999): loss=0.35621620057737113\n",
      "Stochastic Gradient Descent(304/999): loss=0.3561784273862553\n",
      "Stochastic Gradient Descent(305/999): loss=0.3563848028100737\n",
      "Stochastic Gradient Descent(306/999): loss=0.35620141384379483\n",
      "Stochastic Gradient Descent(307/999): loss=0.3561823055380666\n",
      "Stochastic Gradient Descent(308/999): loss=0.35618477151656047\n",
      "Stochastic Gradient Descent(309/999): loss=0.357378097925528\n",
      "Stochastic Gradient Descent(310/999): loss=0.3568930042293186\n",
      "Stochastic Gradient Descent(311/999): loss=0.35779031695153385\n",
      "Stochastic Gradient Descent(312/999): loss=0.3577787574223254\n",
      "Stochastic Gradient Descent(313/999): loss=0.35745769487694334\n",
      "Stochastic Gradient Descent(314/999): loss=0.3566449848426225\n",
      "Stochastic Gradient Descent(315/999): loss=0.3566056142895863\n",
      "Stochastic Gradient Descent(316/999): loss=0.35654412996625\n",
      "Stochastic Gradient Descent(317/999): loss=0.35622871501785175\n",
      "Stochastic Gradient Descent(318/999): loss=0.35619857445423775\n",
      "Stochastic Gradient Descent(319/999): loss=0.3560120104782394\n",
      "Stochastic Gradient Descent(320/999): loss=0.35593335958450806\n",
      "Stochastic Gradient Descent(321/999): loss=0.3557845934287371\n",
      "Stochastic Gradient Descent(322/999): loss=0.3556931315848796\n",
      "Stochastic Gradient Descent(323/999): loss=0.35555265083376153\n",
      "Stochastic Gradient Descent(324/999): loss=0.35554765459892557\n",
      "Stochastic Gradient Descent(325/999): loss=0.35559409929257185\n",
      "Stochastic Gradient Descent(326/999): loss=0.3553019529714216\n",
      "Stochastic Gradient Descent(327/999): loss=0.35605778145618355\n",
      "Stochastic Gradient Descent(328/999): loss=0.35551852331458855\n",
      "Stochastic Gradient Descent(329/999): loss=0.3561032539884977\n",
      "Stochastic Gradient Descent(330/999): loss=0.3559340958203433\n",
      "Stochastic Gradient Descent(331/999): loss=0.3556907482921319\n",
      "Stochastic Gradient Descent(332/999): loss=0.3561120544294346\n",
      "Stochastic Gradient Descent(333/999): loss=0.35626471694690665\n",
      "Stochastic Gradient Descent(334/999): loss=0.35621947585018815\n",
      "Stochastic Gradient Descent(335/999): loss=0.35686583924544857\n",
      "Stochastic Gradient Descent(336/999): loss=0.3560461069847624\n",
      "Stochastic Gradient Descent(337/999): loss=0.35550274470693605\n",
      "Stochastic Gradient Descent(338/999): loss=0.3549585638343431\n",
      "Stochastic Gradient Descent(339/999): loss=0.35466526869836845\n",
      "Stochastic Gradient Descent(340/999): loss=0.3546278937865035\n",
      "Stochastic Gradient Descent(341/999): loss=0.3546889100096605\n",
      "Stochastic Gradient Descent(342/999): loss=0.35455491912086035\n",
      "Stochastic Gradient Descent(343/999): loss=0.354543795047826\n",
      "Stochastic Gradient Descent(344/999): loss=0.3544630044874862\n",
      "Stochastic Gradient Descent(345/999): loss=0.35461071402526656\n",
      "Stochastic Gradient Descent(346/999): loss=0.35457276631746265\n",
      "Stochastic Gradient Descent(347/999): loss=0.35451456964220024\n",
      "Stochastic Gradient Descent(348/999): loss=0.3543214480073629\n",
      "Stochastic Gradient Descent(349/999): loss=0.3543725267514327\n",
      "Stochastic Gradient Descent(350/999): loss=0.3543410344450183\n",
      "Stochastic Gradient Descent(351/999): loss=0.3543220536331846\n",
      "Stochastic Gradient Descent(352/999): loss=0.3542583601911737\n",
      "Stochastic Gradient Descent(353/999): loss=0.35431386011376886\n",
      "Stochastic Gradient Descent(354/999): loss=0.35414392873818096\n",
      "Stochastic Gradient Descent(355/999): loss=0.3540716530099786\n",
      "Stochastic Gradient Descent(356/999): loss=0.35402387087986503\n",
      "Stochastic Gradient Descent(357/999): loss=0.3540134920132916\n",
      "Stochastic Gradient Descent(358/999): loss=0.35398116198695617\n",
      "Stochastic Gradient Descent(359/999): loss=0.3539093344922634\n",
      "Stochastic Gradient Descent(360/999): loss=0.35409905181633095\n",
      "Stochastic Gradient Descent(361/999): loss=0.3539433290687554\n",
      "Stochastic Gradient Descent(362/999): loss=0.3542310562121418\n",
      "Stochastic Gradient Descent(363/999): loss=0.3539616126169388\n",
      "Stochastic Gradient Descent(364/999): loss=0.35383031766852197\n",
      "Stochastic Gradient Descent(365/999): loss=0.3538710478078092\n",
      "Stochastic Gradient Descent(366/999): loss=0.3536331463191024\n",
      "Stochastic Gradient Descent(367/999): loss=0.35363412801733957\n",
      "Stochastic Gradient Descent(368/999): loss=0.35375037149720384\n",
      "Stochastic Gradient Descent(369/999): loss=0.35393355401712406\n",
      "Stochastic Gradient Descent(370/999): loss=0.3538844456545209\n",
      "Stochastic Gradient Descent(371/999): loss=0.3536205476719022\n",
      "Stochastic Gradient Descent(372/999): loss=0.3534456001539383\n",
      "Stochastic Gradient Descent(373/999): loss=0.35365249103508173\n",
      "Stochastic Gradient Descent(374/999): loss=0.3538611344884915\n",
      "Stochastic Gradient Descent(375/999): loss=0.3536626338444623\n",
      "Stochastic Gradient Descent(376/999): loss=0.35377454888836113\n",
      "Stochastic Gradient Descent(377/999): loss=0.3537570147981344\n",
      "Stochastic Gradient Descent(378/999): loss=0.3536100575006779\n",
      "Stochastic Gradient Descent(379/999): loss=0.3535843988540723\n",
      "Stochastic Gradient Descent(380/999): loss=0.3536271128420458\n",
      "Stochastic Gradient Descent(381/999): loss=0.35360639717056896\n",
      "Stochastic Gradient Descent(382/999): loss=0.35344367256226245\n",
      "Stochastic Gradient Descent(383/999): loss=0.3534025986103604\n",
      "Stochastic Gradient Descent(384/999): loss=0.353441143260216\n",
      "Stochastic Gradient Descent(385/999): loss=0.35471411176556966\n",
      "Stochastic Gradient Descent(386/999): loss=0.355012573950606\n",
      "Stochastic Gradient Descent(387/999): loss=0.35533641190095644\n",
      "Stochastic Gradient Descent(388/999): loss=0.35669235396844806\n",
      "Stochastic Gradient Descent(389/999): loss=0.35576039106656854\n",
      "Stochastic Gradient Descent(390/999): loss=0.3544560641852395\n",
      "Stochastic Gradient Descent(391/999): loss=0.3539690171229198\n",
      "Stochastic Gradient Descent(392/999): loss=0.3536181719141897\n",
      "Stochastic Gradient Descent(393/999): loss=0.35361164277884544\n",
      "Stochastic Gradient Descent(394/999): loss=0.3532301644810876\n",
      "Stochastic Gradient Descent(395/999): loss=0.35324126020918956\n",
      "Stochastic Gradient Descent(396/999): loss=0.35336548321506867\n",
      "Stochastic Gradient Descent(397/999): loss=0.3535922878255496\n",
      "Stochastic Gradient Descent(398/999): loss=0.3532645874260095\n",
      "Stochastic Gradient Descent(399/999): loss=0.35334306307369034\n",
      "Stochastic Gradient Descent(400/999): loss=0.3531755774611384\n",
      "Stochastic Gradient Descent(401/999): loss=0.3530750569707956\n",
      "Stochastic Gradient Descent(402/999): loss=0.3533060550844204\n",
      "Stochastic Gradient Descent(403/999): loss=0.3539259317985336\n",
      "Stochastic Gradient Descent(404/999): loss=0.35313915583571515\n",
      "Stochastic Gradient Descent(405/999): loss=0.3531438928305648\n",
      "Stochastic Gradient Descent(406/999): loss=0.3530492165399877\n",
      "Stochastic Gradient Descent(407/999): loss=0.3533371239075276\n",
      "Stochastic Gradient Descent(408/999): loss=0.35297033232301317\n",
      "Stochastic Gradient Descent(409/999): loss=0.3530675307055258\n",
      "Stochastic Gradient Descent(410/999): loss=0.3528527677445582\n",
      "Stochastic Gradient Descent(411/999): loss=0.35266574064867284\n",
      "Stochastic Gradient Descent(412/999): loss=0.35265331163802777\n",
      "Stochastic Gradient Descent(413/999): loss=0.35287014625008345\n",
      "Stochastic Gradient Descent(414/999): loss=0.35265259164347973\n",
      "Stochastic Gradient Descent(415/999): loss=0.3527211771232059\n",
      "Stochastic Gradient Descent(416/999): loss=0.3527226696005721\n",
      "Stochastic Gradient Descent(417/999): loss=0.352887936713869\n",
      "Stochastic Gradient Descent(418/999): loss=0.35285836729791326\n",
      "Stochastic Gradient Descent(419/999): loss=0.3531935212895182\n",
      "Stochastic Gradient Descent(420/999): loss=0.353232675600062\n",
      "Stochastic Gradient Descent(421/999): loss=0.3533998605783736\n",
      "Stochastic Gradient Descent(422/999): loss=0.35296482086025466\n",
      "Stochastic Gradient Descent(423/999): loss=0.3537192886465574\n",
      "Stochastic Gradient Descent(424/999): loss=0.35327909216882036\n",
      "Stochastic Gradient Descent(425/999): loss=0.35292183714811015\n",
      "Stochastic Gradient Descent(426/999): loss=0.35326255618595304\n",
      "Stochastic Gradient Descent(427/999): loss=0.3528151438013098\n",
      "Stochastic Gradient Descent(428/999): loss=0.3531261943505578\n",
      "Stochastic Gradient Descent(429/999): loss=0.3524032584304855\n",
      "Stochastic Gradient Descent(430/999): loss=0.35256850193972034\n",
      "Stochastic Gradient Descent(431/999): loss=0.35244048541565914\n",
      "Stochastic Gradient Descent(432/999): loss=0.3523988230999187\n",
      "Stochastic Gradient Descent(433/999): loss=0.3526245353418508\n",
      "Stochastic Gradient Descent(434/999): loss=0.3525575412060671\n",
      "Stochastic Gradient Descent(435/999): loss=0.3524595661140496\n",
      "Stochastic Gradient Descent(436/999): loss=0.35243494187803154\n",
      "Stochastic Gradient Descent(437/999): loss=0.3525845281436314\n",
      "Stochastic Gradient Descent(438/999): loss=0.35257219866981243\n",
      "Stochastic Gradient Descent(439/999): loss=0.352978604011848\n",
      "Stochastic Gradient Descent(440/999): loss=0.3531386384838946\n",
      "Stochastic Gradient Descent(441/999): loss=0.35246658911127743\n",
      "Stochastic Gradient Descent(442/999): loss=0.3526457011152588\n",
      "Stochastic Gradient Descent(443/999): loss=0.35296174455422064\n",
      "Stochastic Gradient Descent(444/999): loss=0.35245364869218654\n",
      "Stochastic Gradient Descent(445/999): loss=0.35223948096102325\n",
      "Stochastic Gradient Descent(446/999): loss=0.3524099496197733\n",
      "Stochastic Gradient Descent(447/999): loss=0.35217192627192356\n",
      "Stochastic Gradient Descent(448/999): loss=0.3520000625413184\n",
      "Stochastic Gradient Descent(449/999): loss=0.35194443968377515\n",
      "Stochastic Gradient Descent(450/999): loss=0.3519095785736178\n",
      "Stochastic Gradient Descent(451/999): loss=0.35227448356328905\n",
      "Stochastic Gradient Descent(452/999): loss=0.3523900391668194\n",
      "Stochastic Gradient Descent(453/999): loss=0.35218574587390833\n",
      "Stochastic Gradient Descent(454/999): loss=0.35226111845629154\n",
      "Stochastic Gradient Descent(455/999): loss=0.35197461257570534\n",
      "Stochastic Gradient Descent(456/999): loss=0.35236934956210614\n",
      "Stochastic Gradient Descent(457/999): loss=0.3521723910211015\n",
      "Stochastic Gradient Descent(458/999): loss=0.3521266992453164\n",
      "Stochastic Gradient Descent(459/999): loss=0.3517569389437365\n",
      "Stochastic Gradient Descent(460/999): loss=0.35245159466082804\n",
      "Stochastic Gradient Descent(461/999): loss=0.35243774821566615\n",
      "Stochastic Gradient Descent(462/999): loss=0.35241429194029783\n",
      "Stochastic Gradient Descent(463/999): loss=0.3521730444274355\n",
      "Stochastic Gradient Descent(464/999): loss=0.3515026523766264\n",
      "Stochastic Gradient Descent(465/999): loss=0.3516117977463713\n",
      "Stochastic Gradient Descent(466/999): loss=0.35165626350250145\n",
      "Stochastic Gradient Descent(467/999): loss=0.3515973445789168\n",
      "Stochastic Gradient Descent(468/999): loss=0.35158784221744194\n",
      "Stochastic Gradient Descent(469/999): loss=0.3525670865590244\n",
      "Stochastic Gradient Descent(470/999): loss=0.3514912917294428\n",
      "Stochastic Gradient Descent(471/999): loss=0.3514904959414371\n",
      "Stochastic Gradient Descent(472/999): loss=0.35146425313678326\n",
      "Stochastic Gradient Descent(473/999): loss=0.3515469217746939\n",
      "Stochastic Gradient Descent(474/999): loss=0.3517229099300548\n",
      "Stochastic Gradient Descent(475/999): loss=0.35164967636366573\n",
      "Stochastic Gradient Descent(476/999): loss=0.35134443087455053\n",
      "Stochastic Gradient Descent(477/999): loss=0.351581668299809\n",
      "Stochastic Gradient Descent(478/999): loss=0.351750739644743\n",
      "Stochastic Gradient Descent(479/999): loss=0.3519416650051681\n",
      "Stochastic Gradient Descent(480/999): loss=0.3520969197446138\n",
      "Stochastic Gradient Descent(481/999): loss=0.3520850964638697\n",
      "Stochastic Gradient Descent(482/999): loss=0.35273994207013243\n",
      "Stochastic Gradient Descent(483/999): loss=0.3523144384626631\n",
      "Stochastic Gradient Descent(484/999): loss=0.3518807840475202\n",
      "Stochastic Gradient Descent(485/999): loss=0.3518476209837529\n",
      "Stochastic Gradient Descent(486/999): loss=0.3518284961582403\n",
      "Stochastic Gradient Descent(487/999): loss=0.3517376308062238\n",
      "Stochastic Gradient Descent(488/999): loss=0.3521138041834591\n",
      "Stochastic Gradient Descent(489/999): loss=0.3522154129592997\n",
      "Stochastic Gradient Descent(490/999): loss=0.35146037030689886\n",
      "Stochastic Gradient Descent(491/999): loss=0.35133234172172484\n",
      "Stochastic Gradient Descent(492/999): loss=0.3513307752607319\n",
      "Stochastic Gradient Descent(493/999): loss=0.35174783140315435\n",
      "Stochastic Gradient Descent(494/999): loss=0.3520695454271573\n",
      "Stochastic Gradient Descent(495/999): loss=0.35244665505394973\n",
      "Stochastic Gradient Descent(496/999): loss=0.3523261009067994\n",
      "Stochastic Gradient Descent(497/999): loss=0.3517616952731267\n",
      "Stochastic Gradient Descent(498/999): loss=0.35105781781049816\n",
      "Stochastic Gradient Descent(499/999): loss=0.3508817612176095\n",
      "Stochastic Gradient Descent(500/999): loss=0.3508035989023884\n",
      "Stochastic Gradient Descent(501/999): loss=0.35102888009867056\n",
      "Stochastic Gradient Descent(502/999): loss=0.350983238359144\n",
      "Stochastic Gradient Descent(503/999): loss=0.35083975031500025\n",
      "Stochastic Gradient Descent(504/999): loss=0.35194029826710466\n",
      "Stochastic Gradient Descent(505/999): loss=0.35223772219076327\n",
      "Stochastic Gradient Descent(506/999): loss=0.3518062586892929\n",
      "Stochastic Gradient Descent(507/999): loss=0.35105395611168294\n",
      "Stochastic Gradient Descent(508/999): loss=0.35056628318917\n",
      "Stochastic Gradient Descent(509/999): loss=0.3504515099332808\n",
      "Stochastic Gradient Descent(510/999): loss=0.3504447341867095\n",
      "Stochastic Gradient Descent(511/999): loss=0.35027116265074987\n",
      "Stochastic Gradient Descent(512/999): loss=0.35026774920702075\n",
      "Stochastic Gradient Descent(513/999): loss=0.35022173150184727\n",
      "Stochastic Gradient Descent(514/999): loss=0.35057895675928713\n",
      "Stochastic Gradient Descent(515/999): loss=0.35016762661138495\n",
      "Stochastic Gradient Descent(516/999): loss=0.35019993043549685\n",
      "Stochastic Gradient Descent(517/999): loss=0.35035526732619326\n",
      "Stochastic Gradient Descent(518/999): loss=0.3502593580923783\n",
      "Stochastic Gradient Descent(519/999): loss=0.35034726644825304\n",
      "Stochastic Gradient Descent(520/999): loss=0.35028553021639575\n",
      "Stochastic Gradient Descent(521/999): loss=0.3501932595215823\n",
      "Stochastic Gradient Descent(522/999): loss=0.35097712839907336\n",
      "Stochastic Gradient Descent(523/999): loss=0.3518790050147002\n",
      "Stochastic Gradient Descent(524/999): loss=0.3509830926020481\n",
      "Stochastic Gradient Descent(525/999): loss=0.3504676375848033\n",
      "Stochastic Gradient Descent(526/999): loss=0.35084862904676734\n",
      "Stochastic Gradient Descent(527/999): loss=0.3501931785517061\n",
      "Stochastic Gradient Descent(528/999): loss=0.35049044299165616\n",
      "Stochastic Gradient Descent(529/999): loss=0.35013041272148193\n",
      "Stochastic Gradient Descent(530/999): loss=0.3499012777332957\n",
      "Stochastic Gradient Descent(531/999): loss=0.3502953753380706\n",
      "Stochastic Gradient Descent(532/999): loss=0.3505588881073429\n",
      "Stochastic Gradient Descent(533/999): loss=0.3502589678325935\n",
      "Stochastic Gradient Descent(534/999): loss=0.35009384267199967\n",
      "Stochastic Gradient Descent(535/999): loss=0.35141281403919944\n",
      "Stochastic Gradient Descent(536/999): loss=0.35034658146725345\n",
      "Stochastic Gradient Descent(537/999): loss=0.34998645541396023\n",
      "Stochastic Gradient Descent(538/999): loss=0.3500451639557045\n",
      "Stochastic Gradient Descent(539/999): loss=0.3515353695298113\n",
      "Stochastic Gradient Descent(540/999): loss=0.3505025048986012\n",
      "Stochastic Gradient Descent(541/999): loss=0.3509506850256979\n",
      "Stochastic Gradient Descent(542/999): loss=0.35112608310204974\n",
      "Stochastic Gradient Descent(543/999): loss=0.3524686301445833\n",
      "Stochastic Gradient Descent(544/999): loss=0.35436401237016973\n",
      "Stochastic Gradient Descent(545/999): loss=0.3540650287543407\n",
      "Stochastic Gradient Descent(546/999): loss=0.3548577620171159\n",
      "Stochastic Gradient Descent(547/999): loss=0.3525567327846882\n",
      "Stochastic Gradient Descent(548/999): loss=0.3518234916734693\n",
      "Stochastic Gradient Descent(549/999): loss=0.35062582902343076\n",
      "Stochastic Gradient Descent(550/999): loss=0.351372051107773\n",
      "Stochastic Gradient Descent(551/999): loss=0.3517361796470023\n",
      "Stochastic Gradient Descent(552/999): loss=0.34933791117268986\n",
      "Stochastic Gradient Descent(553/999): loss=0.3497202531790858\n",
      "Stochastic Gradient Descent(554/999): loss=0.3494416405188888\n",
      "Stochastic Gradient Descent(555/999): loss=0.34935257228197525\n",
      "Stochastic Gradient Descent(556/999): loss=0.3495250627527069\n",
      "Stochastic Gradient Descent(557/999): loss=0.34941360100096197\n",
      "Stochastic Gradient Descent(558/999): loss=0.34975093193515766\n",
      "Stochastic Gradient Descent(559/999): loss=0.34964974645251573\n",
      "Stochastic Gradient Descent(560/999): loss=0.3494132646898403\n",
      "Stochastic Gradient Descent(561/999): loss=0.3493047649493687\n",
      "Stochastic Gradient Descent(562/999): loss=0.35059184539812616\n",
      "Stochastic Gradient Descent(563/999): loss=0.3515534300587886\n",
      "Stochastic Gradient Descent(564/999): loss=0.3510395498938571\n",
      "Stochastic Gradient Descent(565/999): loss=0.3517474754473515\n",
      "Stochastic Gradient Descent(566/999): loss=0.3507575023783106\n",
      "Stochastic Gradient Descent(567/999): loss=0.3508548571914199\n",
      "Stochastic Gradient Descent(568/999): loss=0.3502100205758527\n",
      "Stochastic Gradient Descent(569/999): loss=0.3493470471099056\n",
      "Stochastic Gradient Descent(570/999): loss=0.34888233274990876\n",
      "Stochastic Gradient Descent(571/999): loss=0.34890655395556835\n",
      "Stochastic Gradient Descent(572/999): loss=0.3486845290585408\n",
      "Stochastic Gradient Descent(573/999): loss=0.3487199607171686\n",
      "Stochastic Gradient Descent(574/999): loss=0.34870935101848316\n",
      "Stochastic Gradient Descent(575/999): loss=0.3486467767994116\n",
      "Stochastic Gradient Descent(576/999): loss=0.34864948161314246\n",
      "Stochastic Gradient Descent(577/999): loss=0.34868219912430315\n",
      "Stochastic Gradient Descent(578/999): loss=0.34867601109882873\n",
      "Stochastic Gradient Descent(579/999): loss=0.34860196053309694\n",
      "Stochastic Gradient Descent(580/999): loss=0.3486293864832566\n",
      "Stochastic Gradient Descent(581/999): loss=0.3501499128560581\n",
      "Stochastic Gradient Descent(582/999): loss=0.3495971059185249\n",
      "Stochastic Gradient Descent(583/999): loss=0.3506047876728\n",
      "Stochastic Gradient Descent(584/999): loss=0.3506111176336506\n",
      "Stochastic Gradient Descent(585/999): loss=0.3519467593916961\n",
      "Stochastic Gradient Descent(586/999): loss=0.3506843853531591\n",
      "Stochastic Gradient Descent(587/999): loss=0.3494418945493959\n",
      "Stochastic Gradient Descent(588/999): loss=0.34979632180441206\n",
      "Stochastic Gradient Descent(589/999): loss=0.34899771317518546\n",
      "Stochastic Gradient Descent(590/999): loss=0.34914265204905487\n",
      "Stochastic Gradient Descent(591/999): loss=0.3487450795460737\n",
      "Stochastic Gradient Descent(592/999): loss=0.34900119290797654\n",
      "Stochastic Gradient Descent(593/999): loss=0.34897295276072066\n",
      "Stochastic Gradient Descent(594/999): loss=0.3491330739495609\n",
      "Stochastic Gradient Descent(595/999): loss=0.3501584601825804\n",
      "Stochastic Gradient Descent(596/999): loss=0.3530114373605246\n",
      "Stochastic Gradient Descent(597/999): loss=0.35154410450154133\n",
      "Stochastic Gradient Descent(598/999): loss=0.35075851784786927\n",
      "Stochastic Gradient Descent(599/999): loss=0.35022453591282626\n",
      "Stochastic Gradient Descent(600/999): loss=0.35080002512492137\n",
      "Stochastic Gradient Descent(601/999): loss=0.3500540195453634\n",
      "Stochastic Gradient Descent(602/999): loss=0.3502362768753396\n",
      "Stochastic Gradient Descent(603/999): loss=0.3492740972843671\n",
      "Stochastic Gradient Descent(604/999): loss=0.34866690346966656\n",
      "Stochastic Gradient Descent(605/999): loss=0.34827537733580016\n",
      "Stochastic Gradient Descent(606/999): loss=0.34839371603135916\n",
      "Stochastic Gradient Descent(607/999): loss=0.34829977783323673\n",
      "Stochastic Gradient Descent(608/999): loss=0.34837414429759395\n",
      "Stochastic Gradient Descent(609/999): loss=0.3482933229911699\n",
      "Stochastic Gradient Descent(610/999): loss=0.34809494531551566\n",
      "Stochastic Gradient Descent(611/999): loss=0.3480763587779303\n",
      "Stochastic Gradient Descent(612/999): loss=0.34795917292413414\n",
      "Stochastic Gradient Descent(613/999): loss=0.3480577225990368\n",
      "Stochastic Gradient Descent(614/999): loss=0.34796581433256135\n",
      "Stochastic Gradient Descent(615/999): loss=0.34792160109277814\n",
      "Stochastic Gradient Descent(616/999): loss=0.3477719200154098\n",
      "Stochastic Gradient Descent(617/999): loss=0.3477789344409969\n",
      "Stochastic Gradient Descent(618/999): loss=0.34791461172957144\n",
      "Stochastic Gradient Descent(619/999): loss=0.3477804254327636\n",
      "Stochastic Gradient Descent(620/999): loss=0.347694759536964\n",
      "Stochastic Gradient Descent(621/999): loss=0.3476313981362389\n",
      "Stochastic Gradient Descent(622/999): loss=0.34764322047417223\n",
      "Stochastic Gradient Descent(623/999): loss=0.3476894890057884\n",
      "Stochastic Gradient Descent(624/999): loss=0.3476823091008557\n",
      "Stochastic Gradient Descent(625/999): loss=0.3476712230722773\n",
      "Stochastic Gradient Descent(626/999): loss=0.34770037437320966\n",
      "Stochastic Gradient Descent(627/999): loss=0.34762348972982804\n",
      "Stochastic Gradient Descent(628/999): loss=0.34759754777976304\n",
      "Stochastic Gradient Descent(629/999): loss=0.3476246150689147\n",
      "Stochastic Gradient Descent(630/999): loss=0.347603585872595\n",
      "Stochastic Gradient Descent(631/999): loss=0.34760745335670584\n",
      "Stochastic Gradient Descent(632/999): loss=0.3476134293692428\n",
      "Stochastic Gradient Descent(633/999): loss=0.3476899420774045\n",
      "Stochastic Gradient Descent(634/999): loss=0.3477736213375291\n",
      "Stochastic Gradient Descent(635/999): loss=0.3477092772196412\n",
      "Stochastic Gradient Descent(636/999): loss=0.3476022512795624\n",
      "Stochastic Gradient Descent(637/999): loss=0.3475267129703775\n",
      "Stochastic Gradient Descent(638/999): loss=0.34773973782916534\n",
      "Stochastic Gradient Descent(639/999): loss=0.347728740322244\n",
      "Stochastic Gradient Descent(640/999): loss=0.3479045533206933\n",
      "Stochastic Gradient Descent(641/999): loss=0.3475637394583753\n",
      "Stochastic Gradient Descent(642/999): loss=0.34791438986643064\n",
      "Stochastic Gradient Descent(643/999): loss=0.34765051194876107\n",
      "Stochastic Gradient Descent(644/999): loss=0.3478366685123085\n",
      "Stochastic Gradient Descent(645/999): loss=0.3478574094325007\n",
      "Stochastic Gradient Descent(646/999): loss=0.3485819016290323\n",
      "Stochastic Gradient Descent(647/999): loss=0.34820359882005175\n",
      "Stochastic Gradient Descent(648/999): loss=0.34775583718768266\n",
      "Stochastic Gradient Descent(649/999): loss=0.3478700548412753\n",
      "Stochastic Gradient Descent(650/999): loss=0.34781558089327325\n",
      "Stochastic Gradient Descent(651/999): loss=0.34845417761512937\n",
      "Stochastic Gradient Descent(652/999): loss=0.3484264456485494\n",
      "Stochastic Gradient Descent(653/999): loss=0.3492017157032506\n",
      "Stochastic Gradient Descent(654/999): loss=0.3487743695968152\n",
      "Stochastic Gradient Descent(655/999): loss=0.3481111026710987\n",
      "Stochastic Gradient Descent(656/999): loss=0.348175218082483\n",
      "Stochastic Gradient Descent(657/999): loss=0.3481336304763509\n",
      "Stochastic Gradient Descent(658/999): loss=0.3479724371790366\n",
      "Stochastic Gradient Descent(659/999): loss=0.3478971626235473\n",
      "Stochastic Gradient Descent(660/999): loss=0.34786080417694415\n",
      "Stochastic Gradient Descent(661/999): loss=0.3476940627970923\n",
      "Stochastic Gradient Descent(662/999): loss=0.34751542042134775\n",
      "Stochastic Gradient Descent(663/999): loss=0.34746924934200546\n",
      "Stochastic Gradient Descent(664/999): loss=0.3474348927716538\n",
      "Stochastic Gradient Descent(665/999): loss=0.34755835831381976\n",
      "Stochastic Gradient Descent(666/999): loss=0.3473055955622766\n",
      "Stochastic Gradient Descent(667/999): loss=0.3473557298004766\n",
      "Stochastic Gradient Descent(668/999): loss=0.34784159610309057\n",
      "Stochastic Gradient Descent(669/999): loss=0.3488411720246025\n",
      "Stochastic Gradient Descent(670/999): loss=0.3481194396982181\n",
      "Stochastic Gradient Descent(671/999): loss=0.34890167635361563\n",
      "Stochastic Gradient Descent(672/999): loss=0.34795738714527413\n",
      "Stochastic Gradient Descent(673/999): loss=0.34852757680322577\n",
      "Stochastic Gradient Descent(674/999): loss=0.34809414997231436\n",
      "Stochastic Gradient Descent(675/999): loss=0.34746580082433215\n",
      "Stochastic Gradient Descent(676/999): loss=0.347427917555028\n",
      "Stochastic Gradient Descent(677/999): loss=0.3474593599692895\n",
      "Stochastic Gradient Descent(678/999): loss=0.3476724775236816\n",
      "Stochastic Gradient Descent(679/999): loss=0.3473859754600685\n",
      "Stochastic Gradient Descent(680/999): loss=0.34739477937611707\n",
      "Stochastic Gradient Descent(681/999): loss=0.34726338175570676\n",
      "Stochastic Gradient Descent(682/999): loss=0.3477180584140234\n",
      "Stochastic Gradient Descent(683/999): loss=0.3470197661276452\n",
      "Stochastic Gradient Descent(684/999): loss=0.34724157477856926\n",
      "Stochastic Gradient Descent(685/999): loss=0.3476323246550623\n",
      "Stochastic Gradient Descent(686/999): loss=0.3492263623651634\n",
      "Stochastic Gradient Descent(687/999): loss=0.3521429947179759\n",
      "Stochastic Gradient Descent(688/999): loss=0.35227738816678394\n",
      "Stochastic Gradient Descent(689/999): loss=0.3515473062979807\n",
      "Stochastic Gradient Descent(690/999): loss=0.3501355657381479\n",
      "Stochastic Gradient Descent(691/999): loss=0.3487400546384569\n",
      "Stochastic Gradient Descent(692/999): loss=0.3504775782697332\n",
      "Stochastic Gradient Descent(693/999): loss=0.3493873834719241\n",
      "Stochastic Gradient Descent(694/999): loss=0.34842493687079756\n",
      "Stochastic Gradient Descent(695/999): loss=0.34938610690638733\n",
      "Stochastic Gradient Descent(696/999): loss=0.3475419007077603\n",
      "Stochastic Gradient Descent(697/999): loss=0.3482221444758211\n",
      "Stochastic Gradient Descent(698/999): loss=0.3477010608652253\n",
      "Stochastic Gradient Descent(699/999): loss=0.34715534679794835\n",
      "Stochastic Gradient Descent(700/999): loss=0.34655751425070297\n",
      "Stochastic Gradient Descent(701/999): loss=0.3466047414252115\n",
      "Stochastic Gradient Descent(702/999): loss=0.3466768097159137\n",
      "Stochastic Gradient Descent(703/999): loss=0.34702774791877633\n",
      "Stochastic Gradient Descent(704/999): loss=0.3470824960794538\n",
      "Stochastic Gradient Descent(705/999): loss=0.3472605333804939\n",
      "Stochastic Gradient Descent(706/999): loss=0.34736313481030734\n",
      "Stochastic Gradient Descent(707/999): loss=0.3466586690249485\n",
      "Stochastic Gradient Descent(708/999): loss=0.3465991709482927\n",
      "Stochastic Gradient Descent(709/999): loss=0.34655129738480017\n",
      "Stochastic Gradient Descent(710/999): loss=0.3465074077608587\n",
      "Stochastic Gradient Descent(711/999): loss=0.34653328615719375\n",
      "Stochastic Gradient Descent(712/999): loss=0.3464901460411556\n",
      "Stochastic Gradient Descent(713/999): loss=0.3466025514456823\n",
      "Stochastic Gradient Descent(714/999): loss=0.3469506889063801\n",
      "Stochastic Gradient Descent(715/999): loss=0.34670917182580724\n",
      "Stochastic Gradient Descent(716/999): loss=0.34661260739126204\n",
      "Stochastic Gradient Descent(717/999): loss=0.346527978925853\n",
      "Stochastic Gradient Descent(718/999): loss=0.34636658062469516\n",
      "Stochastic Gradient Descent(719/999): loss=0.34648548009340585\n",
      "Stochastic Gradient Descent(720/999): loss=0.3466406428125172\n",
      "Stochastic Gradient Descent(721/999): loss=0.346767298019369\n",
      "Stochastic Gradient Descent(722/999): loss=0.34676840190462077\n",
      "Stochastic Gradient Descent(723/999): loss=0.34648305271016383\n",
      "Stochastic Gradient Descent(724/999): loss=0.34653076928647314\n",
      "Stochastic Gradient Descent(725/999): loss=0.34658258874621056\n",
      "Stochastic Gradient Descent(726/999): loss=0.3464855914623943\n",
      "Stochastic Gradient Descent(727/999): loss=0.3469408578466318\n",
      "Stochastic Gradient Descent(728/999): loss=0.3465509825543511\n",
      "Stochastic Gradient Descent(729/999): loss=0.3468098955336775\n",
      "Stochastic Gradient Descent(730/999): loss=0.3466711281438827\n",
      "Stochastic Gradient Descent(731/999): loss=0.34713576050345624\n",
      "Stochastic Gradient Descent(732/999): loss=0.346415884136594\n",
      "Stochastic Gradient Descent(733/999): loss=0.3468360076449505\n",
      "Stochastic Gradient Descent(734/999): loss=0.34629544839933407\n",
      "Stochastic Gradient Descent(735/999): loss=0.3464250437595735\n",
      "Stochastic Gradient Descent(736/999): loss=0.34630613366086666\n",
      "Stochastic Gradient Descent(737/999): loss=0.34630794244838553\n",
      "Stochastic Gradient Descent(738/999): loss=0.34682577771636885\n",
      "Stochastic Gradient Descent(739/999): loss=0.3475978812698538\n",
      "Stochastic Gradient Descent(740/999): loss=0.34717986954609886\n",
      "Stochastic Gradient Descent(741/999): loss=0.34731295336909945\n",
      "Stochastic Gradient Descent(742/999): loss=0.3470297449846563\n",
      "Stochastic Gradient Descent(743/999): loss=0.3466626905112588\n",
      "Stochastic Gradient Descent(744/999): loss=0.34628376403687383\n",
      "Stochastic Gradient Descent(745/999): loss=0.3461894936457462\n",
      "Stochastic Gradient Descent(746/999): loss=0.3462466982687513\n",
      "Stochastic Gradient Descent(747/999): loss=0.3464674061169934\n",
      "Stochastic Gradient Descent(748/999): loss=0.34689976675342266\n",
      "Stochastic Gradient Descent(749/999): loss=0.34675090160567645\n",
      "Stochastic Gradient Descent(750/999): loss=0.3468107072751025\n",
      "Stochastic Gradient Descent(751/999): loss=0.34649206008043276\n",
      "Stochastic Gradient Descent(752/999): loss=0.3463252777993195\n",
      "Stochastic Gradient Descent(753/999): loss=0.3461326347871249\n",
      "Stochastic Gradient Descent(754/999): loss=0.34629786360248965\n",
      "Stochastic Gradient Descent(755/999): loss=0.34677524734313164\n",
      "Stochastic Gradient Descent(756/999): loss=0.34666407818953\n",
      "Stochastic Gradient Descent(757/999): loss=0.34615032876192736\n",
      "Stochastic Gradient Descent(758/999): loss=0.34647931490993883\n",
      "Stochastic Gradient Descent(759/999): loss=0.34650704814992567\n",
      "Stochastic Gradient Descent(760/999): loss=0.3464165411443022\n",
      "Stochastic Gradient Descent(761/999): loss=0.34579451384714105\n",
      "Stochastic Gradient Descent(762/999): loss=0.34574733267559127\n",
      "Stochastic Gradient Descent(763/999): loss=0.3457922438643582\n",
      "Stochastic Gradient Descent(764/999): loss=0.34576448694320827\n",
      "Stochastic Gradient Descent(765/999): loss=0.3459128312452723\n",
      "Stochastic Gradient Descent(766/999): loss=0.3459798454202496\n",
      "Stochastic Gradient Descent(767/999): loss=0.3458805560811528\n",
      "Stochastic Gradient Descent(768/999): loss=0.34579751089816263\n",
      "Stochastic Gradient Descent(769/999): loss=0.34588619424172645\n",
      "Stochastic Gradient Descent(770/999): loss=0.3457226403645683\n",
      "Stochastic Gradient Descent(771/999): loss=0.34569739656839116\n",
      "Stochastic Gradient Descent(772/999): loss=0.345772905165697\n",
      "Stochastic Gradient Descent(773/999): loss=0.3456639774224674\n",
      "Stochastic Gradient Descent(774/999): loss=0.3457587883713225\n",
      "Stochastic Gradient Descent(775/999): loss=0.3457038438211684\n",
      "Stochastic Gradient Descent(776/999): loss=0.3456733577132951\n",
      "Stochastic Gradient Descent(777/999): loss=0.34587504950933956\n",
      "Stochastic Gradient Descent(778/999): loss=0.3459206777067557\n",
      "Stochastic Gradient Descent(779/999): loss=0.3462886196698126\n",
      "Stochastic Gradient Descent(780/999): loss=0.34591741783941066\n",
      "Stochastic Gradient Descent(781/999): loss=0.34651324683132634\n",
      "Stochastic Gradient Descent(782/999): loss=0.34602528713117275\n",
      "Stochastic Gradient Descent(783/999): loss=0.3457785358625426\n",
      "Stochastic Gradient Descent(784/999): loss=0.3456048304417085\n",
      "Stochastic Gradient Descent(785/999): loss=0.3456501218305102\n",
      "Stochastic Gradient Descent(786/999): loss=0.34577984336843326\n",
      "Stochastic Gradient Descent(787/999): loss=0.3456806365153895\n",
      "Stochastic Gradient Descent(788/999): loss=0.34558741707392754\n",
      "Stochastic Gradient Descent(789/999): loss=0.34561550339007086\n",
      "Stochastic Gradient Descent(790/999): loss=0.3456744930765123\n",
      "Stochastic Gradient Descent(791/999): loss=0.34567030095682844\n",
      "Stochastic Gradient Descent(792/999): loss=0.3456404335887839\n",
      "Stochastic Gradient Descent(793/999): loss=0.34567349176714474\n",
      "Stochastic Gradient Descent(794/999): loss=0.3457352210593582\n",
      "Stochastic Gradient Descent(795/999): loss=0.3455429499393536\n",
      "Stochastic Gradient Descent(796/999): loss=0.3455099484630648\n",
      "Stochastic Gradient Descent(797/999): loss=0.3455415197898972\n",
      "Stochastic Gradient Descent(798/999): loss=0.3457269090571937\n",
      "Stochastic Gradient Descent(799/999): loss=0.34646000642019775\n",
      "Stochastic Gradient Descent(800/999): loss=0.3461223250902921\n",
      "Stochastic Gradient Descent(801/999): loss=0.3454602129630381\n",
      "Stochastic Gradient Descent(802/999): loss=0.3455978880335056\n",
      "Stochastic Gradient Descent(803/999): loss=0.346067883733019\n",
      "Stochastic Gradient Descent(804/999): loss=0.3459211163108156\n",
      "Stochastic Gradient Descent(805/999): loss=0.3456456047327684\n",
      "Stochastic Gradient Descent(806/999): loss=0.34566797009163036\n",
      "Stochastic Gradient Descent(807/999): loss=0.3456616455466695\n",
      "Stochastic Gradient Descent(808/999): loss=0.34567872570227265\n",
      "Stochastic Gradient Descent(809/999): loss=0.34647762596946363\n",
      "Stochastic Gradient Descent(810/999): loss=0.3457673104690032\n",
      "Stochastic Gradient Descent(811/999): loss=0.34577026981992864\n",
      "Stochastic Gradient Descent(812/999): loss=0.34553739204645023\n",
      "Stochastic Gradient Descent(813/999): loss=0.34571828852066777\n",
      "Stochastic Gradient Descent(814/999): loss=0.3455240980940413\n",
      "Stochastic Gradient Descent(815/999): loss=0.3455561480644981\n",
      "Stochastic Gradient Descent(816/999): loss=0.34555558821333066\n",
      "Stochastic Gradient Descent(817/999): loss=0.34565647674881894\n",
      "Stochastic Gradient Descent(818/999): loss=0.3456451206060296\n",
      "Stochastic Gradient Descent(819/999): loss=0.34562831532517035\n",
      "Stochastic Gradient Descent(820/999): loss=0.3457292623287761\n",
      "Stochastic Gradient Descent(821/999): loss=0.3455963309682855\n",
      "Stochastic Gradient Descent(822/999): loss=0.3454551114833108\n",
      "Stochastic Gradient Descent(823/999): loss=0.3454757393059907\n",
      "Stochastic Gradient Descent(824/999): loss=0.34572801669773673\n",
      "Stochastic Gradient Descent(825/999): loss=0.3453506677905245\n",
      "Stochastic Gradient Descent(826/999): loss=0.3454235257994122\n",
      "Stochastic Gradient Descent(827/999): loss=0.34550882872678634\n",
      "Stochastic Gradient Descent(828/999): loss=0.345277218960436\n",
      "Stochastic Gradient Descent(829/999): loss=0.3454032425284775\n",
      "Stochastic Gradient Descent(830/999): loss=0.3454438625603654\n",
      "Stochastic Gradient Descent(831/999): loss=0.34546999314619364\n",
      "Stochastic Gradient Descent(832/999): loss=0.34524607463062623\n",
      "Stochastic Gradient Descent(833/999): loss=0.3453193023542476\n",
      "Stochastic Gradient Descent(834/999): loss=0.3453458878775873\n",
      "Stochastic Gradient Descent(835/999): loss=0.3453621497140652\n",
      "Stochastic Gradient Descent(836/999): loss=0.3452538194430802\n",
      "Stochastic Gradient Descent(837/999): loss=0.34529651882744955\n",
      "Stochastic Gradient Descent(838/999): loss=0.3452882584533012\n",
      "Stochastic Gradient Descent(839/999): loss=0.3452995438975962\n",
      "Stochastic Gradient Descent(840/999): loss=0.3453989247318426\n",
      "Stochastic Gradient Descent(841/999): loss=0.34564661351577863\n",
      "Stochastic Gradient Descent(842/999): loss=0.34552735289431685\n",
      "Stochastic Gradient Descent(843/999): loss=0.34544166388351344\n",
      "Stochastic Gradient Descent(844/999): loss=0.34566319154265895\n",
      "Stochastic Gradient Descent(845/999): loss=0.3455296607569999\n",
      "Stochastic Gradient Descent(846/999): loss=0.34560728738464513\n",
      "Stochastic Gradient Descent(847/999): loss=0.3451493960861635\n",
      "Stochastic Gradient Descent(848/999): loss=0.3451431083080588\n",
      "Stochastic Gradient Descent(849/999): loss=0.34536216283506066\n",
      "Stochastic Gradient Descent(850/999): loss=0.3451408620546028\n",
      "Stochastic Gradient Descent(851/999): loss=0.34519588641980514\n",
      "Stochastic Gradient Descent(852/999): loss=0.34505821149470517\n",
      "Stochastic Gradient Descent(853/999): loss=0.34511480757837815\n",
      "Stochastic Gradient Descent(854/999): loss=0.34570729677972944\n",
      "Stochastic Gradient Descent(855/999): loss=0.345457501789668\n",
      "Stochastic Gradient Descent(856/999): loss=0.34624172513695445\n",
      "Stochastic Gradient Descent(857/999): loss=0.34604107190783034\n",
      "Stochastic Gradient Descent(858/999): loss=0.34572832199791037\n",
      "Stochastic Gradient Descent(859/999): loss=0.34521442897449395\n",
      "Stochastic Gradient Descent(860/999): loss=0.3455793771978061\n",
      "Stochastic Gradient Descent(861/999): loss=0.3452590028091848\n",
      "Stochastic Gradient Descent(862/999): loss=0.345859181780369\n",
      "Stochastic Gradient Descent(863/999): loss=0.34579658938626556\n",
      "Stochastic Gradient Descent(864/999): loss=0.34527475438834915\n",
      "Stochastic Gradient Descent(865/999): loss=0.3459492262506733\n",
      "Stochastic Gradient Descent(866/999): loss=0.3475788384584514\n",
      "Stochastic Gradient Descent(867/999): loss=0.34624210672437405\n",
      "Stochastic Gradient Descent(868/999): loss=0.3471434933836017\n",
      "Stochastic Gradient Descent(869/999): loss=0.34858124364633747\n",
      "Stochastic Gradient Descent(870/999): loss=0.34822362991189354\n",
      "Stochastic Gradient Descent(871/999): loss=0.3466471178913605\n",
      "Stochastic Gradient Descent(872/999): loss=0.34773667194453933\n",
      "Stochastic Gradient Descent(873/999): loss=0.34680766611922226\n",
      "Stochastic Gradient Descent(874/999): loss=0.346092931246893\n",
      "Stochastic Gradient Descent(875/999): loss=0.34520765415535914\n",
      "Stochastic Gradient Descent(876/999): loss=0.34555044690730585\n",
      "Stochastic Gradient Descent(877/999): loss=0.3451341364635567\n",
      "Stochastic Gradient Descent(878/999): loss=0.34572483008295285\n",
      "Stochastic Gradient Descent(879/999): loss=0.3451304839063837\n",
      "Stochastic Gradient Descent(880/999): loss=0.34503478287414496\n",
      "Stochastic Gradient Descent(881/999): loss=0.34506224991918666\n",
      "Stochastic Gradient Descent(882/999): loss=0.34518965608643476\n",
      "Stochastic Gradient Descent(883/999): loss=0.3450374996447274\n",
      "Stochastic Gradient Descent(884/999): loss=0.34511443326603053\n",
      "Stochastic Gradient Descent(885/999): loss=0.34491318666367915\n",
      "Stochastic Gradient Descent(886/999): loss=0.34494624183691325\n",
      "Stochastic Gradient Descent(887/999): loss=0.34495251776217456\n",
      "Stochastic Gradient Descent(888/999): loss=0.34494179520551477\n",
      "Stochastic Gradient Descent(889/999): loss=0.34571365480532695\n",
      "Stochastic Gradient Descent(890/999): loss=0.34504381834038755\n",
      "Stochastic Gradient Descent(891/999): loss=0.34519175822871534\n",
      "Stochastic Gradient Descent(892/999): loss=0.3448054979606743\n",
      "Stochastic Gradient Descent(893/999): loss=0.3448130072354102\n",
      "Stochastic Gradient Descent(894/999): loss=0.3449841793655095\n",
      "Stochastic Gradient Descent(895/999): loss=0.34586619964702653\n",
      "Stochastic Gradient Descent(896/999): loss=0.3450989052249185\n",
      "Stochastic Gradient Descent(897/999): loss=0.3453512313733658\n",
      "Stochastic Gradient Descent(898/999): loss=0.34511663091881645\n",
      "Stochastic Gradient Descent(899/999): loss=0.3455354789922197\n",
      "Stochastic Gradient Descent(900/999): loss=0.3456036414497777\n",
      "Stochastic Gradient Descent(901/999): loss=0.3460526657733416\n",
      "Stochastic Gradient Descent(902/999): loss=0.34599917018946785\n",
      "Stochastic Gradient Descent(903/999): loss=0.34483544648432657\n",
      "Stochastic Gradient Descent(904/999): loss=0.3446946481257049\n",
      "Stochastic Gradient Descent(905/999): loss=0.34486751110224817\n",
      "Stochastic Gradient Descent(906/999): loss=0.34568643304803637\n",
      "Stochastic Gradient Descent(907/999): loss=0.34497022157435825\n",
      "Stochastic Gradient Descent(908/999): loss=0.34581100433053474\n",
      "Stochastic Gradient Descent(909/999): loss=0.3450506317871053\n",
      "Stochastic Gradient Descent(910/999): loss=0.3448441379735092\n",
      "Stochastic Gradient Descent(911/999): loss=0.3449283876848478\n",
      "Stochastic Gradient Descent(912/999): loss=0.34499989184776625\n",
      "Stochastic Gradient Descent(913/999): loss=0.3453121888461593\n",
      "Stochastic Gradient Descent(914/999): loss=0.34706386968403674\n",
      "Stochastic Gradient Descent(915/999): loss=0.34620255414878\n",
      "Stochastic Gradient Descent(916/999): loss=0.34557542984822337\n",
      "Stochastic Gradient Descent(917/999): loss=0.3455738184899813\n",
      "Stochastic Gradient Descent(918/999): loss=0.34679121910753136\n",
      "Stochastic Gradient Descent(919/999): loss=0.3477026913224026\n",
      "Stochastic Gradient Descent(920/999): loss=0.34879798249930105\n",
      "Stochastic Gradient Descent(921/999): loss=0.34845005490680103\n",
      "Stochastic Gradient Descent(922/999): loss=0.3467912979981406\n",
      "Stochastic Gradient Descent(923/999): loss=0.3445553443116158\n",
      "Stochastic Gradient Descent(924/999): loss=0.3444994716906832\n",
      "Stochastic Gradient Descent(925/999): loss=0.3445465249427624\n",
      "Stochastic Gradient Descent(926/999): loss=0.3445876221704076\n",
      "Stochastic Gradient Descent(927/999): loss=0.34470015823390304\n",
      "Stochastic Gradient Descent(928/999): loss=0.34454389988166045\n",
      "Stochastic Gradient Descent(929/999): loss=0.34464345508872934\n",
      "Stochastic Gradient Descent(930/999): loss=0.34460414396293554\n",
      "Stochastic Gradient Descent(931/999): loss=0.3444353225851075\n",
      "Stochastic Gradient Descent(932/999): loss=0.34455879012886076\n",
      "Stochastic Gradient Descent(933/999): loss=0.34576718908090404\n",
      "Stochastic Gradient Descent(934/999): loss=0.3454637957037783\n",
      "Stochastic Gradient Descent(935/999): loss=0.3459667788706064\n",
      "Stochastic Gradient Descent(936/999): loss=0.3454223423096965\n",
      "Stochastic Gradient Descent(937/999): loss=0.34516752535444234\n",
      "Stochastic Gradient Descent(938/999): loss=0.3446394544705981\n",
      "Stochastic Gradient Descent(939/999): loss=0.34457833270401866\n",
      "Stochastic Gradient Descent(940/999): loss=0.3447156665053911\n",
      "Stochastic Gradient Descent(941/999): loss=0.3446237369171499\n",
      "Stochastic Gradient Descent(942/999): loss=0.34450702533233996\n",
      "Stochastic Gradient Descent(943/999): loss=0.34451243730805037\n",
      "Stochastic Gradient Descent(944/999): loss=0.3446323818562694\n",
      "Stochastic Gradient Descent(945/999): loss=0.3444213665703684\n",
      "Stochastic Gradient Descent(946/999): loss=0.34444502261670956\n",
      "Stochastic Gradient Descent(947/999): loss=0.3444542704910234\n",
      "Stochastic Gradient Descent(948/999): loss=0.34450226803672634\n",
      "Stochastic Gradient Descent(949/999): loss=0.34463122729410495\n",
      "Stochastic Gradient Descent(950/999): loss=0.3450053039738872\n",
      "Stochastic Gradient Descent(951/999): loss=0.344819445933923\n",
      "Stochastic Gradient Descent(952/999): loss=0.3444982589460427\n",
      "Stochastic Gradient Descent(953/999): loss=0.34592318597051736\n",
      "Stochastic Gradient Descent(954/999): loss=0.34664249677432163\n",
      "Stochastic Gradient Descent(955/999): loss=0.3455765764823441\n",
      "Stochastic Gradient Descent(956/999): loss=0.3453950284946325\n",
      "Stochastic Gradient Descent(957/999): loss=0.3444094816339937\n",
      "Stochastic Gradient Descent(958/999): loss=0.3444633280833018\n",
      "Stochastic Gradient Descent(959/999): loss=0.34430619708189864\n",
      "Stochastic Gradient Descent(960/999): loss=0.34433241910045514\n",
      "Stochastic Gradient Descent(961/999): loss=0.3442814123354211\n",
      "Stochastic Gradient Descent(962/999): loss=0.34440802944180915\n",
      "Stochastic Gradient Descent(963/999): loss=0.34435593718712515\n",
      "Stochastic Gradient Descent(964/999): loss=0.3443158116206762\n",
      "Stochastic Gradient Descent(965/999): loss=0.34461788486662226\n",
      "Stochastic Gradient Descent(966/999): loss=0.3444581528026471\n",
      "Stochastic Gradient Descent(967/999): loss=0.3442722299140903\n",
      "Stochastic Gradient Descent(968/999): loss=0.3442238020571955\n",
      "Stochastic Gradient Descent(969/999): loss=0.34419873860424227\n",
      "Stochastic Gradient Descent(970/999): loss=0.34426327004150825\n",
      "Stochastic Gradient Descent(971/999): loss=0.3442668728224239\n",
      "Stochastic Gradient Descent(972/999): loss=0.34422776708178027\n",
      "Stochastic Gradient Descent(973/999): loss=0.34415993153162067\n",
      "Stochastic Gradient Descent(974/999): loss=0.3441083647882587\n",
      "Stochastic Gradient Descent(975/999): loss=0.34434300602413026\n",
      "Stochastic Gradient Descent(976/999): loss=0.3444372604511386\n",
      "Stochastic Gradient Descent(977/999): loss=0.3447974115020293\n",
      "Stochastic Gradient Descent(978/999): loss=0.34469511352325216\n",
      "Stochastic Gradient Descent(979/999): loss=0.34507056512430234\n",
      "Stochastic Gradient Descent(980/999): loss=0.34438912872530203\n",
      "Stochastic Gradient Descent(981/999): loss=0.34496183699127875\n",
      "Stochastic Gradient Descent(982/999): loss=0.3453203336433101\n",
      "Stochastic Gradient Descent(983/999): loss=0.3454770198728151\n",
      "Stochastic Gradient Descent(984/999): loss=0.3462305430742185\n",
      "Stochastic Gradient Descent(985/999): loss=0.34472286881627023\n",
      "Stochastic Gradient Descent(986/999): loss=0.3443805850760106\n",
      "Stochastic Gradient Descent(987/999): loss=0.344326093327662\n",
      "Stochastic Gradient Descent(988/999): loss=0.344067031220367\n",
      "Stochastic Gradient Descent(989/999): loss=0.3440334699411217\n",
      "Stochastic Gradient Descent(990/999): loss=0.34397937214466917\n",
      "Stochastic Gradient Descent(991/999): loss=0.34390375537024515\n",
      "Stochastic Gradient Descent(992/999): loss=0.34400370123488283\n",
      "Stochastic Gradient Descent(993/999): loss=0.3441964094361832\n",
      "Stochastic Gradient Descent(994/999): loss=0.3440805346262147\n",
      "Stochastic Gradient Descent(995/999): loss=0.34402739942863747\n",
      "Stochastic Gradient Descent(996/999): loss=0.3441516198091836\n",
      "Stochastic Gradient Descent(997/999): loss=0.3441500943776882\n",
      "Stochastic Gradient Descent(998/999): loss=0.3441990902248855\n",
      "Stochastic Gradient Descent(999/999): loss=0.34426147986438166\n",
      "parameters w:  [-0.31823869  0.04076699 -0.23527023 -0.16497859  0.01265199 -0.01011551\n",
      "  0.25213761 -0.0155716   0.20396695 -0.03418952 -0.00308197 -0.11514257\n",
      "  0.12510309 -0.01155119  0.19029854  0.00615348 -0.00901454  0.19523585\n",
      "  0.01395332  0.0148477   0.08081144  0.00358751 -0.07000195 -0.09804791\n",
      "  0.01675493  0.02361329  0.02370817 -0.01319096 -0.01204374 -0.01197813\n",
      " -0.09110245]\n"
     ]
    }
   ],
   "source": [
    "from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "batch_size = 50\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "# start_time = datetime.datetime.now()\n",
    "stoch_gradient_losses, stoch_gradient_ws = stochastic_gradient_descent(y, tX, w_initial, batch_size, max_iters, gamma)\n",
    "# end_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "min_loss = 100\n",
    "min_i = -1\n",
    "for l in stoch_gradient_losses:\n",
    "    if l < min_loss:\n",
    "        min_loss = l\n",
    "        min_i = i\n",
    "    i = i+1\n",
    "    print(l)\n",
    "\n",
    "print()\n",
    "print(min_loss)\n",
    "print(min_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from least_squares import *\n",
    "\n",
    "# start_ls_time = datetime.datetime.now()\n",
    "ls_wopt, ls_loss = least_squares(y,tX)\n",
    "# end_ls_time = datetime.datetime.now()\n",
    "print(ls_loss)\n",
    "print(ls_wopt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ridge_regression import ridge_regression\n",
    "\n",
    "# lambdas = np.logspace(-3, 1, 10)      \n",
    "# _x = build_poly(x, degree)\n",
    "# x_train, x_test, y_train, y_test = split_data(tX, y, ratio, seed)\n",
    "    \n",
    "#     for lamb in lambdas:\n",
    "\n",
    "w_ridge = ridge_regression(y, tX, 0.01)\n",
    "print(w_ridge)\n",
    "\n",
    "# rmse_tr = np.sqrt(2*compute_loss(y, tX, w_ridge))\n",
    "# rmse_te = np.sqrt(2*compute_loss(y, tX, w_ridge))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = \"../Data/test.csv\" # TODO: download test data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
