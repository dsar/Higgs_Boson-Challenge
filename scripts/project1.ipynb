{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCML Project-1 ~ Team #60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from costs import compute_loss\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape:  (200,)\n",
      "original tX shape:  (200, 2)\n",
      "ids shape:  (200,)\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "\n",
    "DATA_TRAIN_PATH = \"../Data/train.csv\" # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "#print the shape of the offset x matrix.\n",
    "print('y shape: ',y.shape)\n",
    "print('original tX shape: ',tX.shape)\n",
    "print('ids shape: ',ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 2\n",
      "(2,)\n",
      "(2,)\n",
      "standardized tX shape:  (200, 3)\n",
      "tX mean shape:  (2,)\n",
      "tX std shape:  (2,)\n"
     ]
    }
   ],
   "source": [
    "#standardization\n",
    "#tX, mean_x, std_x = standardize(tX, mean_x=None, std_x=None)\n",
    "tX, mean_x, std_x = standardize_outliers(tX)\n",
    "print('standardized tX shape: ',tX.shape)\n",
    "print('tX mean shape: ',mean_x.shape)\n",
    "print('tX std shape: ',std_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of output y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nl6RGGPiS1Ij/D1MJ1RjJeDsNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2deadbc1dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of output y\n",
    "plt.hist(y, bins=10, align='mid')\n",
    "plt.title(\"Histogram of output y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of y as a function of all its features (one by one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEENJREFUeJzt3X+sZGV9x/H3B1cq2opCdTUgUBVUTCnadrvWtkxq1YU2\nLjakav+o2kQJqS1pbAtWG65/NAXT2NaiMTSUQlOCxloFLBWIjIREkPIbuwtLRFxW3aZVjD/QUPj2\njzm7jvvcH3t3zrk77L5fyeSeH8+c5zuz88xnnnPm3k1VIUnStEP2dwGSpPljOEiSGoaDJKlhOEiS\nGoaDJKlhOEiSGr2EQ5KLk+xMcvcS+09J8kiS27vb+/roV5I0jHU9HecS4O+By5Zpc2NVvaGn/iRJ\nA+pl5lBVNwHfWqFZ+uhLkjS8tbzm8Kokdyb5TJIT17BfSdIq9XVaaSW3AcdU1feTnAp8CjhhjfqW\nJK3SmoRDVX13avmaJB9JckRVfXPPtkn8Y0+StEpV1eup+z5PK4UlriskWT+1vAHIYsGwS1XN9e28\n887b7zVYp3Vap3Xuug2hl5lDksuBEXBkkq8C5wGHAlVVFwFnJDkLeAx4FHhTH/1KkobRSzhU1e+u\nsP/DwIf76EuSNDx/Q3ofjEaj/V3CXrHOfllnv6xzvmWo81X7KknNW02SNM+SUHN8QVqSdIAwHCRJ\nDcNBktQwHCRJDcNBktQwHCRJDcNBktQwHCRJDcNBktQwHCRJDcNBktQwHCRJDcNBktQwHCRJDcNB\nktQwHCRJDcNBktQwHCRJDcNBktQwHCRJDcNBktQwHCRJDcNBktQwHCRJDcNBktQwHCRJjV7CIcnF\nSXYmuXuZNh9Ksi3JnUlO7qNfSdIw+po5XAK8fqmdSU4FXlRVxwNnAh/tqV9J0gB6CYequgn41jJN\nNgOXdW1vAQ5Psr6PvqW1kGT3TToYrNU1h6OA7VPrO7pt0txL1gGHAccDh5F4qU4HPl/l0jImM4VD\ngZuB+7ufT3MGoQPeujXqZwfwgqn1o7tti1pYWNi9PBqNGI1GQ9Ul7YWjgZO65ZOYTHof2H/l6KA3\nHo8Zj8eD9pGq6udAyXHAVVX1s4vsOw34g6r6zSQbgb+tqo1LHKf6qkma1WSGcBiTGcNJwN3ARuBR\nfJ1qXiShqnqdzvYSDkkuB0bAkcBO4Dwmc/Gqqou6NhcCm4DvAW+vqtuXOJbhoLkyucbwNCYzhh3A\nD6h6Yv8WJU2Z23Dok+GgeTR9jcHXp+bNEOGwVtccpCc1A0EHG7+tJElqGA6SpIbhIElqGA6SpIbh\nIElqGA6SpIbhIElqGA6SpIbhIElqGA6SpIbhIElqGA6SpIbhIElqGA6SpIbhIElqGA6SpIbhIElq\nGA6SpIbhIElqGA6SpIbhIElqGA6SpIbhIElqGA6SpIbhIElqGA6SpIbhIElqGA6SpEYv4ZBkU5Kt\nSe5Pcs4i+09J8kiS27vb+/roV5I0jHWzHiDJIcCFwGuArwG3Jvl0VW3do+mNVfWGWfuTJA2vj5nD\nBmBbVT1UVY8BVwCbF2mXHvqSJK2BPsLhKGD71PrD3bY9vSrJnUk+k+TEHvqVJA1k5tNKe+k24Jiq\n+n6SU4FPAScs1XhhYWH38mg0YjQaDV2fJD1pjMdjxuPxoH2kqmY7QLIRWKiqTd36uUBV1QXL3OdB\n4Oer6puL7KtZa5Kkg0kSqqrXU/d9nFa6FXhxkmOTHAq8GbhyukGS9VPLG5iEUhMMkqT5MPNppap6\nPMm7gGuZhM3FVbUlyZmT3XURcEaSs4DHgEeBN83aryRpODOfVuqbp5UkaXXm9bSSJOkAYzhIkhqG\ngySpYThIkhqGgySpYThIkhqGgySpYThIkhqGgySpYThIkhqGgySpYThIkhqGgySpYThIkhqGgySp\nYThIkhqGgySpYThIkhqGgySpYThIkhqGgySpYThIkhqGgySpYThIkhqGgySpYThIkhqGgySpYThI\nkhq9hEOSTUm2Jrk/yTlLtPlQkm1J7kxych/9SpKGMXM4JDkEuBB4PfBy4C1JXrpHm1OBF1XV8cCZ\nwEdn7VeSNJw+Zg4bgG1V9VBVPQZcAWzeo81m4DKAqroFODzJ+h76liQNoI9wOArYPrX+cLdtuTY7\nFmkjSZoT6/Z3AYtZWFjYvTwajRiNRvutFkmaN+PxmPF4PGgfqarZDpBsBBaqalO3fi5QVXXBVJuP\nAjdU1ce69a3AKVW1c5Hj1aw1SdLBJAlVlT6P2cdppVuBFyc5NsmhwJuBK/docyXwe7A7TB5ZLBgk\nSfNh5tNKVfV4kncB1zIJm4urakuSMye766Kq+vckpyV5APge8PZZ+5UkDWfm00p987SSJK3OvJ5W\nkiQdYAwHSVLDcJAkNQwHSVLDcJAkNQwHSVLDcJAkNQwHSVLDcJAkNQwHSVLDcJAkNQwHSVLDcJAk\nNQwHSVLDcJAkNQwHSVLDcJAkNQwHSVLDcJAkNQwHSVLDcJAkNQwHSVLDcJAkNQwHSVLDcJAkNQwH\nSVLDcJAkNQwHSVJj3Sx3TvJs4GPAscBXgN+pqm8v0u4rwLeBJ4DHqmrDLP1KkoY168zhXOD6qnoJ\n8DngPUu0ewIYVdUrDAZJmn+zhsNm4NJu+VLg9CXapYe+JElrZNY37OdW1U6AqvoG8Nwl2hVwXZJb\nk7xjxj4lSQNb8ZpDkuuA9dObmLzZv2+R5rXEYV5dVV9P8hwmIbGlqm5aqs+FhYXdy6PRiNFotFKZ\nknTQGI/HjMfjQftI1VLv53tx52QLk2sJO5M8D7ihql62wn3OA75TVR9cYn/NUpMkHWySUFXp85iz\nnla6Enhbt/xW4NN7Nkjy9CQ/2S0/A3gdcO+M/UqSBjTrzOEI4OPAC4CHmHyV9ZEkzwf+oap+K8nP\nAP/G5JTTOuBfqur8ZY7pzEGSVmGImcNM4TAEw0GSVmceTytJkg5AhoMkqWE4SJIahoMkqWE4SJIa\nhoMkqWE4SJIahoMkqWE4SJIahoMkqWE4SJIahoMkqWE4SJIahoMkqWE4SJIahoMkqWE4SJIahoMk\nqWE4SJIahoMkqWE4SJIahoMkqWE4SJIahoMkqWE4SJIahoMkqWE4SJIahoMkqTFTOCQ5I8m9SR5P\n8spl2m1KsjXJ/UnOmaVPSdLwZp053AO8Efj8Ug2SHAJcCLweeDnwliQvnbFfSdKA1s1y56q6DyBJ\nlmm2AdhWVQ91ba8ANgNbZ+lbWkvTL/Gq2o+VSGtjLa45HAVsn1p/uNsmPSkk64DDgOOBw5hMhqUD\n24ozhyTXAeunNwEFvLeqrhqqMGkeTGYMhwE3AycBdwMbSeIMQge0FcOhql47Yx87gGOm1o/uti1p\nYWFh9/JoNGI0Gs1YgjSLo5kEA93Po4AH9l85OuiNx2PG4/GgfaSPTz9JbgD+pKpuW2TfU4D7gNcA\nXwe+CLylqrYscazyE5nmxVIzB3jUmYPmRjeTXe7a76rN+lXW05NsZzJark5yTbf9+UmuBqiqx4F3\nAdcCXwKuWCoYpHkzCYAfMHmJH9/9/IHBoANeLzOHPjlz0Dzy20qaZ0PMHGb6Kqt0sDAQdLDxO3mS\npIbhIElqGA6SpIbhIElqGA6SpIbhIElqGA6SpIbhIElqGA6SpIbhIElqGA6SpIbhIElqGA6SpIbh\nIElqGA6SpIbhIElqGA6SpIbhIElqGA6SpIbhIElqGA6SpIbhIElqGA6SpIbhIElqGA6SpIbhIElq\nGA6SpMZM4ZDkjCT3Jnk8ySuXafeVJHcluSPJF2fpU5I0vFlnDvcAbwQ+v0K7J4BRVb2iqjbM2Od+\nNx6P93cJe8U6+2Wd/bLO+TZTOFTVfVW1DcgKTTNrX/PkyfJisc5+WWe/rHO+rdUbdgHXJbk1yTvW\nqE9J0j5at1KDJNcB66c3MXmzf29VXbWX/by6qr6e5DlMQmJLVd20+nIlSWshVTX7QZIbgHdX1e17\n0fY84DtV9cEl9s9ekCQdZKpqpdP7q7LizGEVFi0sydOBQ6rqu0meAbwOeP9SB+n7AUqSVm/Wr7Ke\nnmQ7sBG4Osk13fbnJ7m6a7YeuCnJHcDNwFVVde0s/UqShtXLaSVJ0oFl0G8rJdmUZGuS+5Ocs8j+\nZyX5ZPcLcjcnOXFq39lJ7uluZ09t/0CSLUnuTPKvSZ45R3X+0SL3fXeSJ5IcMa91JvnD7jm9J8n5\n81Zjkp9L8oVdv0SZ5BdmqbE75sVJdia5e5k2H0qyrXutnbzSY0zy7CTXJrkvyWeTHD6ndfY6hoao\ncWp/n+NnkDr7HD9D1blPY6iqBrkxCZ4HgGOBpwJ3Ai/do80HgL/oll8CXN8tvxy4G/gJ4CnAdcAL\nu32/weQaBsD5wF/NUZ3X7qqz23808B/Ag8ARc1Tn9PM56upe163/9BzW+Fngdd3yqcANPbw+fwU4\nGbh7if2nAp/pln8JuHmlxwhcAPxZt3wOcP6c1tn3GOq9xr7Hz4DPZW/jZ+A6Vz2Ghpw5bAC2VdVD\nVfUYcAWweY82JwKfg8kv1AHHZfJ115cBt1TVD6vqcSa/gf3bXbvrq+qJ7v43M3kBzUudN+6qs/M3\nwJ/OWN8Qde5+PoGzmLyJ/V93v/+ZwxqfAHZ9Cn8WsGOGGun6vgn41jJNNgOXdW1vAQ5Psp7lH+Nm\n4NJu+VLg9Hmss+8xNNBzCf2On6Hq7HP8DFnnqsfQkOFwFLB9av3hbtu0u+jeAJJsAI5h8kK9F/jV\nbpr+dOA04AWL9PH7wDXzWGeSNwDbq+qeGesbtE7gBODXulM8N8x4ymaoGv8Y+OskX2Uy83jPDDXu\nraUey3KPcX1V7QSoqm8Az53TOqf1MYZWsuoaBxg/e2Nfnss+x8+Qda56DPX5VdZ9cT7wd0luZ/J3\nmu4AHq+qrUkuYHJq4bu7tk/fMcl7gceq6vJ5qzPJYcCfA6+dLnne6uzusw54dlVtTPKLwMeBF85Z\njWcBZ1fVp5KcAfwjP/7croV9+ffbH9/22Os613gM/VjXy+7cf+OnKWUv2qz1+FnM3tS56jE0ZDjs\nYPKpcJej2WMqU1XfYfLJBYAkDwJf7vZdAlzSbf9LphIxyduYfLL89Tmt80XAccBdSdId87YkG6rq\nv+eoTph8uvhk1+bW7uLfkVX1v3NU41ur6uyuzSeSXLwPta3WDn58trrrsRzK0o/xG0nWV9XOJM8D\n9vXfeug6+x5DK1ltjUOMnyHqhH7Hz5B1rn4MzXrxZJmLKk/hRxdHDmVyceRle7Q5HHhqt/wO4J+m\n9j2n+3kM8F/AM7v1TcCXgCPnuc497v8gk08Xc1cn8E7g/d3yCcBDc1TjT3XrXwJO6ZZfA9za07/9\nccA9S+w7jR9d9NvIjy76LfkYmVyQPqdb7uWC9EB19jqGhqix7/Ez4HN5Zl/jZ6A6d12QXvUY6uWF\nscwD3ATcB2wDzp16Mt859cDuA7YAnwAOn7rvjUzOQ9/B5M9979q+DXgIuL27fWQe69zj+F+mn29b\nDPF8PhX4ZyaneP5z1wtozmr85a62O4AvAK/o4bm8HPga8EPgq8Dbp+vs2lzYDba7gFcu9xi77UcA\n13f7rgWeNad19jqGhqhxoPEzxHPZ6/gZsM5VjyF/CU6S1Dhg/o8FSVJ/DAdJUsNwkCQ1DAdJUsNw\nkCQ1DAdJUsNwkCQ1DAdJUuP/AUCIEayqTc/qAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2deadce3fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature:  1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFoVJREFUeJzt3HGMnPWd3/H3d9e7eGzHNsbGINvYHE7uHBoSO6qPK6lY\nGkggbYBTo17uerq7VLqmtD6iqq3C3UXFrfpHrn9c1VxSpTSUhiqW06YyIYScTBRvIpASu8EY52ww\nFixxIMFGd8bYOJiab/+YZ+3Z9czurGe8M+b3fkmrmec3v+f3+z7PjD/z7G9nHJmJJOmdb6DXBUiS\nZoeBL0mFMPAlqRAGviQVwsCXpEIY+JJUiK4EfkTcHxGvRMTTLR6/MSKORsST1c/nujGvJKl9c7o0\nzgPAXwAPTtHnB5l5e5fmkyTNUFeu8DPzceBvpukW3ZhLknR+ZnMN/zci4qmI+HZEvHcW55Uk0b0l\nnen8GLgqM9+IiNuAh4D3zNLckiRmKfAz83jD/e9ExH+JiCWZ+deT+0aE/7mPJM1QZk67bN7NJZ2g\nxTp9RCxvuL8RiGZhPy4z++rn3nvv7XkN1vTOqalf67Kmi7emdnXlCj8itgAjwGUR8VPgXmC4nt15\nH/CJiLgLeAs4CfxWN+aVJLWvK4Gfmb8zzeNfAr7UjbkkSefHb9q2YWRkpNclnMOa2tOPNUF/1mVN\n7enHmtoVM1n/mQ0Rkf1WkyT1s4ggZ/mPtpKkPmbgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY\n+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEv\nSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhuhL4EXF/RLwSEU9P0ecLEfFcRDwVER/oxryS\npPZ16wr/AeCjrR6MiNuAazLz3cCngS93aV5JUpvmdGOQzHw8IlZP0eUO4MGq748iYlFELM/MV7ox\n/zvNkSNHGBsbY82aNSxbtqyt/rt37wZg/fr15+zT+PipU6cYHR1lxYoVrFixgsWLF5/Z55FHHuGh\nhx7ipptuYtmyZRw9epRjx47xwgsvALB06VLmzJnDhg0bOHHixITHDx48yMsvv8wtt9zCXXfdxYED\nB/jKV77CL3/5SzZu3Mi1117L/PnzOXjwIGvXruWll15iz549E8a9+eabWbp0KWNjY5w6dYqDBw+y\nceNG1q1b19Zxns+569Rszyd1JDO78gOsBp5u8di3gL/TsP1dYEOLvlmyLVu2Zq22JBct2pC12pLc\nsmXrtP2Hht6VMC9hbQ4PL5qwz5YtW3N4eFHC2oRLEmrV/VrC8oR5OTS0IFeuXFO1XVndXlONOTRp\n+5KE4WrfWsLiqm1e1afWYp/xtndXt3Oq20sntA8MzM3h4XUTatm06e5pj/N8zl2nZns+qZUqN6fP\n6XY6tTWQgd+xw4cPZ622JGFPQibsyVptSR4+fLhl/7lzF1ehee4+E8c7fE4/WJKwI2FhFbA7qrbx\nPjuq9sZ9Lk1YVAX9toS5TcYdH6txn7kNbePjbmsyfi1h36T6annJJQtbHuf5nLvZfq6kC6ndwO/K\nkk4bXgJWNWyvrNqa2rx585n7IyMjjIyMXKi6+srY2BjDw2s4efK6quU6hoZWMzY21nS5YGxsjMHB\n5cB84Ow+AwMrGRsbA2BgYFX12C7g6gn96u/R84HLq7b5wJqGPvOpP22N+6wBTlTbr1X7Lp3UZ0W1\nb+M+rza0zaf+EnityfgrgZ3A7zfUt4LMN4FlTY9z2bJlMz53nZrt+aRGo6OjjI6OznzHdt4V2vmh\n/q96b4vHPgZ8u7p/PfDDKca5cG+Dfc4rfK/wpfPBbC7pAFuAl4E3gZ8Cn6L+aZx/2tDni8BBYA8t\nlnOy8MDPPLsuvHDh+hms4S/I8TX01mv41+RUa/irVo2v4V+RE9ff5+TUa/iLqu3GNfxm+wxNmnsw\nz/4N4Gz71Gv4rY/zfM5dp2Z7PqmVdgM/6n37R0Rkv9U02/yUjp/SkWYiIsjMmLZfv4WrgS9JM9Nu\n4PtfK0hSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWp\nEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph\n4EtSIQx8SSqEgS9JhTDwJakQXQn8iLg1Ip6JiAMR8dkmj98YEUcj4snq53PdmFeS1L45nQ4QEQPA\nF4EPAy8DuyLim5n5zKSuP8jM2zudT5J0frpxhb8ReC4zX8zMt4CtwB1N+kUX5pIknaduBP4K4FDD\n9s+qtsl+IyKeiohvR8R7uzCvJGkGOl7SadOPgasy842IuA14CHhPq86bN28+c39kZISRkZELXZ8k\nXTRGR0cZHR2d8X6RmR1NHBHXA5sz89Zq+x4gM/PPptjnBeCDmfnXTR7LTmuSpJJEBJk57bJ5N5Z0\ndgFrI2J1RAwDnwQenlTM8ob7G6m/0ZwT9pKkC6fjJZ3MPB0Rm4Dt1N9A7s/M/RHx6frDeR/wiYi4\nC3gLOAn8VqfzSpJmpuMlnW5zSUeSZmY2l3QkSRcBA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQV\nwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEM\nfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKkRXAj8ibo2IZyLiQER8tkWf\nL0TEcxHxVER8oBvzSpLa13HgR8QA8EXgo8C1wG9HxK9N6nMbcE1mvhv4NPDlTueVJM1MN67wNwLP\nZeaLmfkWsBW4Y1KfO4AHATLzR8CiiFjehbklSW3qRuCvAA41bP+sapuqz0tN+kiSLqA5vS6gmc2b\nN5+5PzIywsjISM9qkaR+Mzo6yujo6Iz3i8zsaOKIuB7YnJm3Vtv3AJmZf9bQ58vAjsz8erX9DHBj\nZr7SZLzstCZJKklEkJkxXb9uLOnsAtZGxOqIGAY+CTw8qc/DwO9VhV0PHG0W9pKkC6fjJZ3MPB0R\nm4Dt1N9A7s/M/RHx6frDeV9mPhoRH4uIg8AJ4FOdzitJmpmOl3S6zSUdSZqZ2VzSkSRdBAx8SSqE\ngS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4\nklQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9J\nhTDwJakQczrZOSIuBb4OrAbGgH+Uma816TcGvAa8DbyVmRs7mVeSNHOdXuHfA3w3M38V+B7wxy36\nvQ2MZOZ6w16SeqPTwL8D+Gp1/6vAnS36RRfmkiR1oNMQvjwzXwHIzF8Al7fol8BjEbErIv6wwzkl\nSedh2jX8iHgMWN7YRD3AP9eke7YY5obM/HlELKMe/Psz8/FWc27evPnM/ZGREUZGRqYrU5KKMTo6\nyujo6Iz3i8xWGd3GzhH7qa/NvxIRVwA7MnPdNPvcC7yemX/e4vHspCZJKk1EkJkxXb9Ol3QeBv6g\nuv/7wDebFDIvIhZU9+cDHwF+0uG8kqQZ6vQKfwnwv4BVwIvUP5Z5NCKuBP5bZv6DiLga2EZ9uWcO\n8LXM/PwUY3qFL0kz0O4VfkeBfyEY+JI0M7O1pCNJukgY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDw\nJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+S\nCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIToK/Ij4RET8JCJOR8SG\nKfrdGhHPRMSBiPhsJ3NKks5Pp1f4e4HfBL7fqkNEDABfBD4KXAv8dkT8WofzSpJmaE4nO2fmswAR\nEVN02wg8l5kvVn23AncAz3Qy92w4cuQIY2NjrFmzhmXLlk3bPpNxjhw5wu7duwFYv379hPbGvvv3\n72fnzp1cdtllvPDCC9RqNRYuXMixY8c4efIkN998M+vWreOJJ55g+/btXHHFFezdu5fTp0+zdOlS\nXn/9dXbv3s38+fMZGhri2muv5fnnn+fZZ5/lxhtv5EMf+hDHjh3j8OHDrF27lve9730cP36cBQsW\ncPz4cU6dOsWTTz5JrVbj5Zdf5sCBA9x2221n6h2vfe7cubz55psMDg7y6KOPnmkfP86jR4+yePHi\nlsf66quvsm3bNk6cOMH73/9+brrppjP7j42NcerUKQ4ePMjatWs5ceLEmbmBpue2sa3xXK9atYrj\nx4+3/dw1jrdgwQIOHTo05dyTn9tW840/rxs3bmTdunVtvW4at1vN3a6ZvobbHaMb4/aDd8pxnCMz\nO/4BdgAbWjz2D4H7GrZ/F/jCFGNlP9iyZWvWakty0aINWastyS1btk7ZPpNxtmzZmkND70qYl7A2\nh4cXnWlv7HvLLbcl1BLeXd3Or26vqfa9JGE4V65cU7VfOenxOdPsP5wQ1TjzqvZaDg0tT6jl4ODK\nhv61qt811W3tTO0wMGmeyOHhRblp093V41dO6D/5WAcHzz2uwcF5uWnTZ7JWW3KmnrPjL06Yl4OD\n83JoaME557bxHG7a9JmqhrVnjrlWu7qt567x+avVfuXMMbSae7z/2ee2fty12vsm9Nm06TPVWO9J\nqOWmTXdP+7rZtOnuM9vDw4uazt3pa7vTMboxbj+4GI+jys3ps3raDvAY8HTDz97q9uMNfd5RgX/4\n8OGs1ZYk7EnIhD1Zqy3Jffv2NW0/fPhw2+PMnbs4585dnHDppPZLq/bxtm1VKJztU9/e0bB9acLC\nhLlV+5Ip9t/RZLxLq/Be2GSebZPGG++/b1LtH29R5++2HGfisR6uAnzyPOPH1eo8bKv6LarGqJ/b\niee71TEvTtgx5XM38fmbfG73nDP3+Ovj7HN77j612pJ8/PHHmx7Pvn37pnzdNH/uz8491XG089pu\nd/+pxpj4+p35uP2gG+enF9oN/GmXdDLzlg5/iXgJuKphe2XV1tLmzZvP3B8ZGWFkZKTDEmZmbGyM\n4eE1nDx5XdVyHUNDq9m5c2fT9rGxsaa/9jUbZ3Dwck6fTuBdwNn2iBUMDLzV0PYa9VN1tg+sAOY3\nbK8BXgferNrXTLH//CbjraH+VFzWZJ7XJo033n8ncHVD+xMt6vxB1X7uOBOPdRewusk8rwOnqv1X\nTXq8cdwTwBjwtxkcvByotXHMJ4D5Uz530Pj8TT63jePU5x5/fQwOLq/mPXefoaHVbN++venx7Ny5\n88zSTrPXTf04Jj/3Z+ee6jiaH1N7r+F2xxgYWEn9dXj+4/aDbpyf2TA6Osro6OjMd2znXWG6H+pX\n+B9s8dggcJD6v+ph4Clg3RRjXcD3wfZ4he8V/sTnzyv8dsbwCr936NaSzpQ7w53AIeAk8HPgO1X7\nlcAjDf1uBZ4FngPumWbMC3xq2jO+jrdw4fqma/iT22cyTn2dd0GOr5tPXtce7/uRj4yv4a+tbudl\nszX8VavG1/CvmPT44DT7j6/hD2fjGv6cOcuyvoa/oqF/4xr+8Jn2+vp4TJpn8hr+8gn9Jx/r4OC5\nx1Vfw6+vW4/Xc3b8Rdm4jj753Daew7M1nD3muXPXzHgNf+7cNRNqbDb3eP+zz239uGu1vzVpDf/u\nbPybxFRr+I3HMb49vobf7muw3dd2p2N0Y9x+cDEeR7uBH/W+/SMisl9q8lM6fkqncTw/pTP9GO+U\nT7dcbMcREWTmVJ+WrPfrl3Ad10+BL0kXg3YD3/9aQZIKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXC\nwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8\nSSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqREeBHxGfiIifRMTpiNgwRb+x\niNgTEbsjYmcnc0qSzk+nV/h7gd8Evj9Nv7eBkcxcn5kbO5xz1o2Ojva6hHNYU3v6sSboz7qsqT39\nWFO7Ogr8zHw2M58DYpqu0elcvdSPT7A1tacfa4L+rMua2tOPNbVrtkI4gcciYldE/OEszSlJajBn\nug4R8RiwvLGJeoD/aWZ+q815bsjMn0fEMurBvz8zH595uZKk8xWZ2fkgETuAf5WZT7bR917g9cz8\n8xaPd16QJBUmM6dbWp/+Cn8Gmk4WEfOAgcw8HhHzgY8A/67VIO0ULUmauU4/lnlnRBwCrgceiYjv\nVO1XRsQjVbflwOMRsRv4IfCtzNzeybySpJnrypKOJKn/9d1HJSPi3zd8SesvI+KKXtcEEBH/MSL2\nR8RTEfF/ImJhH9TU1hffZqmWWyPimYg4EBGf7WUtVT33R8QrEfF0r2sZFxErI+J7EfFXEbE3Iu7u\ng5ouiYgfVf/e9lZ/Y+sLETEQEU9GxMO9rmVcP36JNCIWRcT/rvLpryLi11v27bcr/IhYkJnHq/t/\nBLw3M+/qcVlExM3A9zLz7Yj4PJCZ+cc9rulXqX+p7b8C/7qdP5pfoDoGgAPAh4GXgV3AJzPzmV7U\nU9X0IeA48GBmXterOhpVFy9XZOZTEbEA+DFwRy/PU1XXvMx8IyIGgSeAuzOz52EWEf8S+CCwMDNv\n73U9ABHxPPDBzPybXtcyLiL+B/D9zHwgIuYA8zLzWLO+fXeFPx72lfnUA63nMvO7mTleyw+Blb2s\nB2b0xbcLbSPwXGa+mJlvAVuBO3pZUPWx3775RwmQmb/IzKeq+8eB/cCK3lYFmflGdfcS6h/k6PlV\nYESsBD4GfKXXtUzSV18irVYa/m5mPgCQmf+vVdhDHxXeKCL+Q0T8FPgd4N/2up4m/gnwnV4X0UdW\nAIcatn9GHwRZP4uINcAHgB/1tpIzSye7gV8Aj2Xmrl7XBPwn4N/QB28+k/Tbl0ivBl6NiAeq5a/7\nIqLWqnNPAj8iHouIpxt+9la3HwfIzM9l5lXA14A/6pe6qj5/CryVmVv6pSZdXKrlnG8An5n0G21P\nZObbmbme+m+tvx4R7+1lPRHx94FXqt+Ggt7/BtvohszcQP23j39RLR320hxgA/Clqq43gHum6jzr\nMvOWNrtuAR4FNl+4as6arq6I+APqT/Tfm416YEbnqpdeAq5q2F5ZtWmSao31G8D/zMxv9rqeRpl5\nrPoS5a3Avh6WcgNwe0R8DKgB74qIBzPz93pYEwCZ+fPq9khEbKO+nNnL/zXgZ8ChzPy/1fY3gJYf\nmui7JZ2IWNuweSf1dc6ei4hbqf+KeXtmvtnrepro5VXQLmBtRKyOiGHgk0A/fLKi364OAf47sC8z\n/3OvCwGIiKURsai6XwNuAXr6R+TM/JPMvCozf4X6a+l7/RD2ETGv+u2Mhi+R/qSXNWXmK8ChiHhP\n1fRhpniz7skV/jQ+XxX/NvAi8M96XM+4vwCGqa/fAfwwM/95LwuKiDurupZS/+LbU5l522zXkZmn\nI2ITsJ36RcT9mdnTN+qI2AKMAJdVfw+6d/wPWz2s6QbgHwN7qzXzBP4kM/+yh2VdCXy1+qTVAPD1\nzHy0h/X0s+XAtqj/9y9zgK/1yZdI7wa+FhFDwPPAp1p17LuPZUqSLoy+W9KRJF0YBr4kFcLAl6RC\nGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYX4/yk5TRtcSyaoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2deadc190f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2deadba1c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyse y as a function of all the other features (one by one)\n",
    "number_of_params = tX.shape[1]\n",
    "for feature in range(tX.shape[1]):\n",
    "    print('feature: ',feature)\n",
    "    plt.scatter(tX[:,feature], y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.5\n",
      "Gradient Descent(1/999): loss=0.4967971857441695\n",
      "Gradient Descent(2/999): loss=0.49365857536333907\n",
      "Gradient Descent(3/999): loss=0.49058287517561633\n",
      "Gradient Descent(4/999): loss=0.48756881767470217\n",
      "Gradient Descent(5/999): loss=0.4846151609984927\n",
      "Gradient Descent(6/999): loss=0.48172068840849974\n",
      "Gradient Descent(7/999): loss=0.47888420777986695\n",
      "Gradient Descent(8/999): loss=0.476104551101767\n",
      "Gradient Descent(9/999): loss=0.4733805739879675\n",
      "Gradient Descent(10/999): loss=0.47071115519735707\n",
      "Gradient Descent(11/999): loss=0.4680951961642316\n",
      "Gradient Descent(12/999): loss=0.46553162053813696\n",
      "Gradient Descent(13/999): loss=0.4630193737330782\n",
      "Gradient Descent(14/999): loss=0.4605574224858998\n",
      "Gradient Descent(15/999): loss=0.4581447544236535\n",
      "Gradient Descent(16/999): loss=0.45578037763976736\n",
      "Gradient Descent(17/999): loss=0.45346332027883973\n",
      "Gradient Descent(18/999): loss=0.4511926301298787\n",
      "Gradient Descent(19/999): loss=0.44896737422781824\n",
      "Gradient Descent(20/999): loss=0.44678663846313954\n",
      "Gradient Descent(21/999): loss=0.4446495271994341\n",
      "Gradient Descent(22/999): loss=0.4425551628987458\n",
      "Gradient Descent(23/999): loss=0.44050268575453344\n",
      "Gradient Descent(24/999): loss=0.43849125333209826\n",
      "Gradient Descent(25/999): loss=0.4365200402163246\n",
      "Gradient Descent(26/999): loss=0.4345882376665851\n",
      "Gradient Descent(27/999): loss=0.4326950532786624\n",
      "Gradient Descent(28/999): loss=0.43083971065354676\n",
      "Gradient Descent(29/999): loss=0.42902144907296785\n",
      "Gradient Descent(30/999): loss=0.42723952318152425\n",
      "Gradient Descent(31/999): loss=0.42549320267527563\n",
      "Gradient Descent(32/999): loss=0.42378177199666583\n",
      "Gradient Descent(33/999): loss=0.42210453003564885\n",
      "Gradient Descent(34/999): loss=0.4204607898368903\n",
      "Gradient Descent(35/999): loss=0.41884987831292037\n",
      "Gradient Descent(36/999): loss=0.4172711359631181\n",
      "Gradient Descent(37/999): loss=0.4157239165984069\n",
      "Gradient Descent(38/999): loss=0.4142075870715464\n",
      "Gradient Descent(39/999): loss=0.4127215270129049\n",
      "Gradient Descent(40/999): loss=0.4112651285716018\n",
      "Gradient Descent(41/999): loss=0.4098377961619111\n",
      "Gradient Descent(42/999): loss=0.40843894621481697\n",
      "Gradient Descent(43/999): loss=0.40706800693461814\n",
      "Gradient Descent(44/999): loss=0.4057244180604783\n",
      "Gradient Descent(45/999): loss=0.40440763063282004\n",
      "Gradient Descent(46/999): loss=0.40311710676446516\n",
      "Gradient Descent(47/999): loss=0.4018523194164248\n",
      "Gradient Descent(48/999): loss=0.4006127521782434\n",
      "Gradient Descent(49/999): loss=0.3993978990528052\n",
      "Gradient Descent(50/999): loss=0.39820726424551095\n",
      "Gradient Descent(51/999): loss=0.3970403619577375\n",
      "Gradient Descent(52/999): loss=0.39589671618449174\n",
      "Gradient Descent(53/999): loss=0.3947758605161729\n",
      "Gradient Descent(54/999): loss=0.39367733794436305\n",
      "Gradient Descent(55/999): loss=0.3926007006715583\n",
      "Gradient Descent(56/999): loss=0.39154550992476567\n",
      "Gradient Descent(57/999): loss=0.39051133577288366\n",
      "Gradient Descent(58/999): loss=0.38949775694779093\n",
      "Gradient Descent(59/999): loss=0.3885043606690667\n",
      "Gradient Descent(60/999): loss=0.3875307424722701\n",
      "Gradient Descent(61/999): loss=0.38657650604070526\n",
      "Gradient Descent(62/999): loss=0.38564126304060037\n",
      "Gradient Descent(63/999): loss=0.38472463295963394\n",
      "Gradient Descent(64/999): loss=0.38382624294873635\n",
      "Gradient Descent(65/999): loss=0.3829457276671036\n",
      "Gradient Descent(66/999): loss=0.38208272913035557\n",
      "Gradient Descent(67/999): loss=0.38123689656177534\n",
      "Gradient Descent(68/999): loss=0.3804078862465681\n",
      "Gradient Descent(69/999): loss=0.37959536138907607\n",
      "Gradient Descent(70/999): loss=0.3787989919728908\n",
      "Gradient Descent(71/999): loss=0.37801845462380385\n",
      "Gradient Descent(72/999): loss=0.3772534324755375\n",
      "Gradient Descent(73/999): loss=0.37650361503819896\n",
      "Gradient Descent(74/999): loss=0.37576869806940283\n",
      "Gradient Descent(75/999): loss=0.3750483834480087\n",
      "Gradient Descent(76/999): loss=0.3743423790504183\n",
      "Gradient Descent(77/999): loss=0.3736503986293816\n",
      "Gradient Descent(78/999): loss=0.3729721616952618\n",
      "Gradient Descent(79/999): loss=0.37230739339970653\n",
      "Gradient Descent(80/999): loss=0.37165582442167905\n",
      "Gradient Descent(81/999): loss=0.37101719085579954\n",
      "Gradient Descent(82/999): loss=0.37039123410295105\n",
      "Gradient Descent(83/999): loss=0.36977770076310207\n",
      "Gradient Descent(84/999): loss=0.36917634253030274\n",
      "Gradient Descent(85/999): loss=0.3685869160898093\n",
      "Gradient Descent(86/999): loss=0.36800918301729424\n",
      "Gradient Descent(87/999): loss=0.367442909680098\n",
      "Gradient Descent(88/999): loss=0.36688786714048355\n",
      "Gradient Descent(89/999): loss=0.36634383106085106\n",
      "Gradient Descent(90/999): loss=0.36581058161087276\n",
      "Gradient Descent(91/999): loss=0.36528790337651046\n",
      "Gradient Descent(92/999): loss=0.364775585270876\n",
      "Gradient Descent(93/999): loss=0.36427342044689826\n",
      "Gradient Descent(94/999): loss=0.3637812062117586\n",
      "Gradient Descent(95/999): loss=0.3632987439430607\n",
      "Gradient Descent(96/999): loss=0.3628258390066981\n",
      "Gradient Descent(97/999): loss=0.36236230067638514\n",
      "Gradient Descent(98/999): loss=0.36190794205481835\n",
      "Gradient Descent(99/999): loss=0.36146257999643366\n",
      "Gradient Descent(100/999): loss=0.3610260350317293\n",
      "Gradient Descent(101/999): loss=0.36059813129311963\n",
      "Gradient Descent(102/999): loss=0.3601786964422915\n",
      "Gradient Descent(103/999): loss=0.3597675615990311\n",
      "Gradient Descent(104/999): loss=0.3593645612714916\n",
      "Gradient Descent(105/999): loss=0.3589695332878728\n",
      "Gradient Descent(106/999): loss=0.35858231872948393\n",
      "Gradient Descent(107/999): loss=0.3582027618651612\n",
      "Gradient Descent(108/999): loss=0.3578307100870127\n",
      "Gradient Descent(109/999): loss=0.3574660138474644\n",
      "Gradient Descent(110/999): loss=0.3571085265975794\n",
      "Gradient Descent(111/999): loss=0.35675810472662606\n",
      "Gradient Descent(112/999): loss=0.3564146075028684\n",
      "Gradient Descent(113/999): loss=0.35607789701555603\n",
      "Gradient Descent(114/999): loss=0.35574783811808525\n",
      "Gradient Descent(115/999): loss=0.3554242983723126\n",
      "Gradient Descent(116/999): loss=0.3551071479939934\n",
      "Gradient Descent(117/999): loss=0.3547962597993245\n",
      "Gradient Descent(118/999): loss=0.3544915091525689\n",
      "Gradient Descent(119/999): loss=0.3541927739147381\n",
      "Gradient Descent(120/999): loss=0.35389993439331413\n",
      "Gradient Descent(121/999): loss=0.35361287329298724\n",
      "Gradient Descent(122/999): loss=0.35333147566739037\n",
      "Gradient Descent(123/999): loss=0.3530556288718087\n",
      "Gradient Descent(124/999): loss=0.35278522251684646\n",
      "Gradient Descent(125/999): loss=0.35252014842302953\n",
      "Gradient Descent(126/999): loss=0.3522603005763267\n",
      "Gradient Descent(127/999): loss=0.35200557508456887\n",
      "Gradient Descent(128/999): loss=0.3517558701347503\n",
      "Gradient Descent(129/999): loss=0.35151108595119196\n",
      "Gradient Descent(130/999): loss=0.3512711247545512\n",
      "Gradient Descent(131/999): loss=0.3510358907216592\n",
      "Gradient Descent(132/999): loss=0.35080528994616983\n",
      "Gradient Descent(133/999): loss=0.3505792304000049\n",
      "Gradient Descent(134/999): loss=0.3503576218955762\n",
      "Gradient Descent(135/999): loss=0.3501403760487727\n",
      "Gradient Descent(136/999): loss=0.34992740624269475\n",
      "Gradient Descent(137/999): loss=0.3497186275921206\n",
      "Gradient Descent(138/999): loss=0.34951395690869064\n",
      "Gradient Descent(139/999): loss=0.3493133126667957\n",
      "Gradient Descent(140/999): loss=0.3491166149701526\n",
      "Gradient Descent(141/999): loss=0.34892378551905634\n",
      "Gradient Descent(142/999): loss=0.34873474757829237\n",
      "Gradient Descent(143/999): loss=0.34854942594569704\n",
      "Gradient Descent(144/999): loss=0.3483677469213535\n",
      "Gradient Descent(145/999): loss=0.3481896382774079\n",
      "Gradient Descent(146/999): loss=0.3480150292284965\n",
      "Gradient Descent(147/999): loss=0.3478438504027686\n",
      "Gradient Descent(148/999): loss=0.3476760338134951\n",
      "Gradient Descent(149/999): loss=0.3475115128312488\n",
      "Gradient Descent(150/999): loss=0.3473502221566479\n",
      "Gradient Descent(151/999): loss=0.3471920977936476\n",
      "Gradient Descent(152/999): loss=0.347037077023372\n",
      "Gradient Descent(153/999): loss=0.34688509837847303\n",
      "Gradient Descent(154/999): loss=0.34673610161800716\n",
      "Gradient Descent(155/999): loss=0.3465900277028186\n",
      "Gradient Descent(156/999): loss=0.3464468187714192\n",
      "Gradient Descent(157/999): loss=0.3463064181163549\n",
      "Gradient Descent(158/999): loss=0.34616877016104797\n",
      "Gradient Descent(159/999): loss=0.3460338204371076\n",
      "Gradient Descent(160/999): loss=0.345901515562097\n",
      "Gradient Descent(161/999): loss=0.34577180321774875\n",
      "Gradient Descent(162/999): loss=0.3456446321286199\n",
      "Gradient Descent(163/999): loss=0.34551995204117686\n",
      "Gradient Descent(164/999): loss=0.34539771370330213\n",
      "Gradient Descent(165/999): loss=0.34527786884421374\n",
      "Gradient Descent(166/999): loss=0.34516037015478995\n",
      "Gradient Descent(167/999): loss=0.3450451712682899\n",
      "Gradient Descent(168/999): loss=0.3449322267414629\n",
      "Gradient Descent(169/999): loss=0.3448214920360385\n",
      "Gradient Descent(170/999): loss=0.3447129235005893\n",
      "Gradient Descent(171/999): loss=0.3446064783527591\n",
      "Gradient Descent(172/999): loss=0.3445021146618489\n",
      "Gradient Descent(173/999): loss=0.34439979133175463\n",
      "Gradient Descent(174/999): loss=0.3442994680842466\n",
      "Gradient Descent(175/999): loss=0.3442011054425877\n",
      "Gradient Descent(176/999): loss=0.3441046647154799\n",
      "Gradient Descent(177/999): loss=0.34401010798133524\n",
      "Gradient Descent(178/999): loss=0.34391739807286226\n",
      "Gradient Descent(179/999): loss=0.34382649856196373\n",
      "Gradient Descent(180/999): loss=0.34373737374493774\n",
      "Gradient Descent(181/999): loss=0.3436499886279771\n",
      "Gradient Descent(182/999): loss=0.34356430891296014\n",
      "Gradient Descent(183/999): loss=0.3434803009835279\n",
      "Gradient Descent(184/999): loss=0.3433979318914408\n",
      "Gradient Descent(185/999): loss=0.34331716934321094\n",
      "Gradient Descent(186/999): loss=0.3432379816870019\n",
      "Gradient Descent(187/999): loss=0.3431603378997935\n",
      "Gradient Descent(188/999): loss=0.3430842075748035\n",
      "Gradient Descent(189/999): loss=0.343009560909163\n",
      "Gradient Descent(190/999): loss=0.34293636869183985\n",
      "Gradient Descent(191/999): loss=0.3428646022918043\n",
      "Gradient Descent(192/999): loss=0.3427942336464328\n",
      "Gradient Descent(193/999): loss=0.3427252352501452\n",
      "Gradient Descent(194/999): loss=0.3426575801432704\n",
      "Gradient Descent(195/999): loss=0.3425912419011354\n",
      "Gradient Descent(196/999): loss=0.34252619462337397\n",
      "Gradient Descent(197/999): loss=0.3424624129234503\n",
      "Gradient Descent(198/999): loss=0.3423998719183926\n",
      "Gradient Descent(199/999): loss=0.3423385472187339\n",
      "Gradient Descent(200/999): loss=0.3422784149186542\n",
      "Gradient Descent(201/999): loss=0.34221945158632094\n",
      "Gradient Descent(202/999): loss=0.3421616342544236\n",
      "Gradient Descent(203/999): loss=0.3421049404108982\n",
      "Gradient Descent(204/999): loss=0.34204934798983827\n",
      "Gradient Descent(205/999): loss=0.3419948353625887\n",
      "Gradient Descent(206/999): loss=0.34194138132901736\n",
      "Gradient Descent(207/999): loss=0.34188896510896316\n",
      "Gradient Descent(208/999): loss=0.3418375663338549\n",
      "Gradient Descent(209/999): loss=0.34178716503849926\n",
      "Gradient Descent(210/999): loss=0.34173774165303283\n",
      "Gradient Descent(211/999): loss=0.341689276995036\n",
      "Gradient Descent(212/999): loss=0.34164175226180504\n",
      "Gradient Descent(213/999): loss=0.34159514902277927\n",
      "Gradient Descent(214/999): loss=0.34154944921212055\n",
      "Gradient Descent(215/999): loss=0.34150463512144086\n",
      "Gradient Descent(216/999): loss=0.3414606893926765\n",
      "Gradient Descent(217/999): loss=0.3414175950111048\n",
      "Gradient Descent(218/999): loss=0.34137533529850117\n",
      "Gradient Descent(219/999): loss=0.34133389390643293\n",
      "Gradient Descent(220/999): loss=0.34129325480968836\n",
      "Gradient Descent(221/999): loss=0.34125340229983664\n",
      "Gradient Descent(222/999): loss=0.3412143209789177\n",
      "Gradient Descent(223/999): loss=0.34117599575325785\n",
      "Gradient Descent(224/999): loss=0.3411384118274099\n",
      "Gradient Descent(225/999): loss=0.3411015546982146\n",
      "Gradient Descent(226/999): loss=0.34106541014898106\n",
      "Gradient Descent(227/999): loss=0.34102996424378335\n",
      "Gradient Descent(228/999): loss=0.3409952033218721\n",
      "Gradient Descent(229/999): loss=0.3409611139921981\n",
      "Gradient Descent(230/999): loss=0.3409276831280444\n",
      "Gradient Descent(231/999): loss=0.3408948978617673\n",
      "Gradient Descent(232/999): loss=0.3408627455796423\n",
      "Gradient Descent(233/999): loss=0.3408312139168123\n",
      "Gradient Descent(234/999): loss=0.34080029075233875\n",
      "Gradient Descent(235/999): loss=0.34076996420434996\n",
      "Gradient Descent(236/999): loss=0.3407402226252879\n",
      "Gradient Descent(237/999): loss=0.3407110545972499\n",
      "Gradient Descent(238/999): loss=0.34068244892742255\n",
      "Gradient Descent(239/999): loss=0.34065439464360864\n",
      "Gradient Descent(240/999): loss=0.34062688098984245\n",
      "Gradient Descent(241/999): loss=0.34059989742209285\n",
      "Gradient Descent(242/999): loss=0.34057343360405257\n",
      "Gradient Descent(243/999): loss=0.34054747940301233\n",
      "Gradient Descent(244/999): loss=0.3405220248858161\n",
      "Gradient Descent(245/999): loss=0.3404970603148986\n",
      "Gradient Descent(246/999): loss=0.34047257614440085\n",
      "Gradient Descent(247/999): loss=0.34044856301636445\n",
      "Gradient Descent(248/999): loss=0.34042501175700146\n",
      "Gradient Descent(249/999): loss=0.34040191337303866\n",
      "Gradient Descent(250/999): loss=0.3403792590481353\n",
      "Gradient Descent(251/999): loss=0.34035704013937185\n",
      "Gradient Descent(252/999): loss=0.34033524817380967\n",
      "Gradient Descent(253/999): loss=0.3403138748451188\n",
      "Gradient Descent(254/999): loss=0.3402929120102736\n",
      "Gradient Descent(255/999): loss=0.3402723516863139\n",
      "Gradient Descent(256/999): loss=0.34025218604717095\n",
      "Gradient Descent(257/999): loss=0.3402324074205576\n",
      "Gradient Descent(258/999): loss=0.340213008284919\n",
      "Gradient Descent(259/999): loss=0.34019398126644546\n",
      "Gradient Descent(260/999): loss=0.34017531913614474\n",
      "Gradient Descent(261/999): loss=0.3401570148069723\n",
      "Gradient Descent(262/999): loss=0.34013906133101907\n",
      "Gradient Descent(263/999): loss=0.3401214518967555\n",
      "Gradient Descent(264/999): loss=0.34010417982633073\n",
      "Gradient Descent(265/999): loss=0.3400872385729248\n",
      "Gradient Descent(266/999): loss=0.3400706217181549\n",
      "Gradient Descent(267/999): loss=0.3400543229695323\n",
      "Gradient Descent(268/999): loss=0.34003833615797086\n",
      "Gradient Descent(269/999): loss=0.34002265523534403\n",
      "Gradient Descent(270/999): loss=0.34000727427209254\n",
      "Gradient Descent(271/999): loss=0.33999218745487736\n",
      "Gradient Descent(272/999): loss=0.3399773890842815\n",
      "Gradient Descent(273/999): loss=0.3399628735725562\n",
      "Gradient Descent(274/999): loss=0.33994863544141346\n",
      "Gradient Descent(275/999): loss=0.3399346693198605\n",
      "Gradient Descent(276/999): loss=0.33992096994207993\n",
      "Gradient Descent(277/999): loss=0.3399075321453494\n",
      "Gradient Descent(278/999): loss=0.3398943508680056\n",
      "Gradient Descent(279/999): loss=0.33988142114744613\n",
      "Gradient Descent(280/999): loss=0.33986873811817275\n",
      "Gradient Descent(281/999): loss=0.33985629700987346\n",
      "Gradient Descent(282/999): loss=0.339844093145542\n",
      "Gradient Descent(283/999): loss=0.3398321219396354\n",
      "Gradient Descent(284/999): loss=0.3398203788962686\n",
      "Gradient Descent(285/999): loss=0.3398088596074436\n",
      "Gradient Descent(286/999): loss=0.33979755975131537\n",
      "Gradient Descent(287/999): loss=0.33978647509049154\n",
      "Gradient Descent(288/999): loss=0.3397756014703654\n",
      "Gradient Descent(289/999): loss=0.33976493481748354\n",
      "Gradient Descent(290/999): loss=0.3397544711379446\n",
      "Gradient Descent(291/999): loss=0.33974420651582987\n",
      "Gradient Descent(292/999): loss=0.3397341371116667\n",
      "Gradient Descent(293/999): loss=0.33972425916092014\n",
      "Gradient Descent(294/999): loss=0.3397145689725163\n",
      "Gradient Descent(295/999): loss=0.3397050629273945\n",
      "Gradient Descent(296/999): loss=0.3396957374770879\n",
      "Gradient Descent(297/999): loss=0.3396865891423331\n",
      "Gradient Descent(298/999): loss=0.339677614511706\n",
      "Gradient Descent(299/999): loss=0.3396688102402868\n",
      "Gradient Descent(300/999): loss=0.3396601730483492\n",
      "Gradient Descent(301/999): loss=0.3396516997200776\n",
      "Gradient Descent(302/999): loss=0.33964338710230846\n",
      "Gradient Descent(303/999): loss=0.3396352321032975\n",
      "Gradient Descent(304/999): loss=0.33962723169151054\n",
      "Gradient Descent(305/999): loss=0.3396193828944392\n",
      "Gradient Descent(306/999): loss=0.3396116827974398\n",
      "Gradient Descent(307/999): loss=0.33960412854259453\n",
      "Gradient Descent(308/999): loss=0.3395967173275966\n",
      "Gradient Descent(309/999): loss=0.3395894464046564\n",
      "Gradient Descent(310/999): loss=0.3395823130794301\n",
      "Gradient Descent(311/999): loss=0.3395753147099686\n",
      "Gradient Descent(312/999): loss=0.3395684487056883\n",
      "Gradient Descent(313/999): loss=0.3395617125263617\n",
      "Gradient Descent(314/999): loss=0.33955510368112796\n",
      "Gradient Descent(315/999): loss=0.3395486197275237\n",
      "Gradient Descent(316/999): loss=0.3395422582705321\n",
      "Gradient Descent(317/999): loss=0.33953601696165153\n",
      "Gradient Descent(318/999): loss=0.3395298934979823\n",
      "Gradient Descent(319/999): loss=0.3395238856213317\n",
      "Gradient Descent(320/999): loss=0.3395179911173365\n",
      "Gradient Descent(321/999): loss=0.33951220781460323\n",
      "Gradient Descent(322/999): loss=0.339506533583865\n",
      "Gradient Descent(323/999): loss=0.3395009663371554\n",
      "Gradient Descent(324/999): loss=0.3394955040269984\n",
      "Gradient Descent(325/999): loss=0.3394901446456148\n",
      "Gradient Descent(326/999): loss=0.3394848862241438\n",
      "Gradient Descent(327/999): loss=0.33947972683188027\n",
      "Gradient Descent(328/999): loss=0.3394746645755272\n",
      "Gradient Descent(329/999): loss=0.3394696975984627\n",
      "Gradient Descent(330/999): loss=0.33946482408002154\n",
      "Gradient Descent(331/999): loss=0.33946004223479115\n",
      "Gradient Descent(332/999): loss=0.33945535031192103\n",
      "Gradient Descent(333/999): loss=0.3394507465944466\n",
      "Gradient Descent(334/999): loss=0.33944622939862534\n",
      "Gradient Descent(335/999): loss=0.33944179707328703\n",
      "Gradient Descent(336/999): loss=0.3394374479991964\n",
      "Gradient Descent(337/999): loss=0.33943318058842853\n",
      "Gradient Descent(338/999): loss=0.3394289932837558\n",
      "Gradient Descent(339/999): loss=0.3394248845580485\n",
      "Gradient Descent(340/999): loss=0.33942085291368573\n",
      "Gradient Descent(341/999): loss=0.33941689688197896\n",
      "Gradient Descent(342/999): loss=0.33941301502260635\n",
      "Gradient Descent(343/999): loss=0.3394092059230587\n",
      "Gradient Descent(344/999): loss=0.33940546819809553\n",
      "Gradient Descent(345/999): loss=0.33940180048921365\n",
      "Gradient Descent(346/999): loss=0.3393982014641236\n",
      "Gradient Descent(347/999): loss=0.33939466981623867\n",
      "Gradient Descent(348/999): loss=0.33939120426417313\n",
      "Gradient Descent(349/999): loss=0.3393878035512502\n",
      "Gradient Descent(350/999): loss=0.3393844664450198\n",
      "Gradient Descent(351/999): loss=0.33938119173678644\n",
      "Gradient Descent(352/999): loss=0.33937797824114513\n",
      "Gradient Descent(353/999): loss=0.33937482479552755\n",
      "Gradient Descent(354/999): loss=0.33937173025975725\n",
      "Gradient Descent(355/999): loss=0.3393686935156123\n",
      "Gradient Descent(356/999): loss=0.33936571346639793\n",
      "Gradient Descent(357/999): loss=0.33936278903652695\n",
      "Gradient Descent(358/999): loss=0.3393599191711085\n",
      "Gradient Descent(359/999): loss=0.33935710283554527\n",
      "Gradient Descent(360/999): loss=0.3393543390151376\n",
      "Gradient Descent(361/999): loss=0.3393516267146967\n",
      "Gradient Descent(362/999): loss=0.3393489649581647\n",
      "Gradient Descent(363/999): loss=0.33934635278824216\n",
      "Gradient Descent(364/999): loss=0.3393437892660234\n",
      "Gradient Descent(365/999): loss=0.3393412734706381\n",
      "Gradient Descent(366/999): loss=0.33933880449890147\n",
      "Gradient Descent(367/999): loss=0.3393363814649693\n",
      "Gradient Descent(368/999): loss=0.33933400350000154\n",
      "Gradient Descent(369/999): loss=0.33933166975183143\n",
      "Gradient Descent(370/999): loss=0.3393293793846418\n",
      "Gradient Descent(371/999): loss=0.33932713157864713\n",
      "Gradient Descent(372/999): loss=0.33932492552978216\n",
      "Gradient Descent(373/999): loss=0.33932276044939674\n",
      "Gradient Descent(374/999): loss=0.33932063556395664\n",
      "Gradient Descent(375/999): loss=0.33931855011474965\n",
      "Gradient Descent(376/999): loss=0.3393165033575981\n",
      "Gradient Descent(377/999): loss=0.3393144945625774\n",
      "Gradient Descent(378/999): loss=0.3393125230137386\n",
      "Gradient Descent(379/999): loss=0.33931058800883773\n",
      "Gradient Descent(380/999): loss=0.3393086888590706\n",
      "Gradient Descent(381/999): loss=0.3393068248888111\n",
      "Gradient Descent(382/999): loss=0.3393049954353568\n",
      "Gradient Descent(383/999): loss=0.33930319984867785\n",
      "Gradient Descent(384/999): loss=0.3393014374911719\n",
      "Gradient Descent(385/999): loss=0.33929970773742296\n",
      "Gradient Descent(386/999): loss=0.3392980099739657\n",
      "Gradient Descent(387/999): loss=0.3392963435990542\n",
      "Gradient Descent(388/999): loss=0.3392947080224345\n",
      "Gradient Descent(389/999): loss=0.33929310266512275\n",
      "Gradient Descent(390/999): loss=0.339291526959187\n",
      "Gradient Descent(391/999): loss=0.33928998034753327\n",
      "Gradient Descent(392/999): loss=0.3392884622836961\n",
      "Gradient Descent(393/999): loss=0.3392869722316331\n",
      "Gradient Descent(394/999): loss=0.33928550966552307\n",
      "Gradient Descent(395/999): loss=0.33928407406956906\n",
      "Gradient Descent(396/999): loss=0.3392826649378041\n",
      "Gradient Descent(397/999): loss=0.3392812817739017\n",
      "Gradient Descent(398/999): loss=0.33927992409098934\n",
      "Gradient Descent(399/999): loss=0.33927859141146643\n",
      "Gradient Descent(400/999): loss=0.3392772832668246\n",
      "Gradient Descent(401/999): loss=0.33927599919747287\n",
      "Gradient Descent(402/999): loss=0.3392747387525651\n",
      "Gradient Descent(403/999): loss=0.33927350148983165\n",
      "Gradient Descent(404/999): loss=0.3392722869754138\n",
      "Gradient Descent(405/999): loss=0.33927109478370154\n",
      "Gradient Descent(406/999): loss=0.33926992449717475\n",
      "Gradient Descent(407/999): loss=0.33926877570624675\n",
      "Gradient Descent(408/999): loss=0.3392676480091123\n",
      "Gradient Descent(409/999): loss=0.33926654101159687\n",
      "Gradient Descent(410/999): loss=0.33926545432701016\n",
      "Gradient Descent(411/999): loss=0.33926438757600186\n",
      "Gradient Descent(412/999): loss=0.3392633403864202\n",
      "Gradient Descent(413/999): loss=0.3392623123931736\n",
      "Gradient Descent(414/999): loss=0.33926130323809495\n",
      "Gradient Descent(415/999): loss=0.33926031256980793\n",
      "Gradient Descent(416/999): loss=0.339259340043597\n",
      "Gradient Descent(417/999): loss=0.3392583853212787\n",
      "Gradient Descent(418/999): loss=0.33925744807107683\n",
      "Gradient Descent(419/999): loss=0.33925652796749844\n",
      "Gradient Descent(420/999): loss=0.3392556246912141\n",
      "Gradient Descent(421/999): loss=0.33925473792893845\n",
      "Gradient Descent(422/999): loss=0.3392538673733151\n",
      "Gradient Descent(423/999): loss=0.339253012722802\n",
      "Gradient Descent(424/999): loss=0.3392521736815604\n",
      "Gradient Descent(425/999): loss=0.33925134995934525\n",
      "Gradient Descent(426/999): loss=0.33925054127139775\n",
      "Gradient Descent(427/999): loss=0.33924974733834007\n",
      "Gradient Descent(428/999): loss=0.33924896788607256\n",
      "Gradient Descent(429/999): loss=0.339248202645672\n",
      "Gradient Descent(430/999): loss=0.33924745135329304\n",
      "Gradient Descent(431/999): loss=0.3392467137500703\n",
      "Gradient Descent(432/999): loss=0.3392459895820233\n",
      "Gradient Descent(433/999): loss=0.3392452785999629\n",
      "Gradient Descent(434/999): loss=0.33924458055939943\n",
      "Gradient Descent(435/999): loss=0.33924389522045273\n",
      "Gradient Descent(436/999): loss=0.3392432223477641\n",
      "Gradient Descent(437/999): loss=0.3392425617104099\n",
      "Gradient Descent(438/999): loss=0.3392419130818159\n",
      "Gradient Descent(439/999): loss=0.3392412762396755\n",
      "Gradient Descent(440/999): loss=0.33924065096586675\n",
      "Gradient Descent(441/999): loss=0.33924003704637334\n",
      "Gradient Descent(442/999): loss=0.3392394342712057\n",
      "Gradient Descent(443/999): loss=0.3392388424343241\n",
      "Gradient Descent(444/999): loss=0.33923826133356355\n",
      "Gradient Descent(445/999): loss=0.3392376907705594\n",
      "Gradient Descent(446/999): loss=0.3392371305506747\n",
      "Gradient Descent(447/999): loss=0.33923658048292993\n",
      "Gradient Descent(448/999): loss=0.339236040379932\n",
      "Gradient Descent(449/999): loss=0.3392355100578071\n",
      "Gradient Descent(450/999): loss=0.33923498933613233\n",
      "Gradient Descent(451/999): loss=0.339234478037871\n",
      "Gradient Descent(452/999): loss=0.33923397598930755\n",
      "Gradient Descent(453/999): loss=0.3392334830199842\n",
      "Gradient Descent(454/999): loss=0.33923299896263925\n",
      "Gradient Descent(455/999): loss=0.33923252365314605\n",
      "Gradient Descent(456/999): loss=0.33923205693045305\n",
      "Gradient Descent(457/999): loss=0.3392315986365259\n",
      "Gradient Descent(458/999): loss=0.3392311486162894\n",
      "Gradient Descent(459/999): loss=0.33923070671757144\n",
      "Gradient Descent(460/999): loss=0.33923027279104817\n",
      "Gradient Descent(461/999): loss=0.339229846690189\n",
      "Gradient Descent(462/999): loss=0.3392294282712045\n",
      "Gradient Descent(463/999): loss=0.33922901739299344\n",
      "Gradient Descent(464/999): loss=0.33922861391709236\n",
      "Gradient Descent(465/999): loss=0.3392282177076251\n",
      "Gradient Descent(466/999): loss=0.3392278286312538\n",
      "Gradient Descent(467/999): loss=0.3392274465571309\n",
      "Gradient Descent(468/999): loss=0.3392270713568514\n",
      "Gradient Descent(469/999): loss=0.33922670290440704\n",
      "Gradient Descent(470/999): loss=0.33922634107614064\n",
      "Gradient Descent(471/999): loss=0.3392259857507014\n",
      "Gradient Descent(472/999): loss=0.3392256368090013\n",
      "Gradient Descent(473/999): loss=0.33922529413417235\n",
      "Gradient Descent(474/999): loss=0.3392249576115243\n",
      "Gradient Descent(475/999): loss=0.3392246271285036\n",
      "Gradient Descent(476/999): loss=0.3392243025746526\n",
      "Gradient Descent(477/999): loss=0.3392239838415703\n",
      "Gradient Descent(478/999): loss=0.3392236708228733\n",
      "Gradient Descent(479/999): loss=0.33922336341415743\n",
      "Gradient Descent(480/999): loss=0.3392230615129605\n",
      "Gradient Descent(481/999): loss=0.33922276501872567\n",
      "Gradient Descent(482/999): loss=0.33922247383276505\n",
      "Gradient Descent(483/999): loss=0.339222187858225\n",
      "Gradient Descent(484/999): loss=0.3392219070000509\n",
      "Gradient Descent(485/999): loss=0.33922163116495335\n",
      "Gradient Descent(486/999): loss=0.33922136026137506\n",
      "Gradient Descent(487/999): loss=0.3392210941994578\n",
      "Gradient Descent(488/999): loss=0.3392208328910107\n",
      "Gradient Descent(489/999): loss=0.3392205762494784\n",
      "Gradient Descent(490/999): loss=0.33922032418991077\n",
      "Gradient Descent(491/999): loss=0.33922007662893194\n",
      "Gradient Descent(492/999): loss=0.3392198334847113\n",
      "Gradient Descent(493/999): loss=0.33921959467693374\n",
      "Gradient Descent(494/999): loss=0.33921936012677173\n",
      "Gradient Descent(495/999): loss=0.3392191297568567\n",
      "Gradient Descent(496/999): loss=0.33921890349125233\n",
      "Gradient Descent(497/999): loss=0.33921868125542676\n",
      "Gradient Descent(498/999): loss=0.3392184629762266\n",
      "Gradient Descent(499/999): loss=0.3392182485818512\n",
      "Gradient Descent(500/999): loss=0.3392180380018266\n",
      "Gradient Descent(501/999): loss=0.33921783116698134\n",
      "Gradient Descent(502/999): loss=0.33921762800942123\n",
      "Gradient Descent(503/999): loss=0.3392174284625061\n",
      "Gradient Descent(504/999): loss=0.3392172324608255\n",
      "Gradient Descent(505/999): loss=0.3392170399401761\n",
      "Gradient Descent(506/999): loss=0.33921685083753905\n",
      "Gradient Descent(507/999): loss=0.33921666509105763\n",
      "Gradient Descent(508/999): loss=0.3392164826400149\n",
      "Gradient Descent(509/999): loss=0.3392163034248136\n",
      "Gradient Descent(510/999): loss=0.339216127386954\n",
      "Gradient Descent(511/999): loss=0.3392159544690137\n",
      "Gradient Descent(512/999): loss=0.3392157846146276\n",
      "Gradient Descent(513/999): loss=0.339215617768468\n",
      "Gradient Descent(514/999): loss=0.33921545387622515\n",
      "Gradient Descent(515/999): loss=0.339215292884588\n",
      "Gradient Descent(516/999): loss=0.339215134741226\n",
      "Gradient Descent(517/999): loss=0.3392149793947702\n",
      "Gradient Descent(518/999): loss=0.33921482679479564\n",
      "Gradient Descent(519/999): loss=0.3392146768918037\n",
      "Gradient Descent(520/999): loss=0.3392145296372044\n",
      "Gradient Descent(521/999): loss=0.3392143849832998\n",
      "Gradient Descent(522/999): loss=0.3392142428832674\n",
      "Gradient Descent(523/999): loss=0.3392141032911432\n",
      "Gradient Descent(524/999): loss=0.33921396616180666\n",
      "Gradient Descent(525/999): loss=0.33921383145096384\n",
      "Gradient Descent(526/999): loss=0.3392136991151328\n",
      "Gradient Descent(527/999): loss=0.3392135691116281\n",
      "Gradient Descent(528/999): loss=0.3392134413985461\n",
      "Gradient Descent(529/999): loss=0.3392133159347503\n",
      "Gradient Descent(530/999): loss=0.33921319267985695\n",
      "Gradient Descent(531/999): loss=0.33921307159422104\n",
      "Gradient Descent(532/999): loss=0.33921295263892304\n",
      "Gradient Descent(533/999): loss=0.3392128357757545\n",
      "Gradient Descent(534/999): loss=0.3392127209672057\n",
      "Gradient Descent(535/999): loss=0.339212608176452\n",
      "Gradient Descent(536/999): loss=0.3392124973673414\n",
      "Gradient Descent(537/999): loss=0.339212388504382\n",
      "Gradient Descent(538/999): loss=0.33921228155272976\n",
      "Gradient Descent(539/999): loss=0.3392121764781764\n",
      "Gradient Descent(540/999): loss=0.33921207324713754\n",
      "Gradient Descent(541/999): loss=0.3392119718266411\n",
      "Gradient Descent(542/999): loss=0.33921187218431625\n",
      "Gradient Descent(543/999): loss=0.3392117742883818\n",
      "Gradient Descent(544/999): loss=0.3392116781076355\n",
      "Gradient Descent(545/999): loss=0.3392115836114434\n",
      "Gradient Descent(546/999): loss=0.33921149076972895\n",
      "Gradient Descent(547/999): loss=0.3392113995529632\n",
      "Gradient Descent(548/999): loss=0.3392113099321541\n",
      "Gradient Descent(549/999): loss=0.33921122187883684\n",
      "Gradient Descent(550/999): loss=0.3392111353650642\n",
      "Gradient Descent(551/999): loss=0.33921105036339666\n",
      "Gradient Descent(552/999): loss=0.33921096684689334\n",
      "Gradient Descent(553/999): loss=0.33921088478910233\n",
      "Gradient Descent(554/999): loss=0.3392108041640523\n",
      "Gradient Descent(555/999): loss=0.339210724946243\n",
      "Gradient Descent(556/999): loss=0.3392106471106365\n",
      "Gradient Descent(557/999): loss=0.3392105706326497\n",
      "Gradient Descent(558/999): loss=0.3392104954881443\n",
      "Gradient Descent(559/999): loss=0.3392104216534201\n",
      "Gradient Descent(560/999): loss=0.339210349105206\n",
      "Gradient Descent(561/999): loss=0.3392102778206522\n",
      "Gradient Descent(562/999): loss=0.339210207777323\n",
      "Gradient Descent(563/999): loss=0.33921013895318824\n",
      "Gradient Descent(564/999): loss=0.33921007132661674\n",
      "Gradient Descent(565/999): loss=0.33921000487636815\n",
      "Gradient Descent(566/999): loss=0.3392099395815864\n",
      "Gradient Descent(567/999): loss=0.3392098754217923\n",
      "Gradient Descent(568/999): loss=0.33920981237687686\n",
      "Gradient Descent(569/999): loss=0.33920975042709345\n",
      "Gradient Descent(570/999): loss=0.33920968955305314\n",
      "Gradient Descent(571/999): loss=0.33920962973571583\n",
      "Gradient Descent(572/999): loss=0.3392095709563858\n",
      "Gradient Descent(573/999): loss=0.339209513196704\n",
      "Gradient Descent(574/999): loss=0.3392094564386426\n",
      "Gradient Descent(575/999): loss=0.3392094006644985\n",
      "Gradient Descent(576/999): loss=0.33920934585688783\n",
      "Gradient Descent(577/999): loss=0.33920929199873967\n",
      "Gradient Descent(578/999): loss=0.33920923907329026\n",
      "Gradient Descent(579/999): loss=0.339209187064078\n",
      "Gradient Descent(580/999): loss=0.33920913595493696\n",
      "Gradient Descent(581/999): loss=0.33920908572999237\n",
      "Gradient Descent(582/999): loss=0.3392090363736548\n",
      "Gradient Descent(583/999): loss=0.33920898787061476\n",
      "Gradient Descent(584/999): loss=0.33920894020583814\n",
      "Gradient Descent(585/999): loss=0.3392088933645607\n",
      "Gradient Descent(586/999): loss=0.3392088473322834\n",
      "Gradient Descent(587/999): loss=0.3392088020947672\n",
      "Gradient Descent(588/999): loss=0.3392087576380288\n",
      "Gradient Descent(589/999): loss=0.3392087139483356\n",
      "Gradient Descent(590/999): loss=0.3392086710122011\n",
      "Gradient Descent(591/999): loss=0.3392086288163807\n",
      "Gradient Descent(592/999): loss=0.33920858734786713\n",
      "Gradient Descent(593/999): loss=0.33920854659388583\n",
      "Gradient Descent(594/999): loss=0.3392085065418914\n",
      "Gradient Descent(595/999): loss=0.3392084671795624\n",
      "Gradient Descent(596/999): loss=0.3392084284947984\n",
      "Gradient Descent(597/999): loss=0.3392083904757152\n",
      "Gradient Descent(598/999): loss=0.3392083531106408\n",
      "Gradient Descent(599/999): loss=0.3392083163881124\n",
      "Gradient Descent(600/999): loss=0.33920828029687156\n",
      "Gradient Descent(601/999): loss=0.339208244825861\n",
      "Gradient Descent(602/999): loss=0.3392082099642211\n",
      "Gradient Descent(603/999): loss=0.33920817570128564\n",
      "Gradient Descent(604/999): loss=0.33920814202657923\n",
      "Gradient Descent(605/999): loss=0.33920810892981285\n",
      "Gradient Descent(606/999): loss=0.3392080764008813\n",
      "Gradient Descent(607/999): loss=0.33920804442985925\n",
      "Gradient Descent(608/999): loss=0.3392080130069984\n",
      "Gradient Descent(609/999): loss=0.33920798212272385\n",
      "Gradient Descent(610/999): loss=0.33920795176763135\n",
      "Gradient Descent(611/999): loss=0.3392079219324841\n",
      "Gradient Descent(612/999): loss=0.33920789260820944\n",
      "Gradient Descent(613/999): loss=0.33920786378589635\n",
      "Gradient Descent(614/999): loss=0.33920783545679184\n",
      "Gradient Descent(615/999): loss=0.3392078076122989\n",
      "Gradient Descent(616/999): loss=0.33920778024397313\n",
      "Gradient Descent(617/999): loss=0.33920775334352\n",
      "Gradient Descent(618/999): loss=0.33920772690279266\n",
      "Gradient Descent(619/999): loss=0.33920770091378816\n",
      "Gradient Descent(620/999): loss=0.33920767536864604\n",
      "Gradient Descent(621/999): loss=0.33920765025964544\n",
      "Gradient Descent(622/999): loss=0.33920762557920187\n",
      "Gradient Descent(623/999): loss=0.33920760131986555\n",
      "Gradient Descent(624/999): loss=0.33920757747431873\n",
      "Gradient Descent(625/999): loss=0.3392075540353732\n",
      "Gradient Descent(626/999): loss=0.339207530995968\n",
      "Gradient Descent(627/999): loss=0.3392075083491673\n",
      "Gradient Descent(628/999): loss=0.33920748608815776\n",
      "Gradient Descent(629/999): loss=0.3392074642062468\n",
      "Gradient Descent(630/999): loss=0.3392074426968599\n",
      "Gradient Descent(631/999): loss=0.33920742155353895\n",
      "Gradient Descent(632/999): loss=0.3392074007699399\n",
      "Gradient Descent(633/999): loss=0.3392073803398307\n",
      "Gradient Descent(634/999): loss=0.3392073602570893\n",
      "Gradient Descent(635/999): loss=0.33920734051570206\n",
      "Gradient Descent(636/999): loss=0.339207321109761\n",
      "Gradient Descent(637/999): loss=0.3392073020334624\n",
      "Gradient Descent(638/999): loss=0.3392072832811051\n",
      "Gradient Descent(639/999): loss=0.33920726484708846\n",
      "Gradient Descent(640/999): loss=0.3392072467259101\n",
      "Gradient Descent(641/999): loss=0.33920722891216504\n",
      "Gradient Descent(642/999): loss=0.3392072114005433\n",
      "Gradient Descent(643/999): loss=0.3392071941858281\n",
      "Gradient Descent(644/999): loss=0.3392071772628949\n",
      "Gradient Descent(645/999): loss=0.3392071606267089\n",
      "Gradient Descent(646/999): loss=0.33920714427232423\n",
      "Gradient Descent(647/999): loss=0.3392071281948815\n",
      "Gradient Descent(648/999): loss=0.33920711238960716\n",
      "Gradient Descent(649/999): loss=0.339207096851811\n",
      "Gradient Descent(650/999): loss=0.3392070815768857\n",
      "Gradient Descent(651/999): loss=0.33920706656030447\n",
      "Gradient Descent(652/999): loss=0.33920705179761995\n",
      "Gradient Descent(653/999): loss=0.3392070372844629\n",
      "Gradient Descent(654/999): loss=0.3392070230165408\n",
      "Gradient Descent(655/999): loss=0.33920700898963607\n",
      "Gradient Descent(656/999): loss=0.33920699519960523\n",
      "Gradient Descent(657/999): loss=0.3392069816423775\n",
      "Gradient Descent(658/999): loss=0.339206968313953\n",
      "Gradient Descent(659/999): loss=0.33920695521040223\n",
      "Gradient Descent(660/999): loss=0.3392069423278642\n",
      "Gradient Descent(661/999): loss=0.33920692966254573\n",
      "Gradient Descent(662/999): loss=0.3392069172107196\n",
      "Gradient Descent(663/999): loss=0.3392069049687242\n",
      "Gradient Descent(664/999): loss=0.33920689293296136\n",
      "Gradient Descent(665/999): loss=0.3392068810998963\n",
      "Gradient Descent(666/999): loss=0.33920686946605555\n",
      "Gradient Descent(667/999): loss=0.3392068580280265\n",
      "Gradient Descent(668/999): loss=0.339206846782456\n",
      "Gradient Descent(669/999): loss=0.3392068357260494\n",
      "Gradient Descent(670/999): loss=0.33920682485556974\n",
      "Gradient Descent(671/999): loss=0.33920681416783593\n",
      "Gradient Descent(672/999): loss=0.3392068036597229\n",
      "Gradient Descent(673/999): loss=0.3392067933281597\n",
      "Gradient Descent(674/999): loss=0.33920678317012887\n",
      "Gradient Descent(675/999): loss=0.3392067731826655\n",
      "Gradient Descent(676/999): loss=0.33920676336285654\n",
      "Gradient Descent(677/999): loss=0.3392067537078391\n",
      "Gradient Descent(678/999): loss=0.3392067442148006\n",
      "Gradient Descent(679/999): loss=0.3392067348809771\n",
      "Gradient Descent(680/999): loss=0.33920672570365257\n",
      "Gradient Descent(681/999): loss=0.33920671668015845\n",
      "Gradient Descent(682/999): loss=0.3392067078078722\n",
      "Gradient Descent(683/999): loss=0.33920669908421713\n",
      "Gradient Descent(684/999): loss=0.3392066905066612\n",
      "Gradient Descent(685/999): loss=0.3392066820727162\n",
      "Gradient Descent(686/999): loss=0.33920667377993696\n",
      "Gradient Descent(687/999): loss=0.3392066656259209\n",
      "Gradient Descent(688/999): loss=0.33920665760830715\n",
      "Gradient Descent(689/999): loss=0.33920664972477554\n",
      "Gradient Descent(690/999): loss=0.339206641973046\n",
      "Gradient Descent(691/999): loss=0.3392066343508784\n",
      "Gradient Descent(692/999): loss=0.33920662685607084\n",
      "Gradient Descent(693/999): loss=0.3392066194864599\n",
      "Gradient Descent(694/999): loss=0.33920661223991944\n",
      "Gradient Descent(695/999): loss=0.33920660511435996\n",
      "Gradient Descent(696/999): loss=0.33920659810772824\n",
      "Gradient Descent(697/999): loss=0.33920659121800667\n",
      "Gradient Descent(698/999): loss=0.339206584443212\n",
      "Gradient Descent(699/999): loss=0.33920657778139574\n",
      "Gradient Descent(700/999): loss=0.3392065712306429\n",
      "Gradient Descent(701/999): loss=0.33920656478907124\n",
      "Gradient Descent(702/999): loss=0.33920655845483144\n",
      "Gradient Descent(703/999): loss=0.3392065522261058\n",
      "Gradient Descent(704/999): loss=0.3392065461011078\n",
      "Gradient Descent(705/999): loss=0.33920654007808215\n",
      "Gradient Descent(706/999): loss=0.33920653415530344\n",
      "Gradient Descent(707/999): loss=0.3392065283310761\n",
      "Gradient Descent(708/999): loss=0.33920652260373385\n",
      "Gradient Descent(709/999): loss=0.33920651697163884\n",
      "Gradient Descent(710/999): loss=0.33920651143318153\n",
      "Gradient Descent(711/999): loss=0.33920650598678015\n",
      "Gradient Descent(712/999): loss=0.33920650063088004\n",
      "Gradient Descent(713/999): loss=0.33920649536395325\n",
      "Gradient Descent(714/999): loss=0.339206490184498\n",
      "Gradient Descent(715/999): loss=0.33920648509103857\n",
      "Gradient Descent(716/999): loss=0.3392064800821242\n",
      "Gradient Descent(717/999): loss=0.33920647515632935\n",
      "Gradient Descent(718/999): loss=0.33920647031225293\n",
      "Gradient Descent(719/999): loss=0.3392064655485176\n",
      "Gradient Descent(720/999): loss=0.33920646086377\n",
      "Gradient Descent(721/999): loss=0.3392064562566795\n",
      "Gradient Descent(722/999): loss=0.33920645172593894\n",
      "Gradient Descent(723/999): loss=0.33920644727026295\n",
      "Gradient Descent(724/999): loss=0.33920644288838836\n",
      "Gradient Descent(725/999): loss=0.3392064385790738\n",
      "Gradient Descent(726/999): loss=0.33920643434109893\n",
      "Gradient Descent(727/999): loss=0.33920643017326435\n",
      "Gradient Descent(728/999): loss=0.3392064260743912\n",
      "Gradient Descent(729/999): loss=0.33920642204332085\n",
      "Gradient Descent(730/999): loss=0.3392064180789143\n",
      "Gradient Descent(731/999): loss=0.33920641418005204\n",
      "Gradient Descent(732/999): loss=0.33920641034563387\n",
      "Gradient Descent(733/999): loss=0.33920640657457823\n",
      "Gradient Descent(734/999): loss=0.339206402865822\n",
      "Gradient Descent(735/999): loss=0.3392063992183202\n",
      "Gradient Descent(736/999): loss=0.3392063956310458\n",
      "Gradient Descent(737/999): loss=0.33920639210298914\n",
      "Gradient Descent(738/999): loss=0.3392063886331579\n",
      "Gradient Descent(739/999): loss=0.33920638522057656\n",
      "Gradient Descent(740/999): loss=0.3392063818642863\n",
      "Gradient Descent(741/999): loss=0.3392063785633447\n",
      "Gradient Descent(742/999): loss=0.33920637531682485\n",
      "Gradient Descent(743/999): loss=0.33920637212381655\n",
      "Gradient Descent(744/999): loss=0.3392063689834244\n",
      "Gradient Descent(745/999): loss=0.33920636589476844\n",
      "Gradient Descent(746/999): loss=0.3392063628569835\n",
      "Gradient Descent(747/999): loss=0.3392063598692195\n",
      "Gradient Descent(748/999): loss=0.33920635693064055\n",
      "Gradient Descent(749/999): loss=0.33920635404042476\n",
      "Gradient Descent(750/999): loss=0.33920635119776477\n",
      "Gradient Descent(751/999): loss=0.3392063484018663\n",
      "Gradient Descent(752/999): loss=0.3392063456519489\n",
      "Gradient Descent(753/999): loss=0.3392063429472454\n",
      "Gradient Descent(754/999): loss=0.3392063402870014\n",
      "Gradient Descent(755/999): loss=0.33920633767047553\n",
      "Gradient Descent(756/999): loss=0.33920633509693887\n",
      "Gradient Descent(757/999): loss=0.33920633256567484\n",
      "Gradient Descent(758/999): loss=0.3392063300759791\n",
      "Gradient Descent(759/999): loss=0.3392063276271592\n",
      "Gradient Descent(760/999): loss=0.3392063252185342\n",
      "Gradient Descent(761/999): loss=0.339206322849435\n",
      "Gradient Descent(762/999): loss=0.3392063205192038\n",
      "Gradient Descent(763/999): loss=0.3392063182271936\n",
      "Gradient Descent(764/999): loss=0.3392063159727687\n",
      "Gradient Descent(765/999): loss=0.33920631375530386\n",
      "Gradient Descent(766/999): loss=0.3392063115741847\n",
      "Gradient Descent(767/999): loss=0.3392063094288071\n",
      "Gradient Descent(768/999): loss=0.33920630731857687\n",
      "Gradient Descent(769/999): loss=0.33920630524291034\n",
      "Gradient Descent(770/999): loss=0.33920630320123346\n",
      "Gradient Descent(771/999): loss=0.3392063011929817\n",
      "Gradient Descent(772/999): loss=0.33920629921760026\n",
      "Gradient Descent(773/999): loss=0.33920629727454377\n",
      "Gradient Descent(774/999): loss=0.3392062953632757\n",
      "Gradient Descent(775/999): loss=0.339206293483269\n",
      "Gradient Descent(776/999): loss=0.3392062916340051\n",
      "Gradient Descent(777/999): loss=0.3392062898149746\n",
      "Gradient Descent(778/999): loss=0.33920628802567615\n",
      "Gradient Descent(779/999): loss=0.3392062862656174\n",
      "Gradient Descent(780/999): loss=0.3392062845343137\n",
      "Gradient Descent(781/999): loss=0.3392062828312891\n",
      "Gradient Descent(782/999): loss=0.33920628115607543\n",
      "Gradient Descent(783/999): loss=0.33920627950821214\n",
      "Gradient Descent(784/999): loss=0.33920627788724694\n",
      "Gradient Descent(785/999): loss=0.3392062762927347\n",
      "Gradient Descent(786/999): loss=0.33920627472423787\n",
      "Gradient Descent(787/999): loss=0.3392062731813265\n",
      "Gradient Descent(788/999): loss=0.33920627166357753\n",
      "Gradient Descent(789/999): loss=0.3392062701705752\n",
      "Gradient Descent(790/999): loss=0.33920626870191056\n",
      "Gradient Descent(791/999): loss=0.33920626725718167\n",
      "Gradient Descent(792/999): loss=0.3392062658359933\n",
      "Gradient Descent(793/999): loss=0.3392062644379568\n",
      "Gradient Descent(794/999): loss=0.3392062630626901\n",
      "Gradient Descent(795/999): loss=0.3392062617098173\n",
      "Gradient Descent(796/999): loss=0.3392062603789694\n",
      "Gradient Descent(797/999): loss=0.33920625906978275\n",
      "Gradient Descent(798/999): loss=0.3392062577819003\n",
      "Gradient Descent(799/999): loss=0.33920625651497105\n",
      "Gradient Descent(800/999): loss=0.3392062552686495\n",
      "Gradient Descent(801/999): loss=0.33920625404259624\n",
      "Gradient Descent(802/999): loss=0.33920625283647743\n",
      "Gradient Descent(803/999): loss=0.3392062516499648\n",
      "Gradient Descent(804/999): loss=0.33920625048273545\n",
      "Gradient Descent(805/999): loss=0.33920624933447213\n",
      "Gradient Descent(806/999): loss=0.33920624820486267\n",
      "Gradient Descent(807/999): loss=0.33920624709360025\n",
      "Gradient Descent(808/999): loss=0.33920624600038307\n",
      "Gradient Descent(809/999): loss=0.3392062449249146\n",
      "Gradient Descent(810/999): loss=0.33920624386690273\n",
      "Gradient Descent(811/999): loss=0.3392062428260608\n",
      "Gradient Descent(812/999): loss=0.3392062418021067\n",
      "Gradient Descent(813/999): loss=0.33920624079476286\n",
      "Gradient Descent(814/999): loss=0.3392062398037566\n",
      "Gradient Descent(815/999): loss=0.3392062388288197\n",
      "Gradient Descent(816/999): loss=0.33920623786968834\n",
      "Gradient Descent(817/999): loss=0.3392062369261031\n",
      "Gradient Descent(818/999): loss=0.3392062359978089\n",
      "Gradient Descent(819/999): loss=0.339206235084555\n",
      "Gradient Descent(820/999): loss=0.33920623418609475\n",
      "Gradient Descent(821/999): loss=0.33920623330218547\n",
      "Gradient Descent(822/999): loss=0.33920623243258874\n",
      "Gradient Descent(823/999): loss=0.3392062315770701\n",
      "Gradient Descent(824/999): loss=0.33920623073539874\n",
      "Gradient Descent(825/999): loss=0.33920622990734783\n",
      "Gradient Descent(826/999): loss=0.3392062290926944\n",
      "Gradient Descent(827/999): loss=0.3392062282912191\n",
      "Gradient Descent(828/999): loss=0.33920622750270624\n",
      "Gradient Descent(829/999): loss=0.3392062267269436\n",
      "Gradient Descent(830/999): loss=0.33920622596372274\n",
      "Gradient Descent(831/999): loss=0.33920622521283833\n",
      "Gradient Descent(832/999): loss=0.33920622447408877\n",
      "Gradient Descent(833/999): loss=0.33920622374727544\n",
      "Gradient Descent(834/999): loss=0.33920622303220344\n",
      "Gradient Descent(835/999): loss=0.33920622232868086\n",
      "Gradient Descent(836/999): loss=0.33920622163651887\n",
      "Gradient Descent(837/999): loss=0.339206220955532\n",
      "Gradient Descent(838/999): loss=0.3392062202855378\n",
      "Gradient Descent(839/999): loss=0.33920621962635666\n",
      "Gradient Descent(840/999): loss=0.33920621897781217\n",
      "Gradient Descent(841/999): loss=0.33920621833973064\n",
      "Gradient Descent(842/999): loss=0.3392062177119414\n",
      "Gradient Descent(843/999): loss=0.33920621709427656\n",
      "Gradient Descent(844/999): loss=0.339206216486571\n",
      "Gradient Descent(845/999): loss=0.3392062158886624\n",
      "Gradient Descent(846/999): loss=0.33920621530039086\n",
      "Gradient Descent(847/999): loss=0.33920621472159945\n",
      "Gradient Descent(848/999): loss=0.3392062141521337\n",
      "Gradient Descent(849/999): loss=0.33920621359184155\n",
      "Gradient Descent(850/999): loss=0.3392062130405737\n",
      "Gradient Descent(851/999): loss=0.3392062124981831\n",
      "Gradient Descent(852/999): loss=0.3392062119645254\n",
      "Gradient Descent(853/999): loss=0.3392062114394582\n",
      "Gradient Descent(854/999): loss=0.3392062109228419\n",
      "Gradient Descent(855/999): loss=0.33920621041453897\n",
      "Gradient Descent(856/999): loss=0.3392062099144141\n",
      "Gradient Descent(857/999): loss=0.3392062094223343\n",
      "Gradient Descent(858/999): loss=0.3392062089381689\n",
      "Gradient Descent(859/999): loss=0.339206208461789\n",
      "Gradient Descent(860/999): loss=0.33920620799306817\n",
      "Gradient Descent(861/999): loss=0.339206207531882\n",
      "Gradient Descent(862/999): loss=0.3392062070781081\n",
      "Gradient Descent(863/999): loss=0.33920620663162593\n",
      "Gradient Descent(864/999): loss=0.33920620619231734\n",
      "Gradient Descent(865/999): loss=0.3392062057600655\n",
      "Gradient Descent(866/999): loss=0.33920620533475615\n",
      "Gradient Descent(867/999): loss=0.33920620491627645\n",
      "Gradient Descent(868/999): loss=0.33920620450451566\n",
      "Gradient Descent(869/999): loss=0.33920620409936475\n",
      "Gradient Descent(870/999): loss=0.3392062037007166\n",
      "Gradient Descent(871/999): loss=0.3392062033084656\n",
      "Gradient Descent(872/999): loss=0.3392062029225082\n",
      "Gradient Descent(873/999): loss=0.33920620254274225\n",
      "Gradient Descent(874/999): loss=0.3392062021690675\n",
      "Gradient Descent(875/999): loss=0.3392062018013852\n",
      "Gradient Descent(876/999): loss=0.33920620143959823\n",
      "Gradient Descent(877/999): loss=0.3392062010836112\n",
      "Gradient Descent(878/999): loss=0.33920620073333024\n",
      "Gradient Descent(879/999): loss=0.3392062003886629\n",
      "Gradient Descent(880/999): loss=0.33920620004951824\n",
      "Gradient Descent(881/999): loss=0.3392061997158069\n",
      "Gradient Descent(882/999): loss=0.339206199387441\n",
      "Gradient Descent(883/999): loss=0.33920619906433414\n",
      "Gradient Descent(884/999): loss=0.33920619874640123\n",
      "Gradient Descent(885/999): loss=0.33920619843355854\n",
      "Gradient Descent(886/999): loss=0.3392061981257238\n",
      "Gradient Descent(887/999): loss=0.339206197822816\n",
      "Gradient Descent(888/999): loss=0.3392061975247556\n",
      "Gradient Descent(889/999): loss=0.3392061972314643\n",
      "Gradient Descent(890/999): loss=0.339206196942865\n",
      "Gradient Descent(891/999): loss=0.3392061966588818\n",
      "Gradient Descent(892/999): loss=0.3392061963794404\n",
      "Gradient Descent(893/999): loss=0.33920619610446723\n",
      "Gradient Descent(894/999): loss=0.33920619583389033\n",
      "Gradient Descent(895/999): loss=0.33920619556763865\n",
      "Gradient Descent(896/999): loss=0.33920619530564244\n",
      "Gradient Descent(897/999): loss=0.33920619504783306\n",
      "Gradient Descent(898/999): loss=0.3392061947941428\n",
      "Gradient Descent(899/999): loss=0.33920619454450546\n",
      "Gradient Descent(900/999): loss=0.3392061942988556\n",
      "Gradient Descent(901/999): loss=0.3392061940571288\n",
      "Gradient Descent(902/999): loss=0.33920619381926215\n",
      "Gradient Descent(903/999): loss=0.3392061935851931\n",
      "Gradient Descent(904/999): loss=0.3392061933548607\n",
      "Gradient Descent(905/999): loss=0.3392061931282045\n",
      "Gradient Descent(906/999): loss=0.33920619290516557\n",
      "Gradient Descent(907/999): loss=0.3392061926856854\n",
      "Gradient Descent(908/999): loss=0.3392061924697069\n",
      "Gradient Descent(909/999): loss=0.3392061922571736\n",
      "Gradient Descent(910/999): loss=0.33920619204803004\n",
      "Gradient Descent(911/999): loss=0.3392061918422218\n",
      "Gradient Descent(912/999): loss=0.3392061916396949\n",
      "Gradient Descent(913/999): loss=0.33920619144039676\n",
      "Gradient Descent(914/999): loss=0.3392061912442756\n",
      "Gradient Descent(915/999): loss=0.33920619105128\n",
      "Gradient Descent(916/999): loss=0.33920619086136\n",
      "Gradient Descent(917/999): loss=0.3392061906744661\n",
      "Gradient Descent(918/999): loss=0.33920619049054934\n",
      "Gradient Descent(919/999): loss=0.33920619030956234\n",
      "Gradient Descent(920/999): loss=0.3392061901314578\n",
      "Gradient Descent(921/999): loss=0.3392061899561894\n",
      "Gradient Descent(922/999): loss=0.33920618978371164\n",
      "Gradient Descent(923/999): loss=0.33920618961397964\n",
      "Gradient Descent(924/999): loss=0.33920618944694936\n",
      "Gradient Descent(925/999): loss=0.3392061892825775\n",
      "Gradient Descent(926/999): loss=0.3392061891208213\n",
      "Gradient Descent(927/999): loss=0.33920618896163873\n",
      "Gradient Descent(928/999): loss=0.33920618880498865\n",
      "Gradient Descent(929/999): loss=0.3392061886508302\n",
      "Gradient Descent(930/999): loss=0.3392061884991237\n",
      "Gradient Descent(931/999): loss=0.33920618834982963\n",
      "Gradient Descent(932/999): loss=0.3392061882029094\n",
      "Gradient Descent(933/999): loss=0.3392061880583249\n",
      "Gradient Descent(934/999): loss=0.33920618791603874\n",
      "Gradient Descent(935/999): loss=0.33920618777601397\n",
      "Gradient Descent(936/999): loss=0.33920618763821453\n",
      "Gradient Descent(937/999): loss=0.3392061875026046\n",
      "Gradient Descent(938/999): loss=0.3392061873691492\n",
      "Gradient Descent(939/999): loss=0.33920618723781376\n",
      "Gradient Descent(940/999): loss=0.33920618710856437\n",
      "Gradient Descent(941/999): loss=0.33920618698136773\n",
      "Gradient Descent(942/999): loss=0.33920618685619075\n",
      "Gradient Descent(943/999): loss=0.33920618673300135\n",
      "Gradient Descent(944/999): loss=0.33920618661176755\n",
      "Gradient Descent(945/999): loss=0.3392061864924581\n",
      "Gradient Descent(946/999): loss=0.3392061863750422\n",
      "Gradient Descent(947/999): loss=0.3392061862594896\n",
      "Gradient Descent(948/999): loss=0.33920618614577047\n",
      "Gradient Descent(949/999): loss=0.33920618603385555\n",
      "Gradient Descent(950/999): loss=0.339206185923716\n",
      "Gradient Descent(951/999): loss=0.33920618581532325\n",
      "Gradient Descent(952/999): loss=0.3392061857086496\n",
      "Gradient Descent(953/999): loss=0.3392061856036675\n",
      "Gradient Descent(954/999): loss=0.33920618550034987\n",
      "Gradient Descent(955/999): loss=0.3392061853986702\n",
      "Gradient Descent(956/999): loss=0.3392061852986023\n",
      "Gradient Descent(957/999): loss=0.33920618520012036\n",
      "Gradient Descent(958/999): loss=0.33920618510319916\n",
      "Gradient Descent(959/999): loss=0.33920618500781374\n",
      "Gradient Descent(960/999): loss=0.3392061849139395\n",
      "Gradient Descent(961/999): loss=0.33920618482155246\n",
      "Gradient Descent(962/999): loss=0.33920618473062875\n",
      "Gradient Descent(963/999): loss=0.33920618464114516\n",
      "Gradient Descent(964/999): loss=0.3392061845530786\n",
      "Gradient Descent(965/999): loss=0.33920618446640655\n",
      "Gradient Descent(966/999): loss=0.33920618438110667\n",
      "Gradient Descent(967/999): loss=0.33920618429715715\n",
      "Gradient Descent(968/999): loss=0.33920618421453647\n",
      "Gradient Descent(969/999): loss=0.3392061841332233\n",
      "Gradient Descent(970/999): loss=0.339206184053197\n",
      "Gradient Descent(971/999): loss=0.33920618397443697\n",
      "Gradient Descent(972/999): loss=0.3392061838969231\n",
      "Gradient Descent(973/999): loss=0.3392061838206354\n",
      "Gradient Descent(974/999): loss=0.33920618374555433\n",
      "Gradient Descent(975/999): loss=0.3392061836716607\n",
      "Gradient Descent(976/999): loss=0.33920618359893573\n",
      "Gradient Descent(977/999): loss=0.3392061835273607\n",
      "Gradient Descent(978/999): loss=0.33920618345691733\n",
      "Gradient Descent(979/999): loss=0.33920618338758757\n",
      "Gradient Descent(980/999): loss=0.3392061833193537\n",
      "Gradient Descent(981/999): loss=0.3392061832521982\n",
      "Gradient Descent(982/999): loss=0.3392061831861042\n",
      "Gradient Descent(983/999): loss=0.3392061831210544\n",
      "Gradient Descent(984/999): loss=0.3392061830570325\n",
      "Gradient Descent(985/999): loss=0.33920618299402205\n",
      "Gradient Descent(986/999): loss=0.3392061829320069\n",
      "Gradient Descent(987/999): loss=0.33920618287097126\n",
      "Gradient Descent(988/999): loss=0.33920618281089965\n",
      "Gradient Descent(989/999): loss=0.33920618275177666\n",
      "Gradient Descent(990/999): loss=0.3392061826935871\n",
      "Gradient Descent(991/999): loss=0.3392061826363164\n",
      "Gradient Descent(992/999): loss=0.3392061825799497\n",
      "Gradient Descent(993/999): loss=0.33920618252447277\n",
      "Gradient Descent(994/999): loss=0.33920618246987144\n",
      "Gradient Descent(995/999): loss=0.3392061824161318\n",
      "Gradient Descent(996/999): loss=0.3392061823632402\n",
      "Gradient Descent(997/999): loss=0.3392061823111831\n",
      "Gradient Descent(998/999): loss=0.3392061822599473\n",
      "Gradient Descent(999/999): loss=0.3392061822095197\n",
      "parameters w:  [-0.36998403  0.02299596 -0.43168153]\n"
     ]
    }
   ],
   "source": [
    "from gradient_descent import least_squares_GD\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "# start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = least_squares_GD(y, tX, w_initial, gamma, max_iters)\n",
    "# end_time = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/999): loss=0.5\n",
      "Stochastic Gradient Descent(1/999): loss=0.4968060143214086\n",
      "Stochastic Gradient Descent(2/999): loss=0.49364606013191575\n",
      "Stochastic Gradient Descent(3/999): loss=0.49028466049458636\n",
      "parameters w:  [-0.01458463 -0.00108089 -0.01695087]\n"
     ]
    }
   ],
   "source": [
    "from stochastic_gradient_descent import least_squares_SGD\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "batch_size = 50\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "# start_time = datetime.datetime.now()\n",
    "stoch_gradient_losses, stoch_gradient_ws = least_squares_SGD(y, tX, w_initial, batch_size, gamma, max_iters)\n",
    "# end_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.496806014321\n",
      "0.493646060132\n",
      "0.490284660495\n",
      "\n",
      "min loss:  0.490284660495\n",
      "min index:  3\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "min_loss = 100\n",
    "min_i = -1\n",
    "for l in stoch_gradient_losses:\n",
    "    if l < min_loss:\n",
    "        min_loss = l\n",
    "        min_i = i\n",
    "    i = i+1\n",
    "    print(l)\n",
    "\n",
    "print()\n",
    "print('min loss: ',min_loss)\n",
    "print('min index: ',min_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss= 0.339206179047\n",
      "parameters w:  [-0.37        0.02307041 -0.43172776]\n"
     ]
    }
   ],
   "source": [
    "from least_squares import least_squares\n",
    "\n",
    "# start_ls_time = datetime.datetime.now()\n",
    "ls_wopt, ls_loss = least_squares(y,tX)\n",
    "# end_ls_time = datetime.datetime.now()\n",
    "print('loss=',ls_loss)\n",
    "print('parameters w: ',ls_wopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.339269396639\n",
      "parameters w:  [-0.3627451   0.02142596 -0.42308929]\n"
     ]
    }
   ],
   "source": [
    "from ridge_regression import ridge_regression\n",
    "\n",
    "# lambdas = np.logspace(-3, 1, 10)      \n",
    "# φ_x = build_poly(x, degree)\n",
    "# x_train, x_test, y_train, y_test = split_data(tX, y, ratio, seed)\n",
    "    \n",
    "#     for lamb in lambdas:\n",
    "\n",
    "w_ridge = ridge_regression(y, tX, 0.01)\n",
    "err = compute_loss(y, tX, w_ridge)\n",
    "\n",
    "print('loss: ',err)\n",
    "print('parameters w: ',w_ridge)\n",
    "\n",
    "# rmse_tr = np.sqrt(2*compute_loss(y, tX, w_ridge))\n",
    "# rmse_te = np.sqrt(2*compute_loss(y, tX, w_ridge))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tx:  (200, 3)\n",
      "w:  (3, 1)\n",
      "y:  (200,)\n",
      "Current iteration=0, the loss=138.62943611198904\n",
      "Current iteration=1000, the loss=134.55691603880524\n",
      "Current iteration=2000, the loss=130.48460243164908\n",
      "Current iteration=3000, the loss=126.41249527991309\n",
      "Current iteration=4000, the loss=122.34059457298201\n",
      "Current iteration=5000, the loss=118.26890030023303\n",
      "Current iteration=6000, the loss=114.1974124510356\n",
      "Current iteration=7000, the loss=110.12613101475159\n",
      "Current iteration=8000, the loss=106.05505598073526\n",
      "Current iteration=9000, the loss=101.9841873383332\n",
      "Current iteration=10000, the loss=97.91352507688447\n",
      "Current iteration=11000, the loss=93.84306918572025\n",
      "Current iteration=12000, the loss=89.7728196541645\n",
      "Current iteration=13000, the loss=85.70277647153313\n",
      "Current iteration=14000, the loss=81.63293962713472\n",
      "Current iteration=15000, the loss=77.56330911027027\n",
      "Current iteration=16000, the loss=73.49388491023305\n",
      "Current iteration=17000, the loss=69.42466701630856\n",
      "Current iteration=18000, the loss=65.35565541777515\n",
      "Current iteration=19000, the loss=61.28685010390305\n",
      "Current iteration=20000, the loss=57.218251063955265\n",
      "Current iteration=21000, the loss=53.14985828718703\n",
      "Current iteration=22000, the loss=49.08167176284611\n",
      "Current iteration=23000, the loss=45.013691480172724\n",
      "Current iteration=24000, the loss=40.9459174283993\n",
      "Current iteration=25000, the loss=36.878349596750674\n",
      "Current iteration=26000, the loss=32.81098797444423\n",
      "Current iteration=27000, the loss=28.74383255068975\n",
      "Current iteration=28000, the loss=24.676883314689594\n",
      "Current iteration=29000, the loss=20.610140255638434\n",
      "Current iteration=30000, the loss=16.543603362723132\n",
      "The loss=13.209196944113792\n"
     ]
    }
   ],
   "source": [
    "from helpers import de_standardize\n",
    "from logistic_regression import learning_by_gradient_descent, calculate_loss\n",
    "from plots import visualization\n",
    "\n",
    "def logistic_regression_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 30820\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.000000001\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = x\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    \n",
    "    print('tx: ',tx.shape)\n",
    "    print('w: ',w.shape)\n",
    "    print('y: ',y.shape)\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 1000 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    print(\"The loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "\n",
    "logistic_regression_gradient_descent_demo(y, tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.683333333333\n",
      "0.683333333333\n"
     ]
    }
   ],
   "source": [
    "from test import *\n",
    "\n",
    "# test_GD(y, tX, 0.1)\n",
    "# test_SGD(y, tX, 0.1)\n",
    "test_LS(y, tX, 0.1)\n",
    "test_RR(y, tX, 0.1)\n",
    "# test_LR(y, tX, 0.1)\n",
    "# test_RLR(y, tX, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = \"../Data/test.csv\" # TODO: download test data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
